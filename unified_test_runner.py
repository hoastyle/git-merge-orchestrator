#!/usr/bin/env python3
"""
Git Merge Orchestrator - ç»Ÿä¸€æµ‹è¯•è¿è¡Œå™¨
æœ‰æœºç»“åˆä¸»é¡¹ç›®æµ‹è¯•å’Œtest-environmentï¼Œæ”¯æŒæ™ºèƒ½é™çº§ç­–ç•¥
"""

import sys
import os
import subprocess
import time
import json
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))


@dataclass
class TestEnvironmentStatus:
    """æµ‹è¯•ç¯å¢ƒçŠ¶æ€"""
    main_tests_available: bool = False
    test_environment_available: bool = False
    git_available: bool = False
    python_available: bool = False
    dependencies_available: bool = False
    

@dataclass
class TestResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    name: str
    passed: bool
    duration: float
    error_message: str = ""
    category: str = "unknown"


@dataclass  
class UnifiedTestReport:
    """ç»Ÿä¸€æµ‹è¯•æŠ¥å‘Š"""
    timestamp: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    success_rate: float
    total_duration: float
    environment_status: TestEnvironmentStatus
    test_results: List[TestResult]
    test_mode: str
    recommendations: List[str]


class EnvironmentDetector:
    """ç¯å¢ƒæ£€æµ‹å™¨"""
    
    @staticmethod
    def detect_environment() -> TestEnvironmentStatus:
        """æ£€æµ‹æµ‹è¯•ç¯å¢ƒçŠ¶æ€"""
        status = TestEnvironmentStatus()
        
        # æ£€æŸ¥ä¸»é¡¹ç›®æµ‹è¯•
        main_test_runner = Path(__file__).parent / "tests" / "run_tests.py"
        status.main_tests_available = main_test_runner.exists()
        
        # æ£€æŸ¥test-environment submodule
        test_env_dir = Path(__file__).parent / "test-environment"
        batch_test_script = test_env_dir / "batch_test.sh"
        status.test_environment_available = (
            test_env_dir.exists() and 
            batch_test_script.exists() and
            batch_test_script.is_file()
        )
        
        # æ£€æŸ¥Gitå¯ç”¨æ€§
        try:
            subprocess.run(["git", "--version"], 
                         capture_output=True, check=True)
            status.git_available = True
        except (subprocess.CalledProcessError, FileNotFoundError):
            status.git_available = False
            
        # æ£€æŸ¥Pythonå¯ç”¨æ€§
        status.python_available = True  # æ—¢ç„¶èƒ½è¿è¡Œè¿™ä¸ªè„šæœ¬ï¼ŒPythonè‚¯å®šå¯ç”¨
        
        # æ£€æŸ¥ä¾èµ–å¯ç”¨æ€§
        try:
            # æ£€æŸ¥æ ¸å¿ƒæ¨¡å—æ˜¯å¦å¯å¯¼å…¥
            from core.git_operations import GitOperations
            from utils.config_manager import ProjectConfigManager
            status.dependencies_available = True
        except ImportError:
            status.dependencies_available = False
            
        return status


class UnifiedTestRunner:
    """ç»Ÿä¸€æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.env_status = EnvironmentDetector.detect_environment()
        self.test_results: List[TestResult] = []
        self.start_time = time.time()
        
    def print_environment_status(self):
        """æ‰“å°ç¯å¢ƒçŠ¶æ€"""
        print("ğŸ” æ£€æµ‹æµ‹è¯•ç¯å¢ƒçŠ¶æ€...")
        print("=" * 60)
        
        status_items = [
            ("ä¸»é¡¹ç›®æµ‹è¯•", self.env_status.main_tests_available, "tests/run_tests.py"),
            ("test-environment", self.env_status.test_environment_available, "test-environment/"),
            ("Gitå·¥å…·", self.env_status.git_available, "gitå‘½ä»¤"),
            ("Pythonç¯å¢ƒ", self.env_status.python_available, "Python3"), 
            ("é¡¹ç›®ä¾èµ–", self.env_status.dependencies_available, "æ ¸å¿ƒæ¨¡å—")
        ]
        
        for name, available, desc in status_items:
            icon = "âœ…" if available else "âŒ"
            status = "å¯ç”¨" if available else "ä¸å¯ç”¨"
            print(f"{icon} {name}: {status} ({desc})")
        
        print()
        
    def run_quick_test(self) -> List[TestResult]:
        """å¿«é€Ÿæµ‹è¯• (< 30ç§’)"""
        print("ğŸš€ è¿è¡Œå¿«é€Ÿæµ‹è¯• (< 30ç§’)")
        print("-" * 40)
        results = []
        
        # 1. ä¸»é¡¹ç›®å¥åº·æ£€æŸ¥
        if self.env_status.main_tests_available:
            result = self._run_main_health_check()
            results.append(result)
        else:
            results.append(TestResult(
                "ä¸»é¡¹ç›®å¥åº·æ£€æŸ¥", False, 0, "ä¸»é¡¹ç›®æµ‹è¯•ä¸å¯ç”¨", "health"
            ))
            
        # 2. test-environmentåŸºç¡€æ£€æŸ¥  
        if self.env_status.test_environment_available:
            result = self._run_test_environment_health()
            results.append(result)
        else:
            results.append(TestResult(
                "test-environmentå¥åº·æ£€æŸ¥", False, 0, "test-environmentä¸å¯ç”¨", "health"
            ))
            
        return results
        
    def run_scenario_test(self, scenarios: List[str]) -> List[TestResult]:
        """è¿è¡ŒæŒ‡å®šåœºæ™¯æµ‹è¯•"""
        print(f"ğŸ¯ è¿è¡Œåœºæ™¯æµ‹è¯•: {', '.join(scenarios)}")
        print("-" * 40)
        results = []
        
        # ä¸»é¡¹ç›®ç‰¹å®šæµ‹è¯•
        if self.env_status.main_tests_available:
            for scenario in scenarios:
                result = self._run_main_specific_test(scenario)
                results.append(result)
                
        # test-environmentåœºæ™¯æµ‹è¯•
        if self.env_status.test_environment_available:
            result = self._run_test_environment_scenarios(scenarios)
            results.append(result)
            
        return results
        
    def run_full_test(self) -> List[TestResult]:
        """å®Œæ•´æµ‹è¯•å¥—ä»¶"""
        print("ğŸŒŸ è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶")
        print("-" * 40)
        results = []
        
        # è®¡ç®—æ€»çš„æµ‹è¯•é¡¹æ•°
        total_tests = 0
        if self.env_status.main_tests_available:
            total_tests += 1
        if self.env_status.test_environment_available:
            total_tests += 1
        total_tests += 1  # æ€§èƒ½æµ‹è¯•
        
        current_test = 0
        
        # 1. ä¸»é¡¹ç›®å®Œæ•´æµ‹è¯•
        if self.env_status.main_tests_available:
            current_test += 1
            print(f"ğŸ“‹ [{current_test}/{total_tests}] è¿è¡Œä¸»é¡¹ç›®å®Œæ•´æµ‹è¯•...")
            result = self._run_main_full_test()
            results.append(result)
            icon = "âœ…" if result.passed else "âŒ"
            print(f"    {icon} å®Œæˆ ({result.duration:.1f}s)")
            
        # 2. test-environmentå®Œæ•´æµ‹è¯•
        if self.env_status.test_environment_available:
            current_test += 1
            print(f"ğŸ“‹ [{current_test}/{total_tests}] è¿è¡Œtest-environmentå®Œæ•´æµ‹è¯•...")
            result = self._run_test_environment_full()
            results.append(result)
            icon = "âœ…" if result.passed else "âŒ"
            print(f"    {icon} å®Œæˆ ({result.duration:.1f}s)")
            
        # 3. æ€§èƒ½æµ‹è¯•
        current_test += 1
        print(f"ğŸ“‹ [{current_test}/{total_tests}] è¿è¡Œæ€§èƒ½æµ‹è¯•...")
        result = self._run_performance_test()
        results.append(result)
        icon = "âœ…" if result.passed else "âŒ"
        print(f"    {icon} å®Œæˆ ({result.duration:.1f}s)")
        
        return results
        
    def run_core_only_test(self) -> List[TestResult]:
        """ä»…ä¸»é¡¹ç›®æµ‹è¯•ï¼ˆé™çº§æ¨¡å¼ï¼‰"""
        print("âš ï¸ è¿è¡Œæ ¸å¿ƒæµ‹è¯• (é™çº§æ¨¡å¼)")
        print("-" * 40)
        results = []
        
        if self.env_status.main_tests_available:
            # ä¸»é¡¹ç›®å¥åº·æ£€æŸ¥
            result = self._run_main_health_check()
            results.append(result)
            
            # ä¸»é¡¹ç›®å®Œæ•´æµ‹è¯•
            result = self._run_main_full_test()
            results.append(result)
            
            # å†…ç½®æ¨¡æ‹Ÿæµ‹è¯•
            result = self._run_builtin_mock_tests()
            results.append(result)
        else:
            results.append(TestResult(
                "æ ¸å¿ƒæµ‹è¯•", False, 0, "ä¸»é¡¹ç›®æµ‹è¯•ä¸å¯ç”¨ï¼Œæ— æ³•è¿è¡Œæ ¸å¿ƒæµ‹è¯•", "error"
            ))
            
        return results
        
    def _run_main_health_check(self) -> TestResult:
        """è¿è¡Œä¸»é¡¹ç›®å¥åº·æ£€æŸ¥"""
        start_time = time.time()
        
        try:
            result = subprocess.run([
                sys.executable, "run_tests.py", "--health"
            ], cwd=self.project_root, capture_output=True, text=True, timeout=60)
            
            duration = time.time() - start_time
            
            if result.returncode == 0:
                return TestResult(
                    "ä¸»é¡¹ç›®å¥åº·æ£€æŸ¥", True, duration, "", "health"
                )
            else:
                return TestResult(
                    "ä¸»é¡¹ç›®å¥åº·æ£€æŸ¥", False, duration, 
                    result.stderr or "å¥åº·æ£€æŸ¥å¤±è´¥", "health"
                )
                
        except subprocess.TimeoutExpired:
            return TestResult(
                "ä¸»é¡¹ç›®å¥åº·æ£€æŸ¥", False, time.time() - start_time, 
                "æµ‹è¯•è¶…æ—¶ï¼ˆ60ç§’ï¼‰", "health"
            )
        except Exception as e:
            return TestResult(
                "ä¸»é¡¹ç›®å¥åº·æ£€æŸ¥", False, time.time() - start_time,
                f"æ‰§è¡Œå¼‚å¸¸: {e}", "health"
            )
            
    def _run_test_environment_health(self) -> TestResult:
        """è¿è¡Œtest-environmentå¥åº·æ£€æŸ¥"""
        start_time = time.time()
        
        try:
            test_env_dir = self.project_root / "test-environment"
            
            # ç®€åŒ–çš„å¥åº·æ£€æŸ¥ï¼šéªŒè¯åŸºç¡€ç»“æ„å’Œå…³é”®æ–‡ä»¶
            checks_passed = 0
            total_checks = 4
            
            # 1. æ£€æŸ¥ç›®å½•å­˜åœ¨
            if test_env_dir.exists():
                checks_passed += 1
            
            # 2. æ£€æŸ¥READMEæ–‡ä»¶å­˜åœ¨  
            readme_file = test_env_dir / "README.md"
            if readme_file.exists():
                checks_passed += 1
                
            # 3. æ£€æŸ¥test-scriptsç›®å½•å­˜åœ¨
            scripts_dir = test_env_dir / "test-scripts"
            if scripts_dir.exists():
                checks_passed += 1
                
            # 4. æ£€æŸ¥è‡³å°‘æœ‰ä¸€ä¸ªæ ¸å¿ƒè„šæœ¬å­˜åœ¨
            core_script = test_env_dir / "test-scripts" / "create_test_repo.py"
            if core_script.exists():
                checks_passed += 1
            
            duration = time.time() - start_time
            
            if checks_passed >= 3:  # è‡³å°‘3/4æ£€æŸ¥é€šè¿‡
                return TestResult(
                    "test-environmentå¥åº·æ£€æŸ¥", True, duration, 
                    f"å¥åº·æ£€æŸ¥é€šè¿‡ ({checks_passed}/{total_checks})", "health"
                )
            else:
                return TestResult(
                    "test-environmentå¥åº·æ£€æŸ¥", False, duration,
                    f"å¥åº·æ£€æŸ¥ä¸è¶³ ({checks_passed}/{total_checks})", "health"
                )
                
        except Exception as e:
            return TestResult(
                "test-environmentå¥åº·æ£€æŸ¥", False, time.time() - start_time,
                f"æ‰§è¡Œå¼‚å¸¸: {e}", "health"
            )
            
    def _run_main_specific_test(self, scenario: str) -> TestResult:
        """è¿è¡Œä¸»é¡¹ç›®ç‰¹å®šæµ‹è¯•"""
        start_time = time.time()
        
        # åœºæ™¯æ˜ å°„åˆ°ä¸»é¡¹ç›®æµ‹è¯•ç±»åˆ«
        scenario_mapping = {
            "config": "é…ç½®ç®¡ç†",
            "performance": "æ€§èƒ½æµ‹è¯•",  
            "git": "Gitæ“ä½œ",
            "merge": "åˆå¹¶ç­–ç•¥",
            "integration": "é›†æˆæµ‹è¯•"
        }
        
        test_name = scenario_mapping.get(scenario, scenario)
        
        try:
            # è¿è¡Œä¸»é¡¹ç›®åˆ†ç±»æµ‹è¯•
            result = subprocess.run([
                sys.executable, "tests/run_tests.py", "--category", scenario
            ], cwd=self.project_root, capture_output=True, text=True, timeout=300)
            
            duration = time.time() - start_time
            
            if result.returncode == 0:
                return TestResult(
                    f"ä¸»é¡¹ç›®{test_name}æµ‹è¯•", True, duration, "", "scenario"
                )
            else:
                return TestResult(
                    f"ä¸»é¡¹ç›®{test_name}æµ‹è¯•", False, duration,
                    result.stderr or f"{test_name}æµ‹è¯•å¤±è´¥", "scenario"
                )
                
        except Exception as e:
            return TestResult(
                f"ä¸»é¡¹ç›®{test_name}æµ‹è¯•", False, time.time() - start_time,
                f"æ‰§è¡Œå¼‚å¸¸: {e}", "scenario"
            )
            
    def _run_test_environment_scenarios(self, scenarios: List[str]) -> TestResult:
        """è¿è¡Œtest-environmentåœºæ™¯æµ‹è¯•"""
        start_time = time.time()
        
        try:
            test_env_dir = self.project_root / "test-environment"
            
            # ç®€åŒ–çš„åœºæ™¯æµ‹è¯•ï¼šéªŒè¯åœºæ™¯é…ç½®æ–‡ä»¶å’Œè„šæœ¬å­˜åœ¨
            success = True
            error_messages = []
            scenarios_checked = 0
            
            # æ£€æŸ¥åœºæ™¯è®¾ç½®è„šæœ¬æ˜¯å¦å­˜åœ¨
            setup_script = test_env_dir / "test-scripts" / "setup_scenarios.py"
            if not setup_script.exists():
                success = False
                error_messages.append("setup_scenarios.pyè„šæœ¬ä¸å­˜åœ¨")
            else:
                # æµ‹è¯•åœºæ™¯è„šæœ¬çš„åŸºæœ¬å¯ç”¨æ€§
                try:
                    result = subprocess.run([
                        sys.executable, "test-scripts/setup_scenarios.py", "--help"
                    ], cwd=test_env_dir, capture_output=True, text=True, timeout=15)
                    
                    if result.returncode == 0:
                        scenarios_checked += 1
                    else:
                        error_messages.append("åœºæ™¯è®¾ç½®è„šæœ¬ä¸å¯æ‰§è¡Œ")
                        
                except subprocess.TimeoutExpired:
                    error_messages.append("åœºæ™¯è„šæœ¬æ‰§è¡Œè¶…æ—¶")
                except Exception as e:
                    error_messages.append(f"åœºæ™¯è„šæœ¬æµ‹è¯•å¤±è´¥: {e}")
            
            # æ£€æŸ¥æµ‹è¯•æ•°æ®ç›®å½•
            test_data_dir = test_env_dir / "test-data"
            if test_data_dir.exists():
                scenarios_checked += 1
            else:
                error_messages.append("test-dataç›®å½•ä¸å­˜åœ¨")
            
            duration = time.time() - start_time
            
            if scenarios_checked >= 1:  # è‡³å°‘ä¸€ä¸ªåœºæ™¯ç»„ä»¶å¯ç”¨
                return TestResult(
                    f"test-environmentåœºæ™¯æµ‹è¯•", True, duration, 
                    f"åœºæ™¯éªŒè¯é€šè¿‡ ({scenarios_checked}/2)", "scenario"
                )
            else:
                return TestResult(
                    f"test-environmentåœºæ™¯æµ‹è¯•", False, duration,
                    "; ".join(error_messages), "scenario"
                )
                
        except Exception as e:
            return TestResult(
                f"test-environmentåœºæ™¯æµ‹è¯•", False, time.time() - start_time,
                f"æ‰§è¡Œå¼‚å¸¸: {e}", "scenario"
            )
            
    def _run_main_full_test(self) -> TestResult:
        """è¿è¡Œä¸»é¡¹ç›®å®Œæ•´æµ‹è¯•"""
        start_time = time.time()
        
        try:
            result = subprocess.run([
                sys.executable, "run_tests.py", "--full"
            ], cwd=self.project_root, capture_output=True, text=True, timeout=600)
            
            duration = time.time() - start_time
            
            if result.returncode == 0:
                return TestResult(
                    "ä¸»é¡¹ç›®å®Œæ•´æµ‹è¯•", True, duration, "", "full"
                )
            else:
                return TestResult(
                    "ä¸»é¡¹ç›®å®Œæ•´æµ‹è¯•", False, duration,
                    result.stderr or "å®Œæ•´æµ‹è¯•å¤±è´¥", "full"
                )
                
        except Exception as e:
            return TestResult(
                "ä¸»é¡¹ç›®å®Œæ•´æµ‹è¯•", False, time.time() - start_time,
                f"æ‰§è¡Œå¼‚å¸¸: {e}", "full"
            )
            
    def _run_test_environment_full(self) -> TestResult:
        """è¿è¡Œtest-environmentå®Œæ•´æµ‹è¯•"""
        start_time = time.time()
        
        try:
            test_env_dir = self.project_root / "test-environment"
            
            # è¿è¡Œå®é™…çš„test-environmentæ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
            # é¿å…batch_test.shçš„å¤–éƒ¨è·¯å¾„ä¾èµ–ï¼Œç›´æ¥è¿è¡Œæ ¸å¿ƒæµ‹è¯•
            
            success = True
            error_messages = []
            tests_passed = 0
            total_tests = 0
            
            # 1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥
            total_tests += 1
            required_dirs = ["test-scripts", "test-data", "scenarios"]
            dirs_exist = all((test_env_dir / dir_name).exists() for dir_name in required_dirs)
            if dirs_exist:
                tests_passed += 1
            else:
                error_messages.append("ç›®å½•ç»“æ„æ£€æŸ¥å¤±è´¥")
            
            # 2. æ ¸å¿ƒè„šæœ¬åŠŸèƒ½æµ‹è¯•
            key_tests = [
                ("create_test_repo.py", "åˆ›å»ºæµ‹è¯•ä»“åº“"),
                ("setup_scenarios.py", "åœºæ™¯è®¾ç½®"), 
                ("integration_tests.py", "é›†æˆæµ‹è¯•è„šæœ¬")
            ]
            
            for script_name, test_name in key_tests:
                total_tests += 1
                script_path = test_env_dir / "test-scripts" / script_name
                
                if script_path.exists():
                    try:
                        # æµ‹è¯•è„šæœ¬çš„åŸºæœ¬å¯æ‰§è¡Œæ€§
                        result = subprocess.run([
                            sys.executable, f"test-scripts/{script_name}", "--help"
                        ], cwd=test_env_dir, capture_output=True, text=True, timeout=15)
                        
                        if result.returncode == 0:
                            tests_passed += 1
                        else:
                            error_messages.append(f"{test_name}è„šæœ¬ä¸å¯æ‰§è¡Œ")
                            
                    except subprocess.TimeoutExpired:
                        error_messages.append(f"{test_name}è„šæœ¬æ‰§è¡Œè¶…æ—¶")
                    except Exception as e:
                        error_messages.append(f"{test_name}æµ‹è¯•å¤±è´¥: {e}")
                else:
                    error_messages.append(f"ç¼ºå°‘{test_name}è„šæœ¬")
            
            # 3. å¿«é€Ÿé›†æˆæµ‹è¯•éªŒè¯ - åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•ä»“åº“
            total_tests += 1
            try:
                import random
                import string
                # ä½¿ç”¨éšæœºåç¼€é¿å…ä»“åº“åå†²çª
                random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
                temp_repo_name = f"test_repo_{random_suffix}"
                temp_repo_path = test_env_dir / "test-repos" / temp_repo_name
                
                # åˆ›å»ºæµ‹è¯•ä»“åº“ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
                result = subprocess.run([
                    sys.executable, "test-scripts/create_test_repo.py",
                    "--name", temp_repo_name,
                    "--type", "simple",
                    "--contributors", "Alice,Bob"
                ], cwd=test_env_dir, capture_output=True, text=True, timeout=45)
                
                if result.returncode == 0 and temp_repo_path.exists():
                    tests_passed += 1
                    # æ¸…ç†æµ‹è¯•ä»“åº“
                    try:
                        import shutil
                        shutil.rmtree(temp_repo_path)
                    except:
                        pass  # æ¸…ç†å¤±è´¥ä¸å½±å“æµ‹è¯•ç»“æœ
                else:
                    error_messages.append(f"æµ‹è¯•ä»“åº“åˆ›å»ºå¤±è´¥: {result.stderr if result.stderr else 'æœªçŸ¥é”™è¯¯'}")
                    
            except subprocess.TimeoutExpired:
                error_messages.append("æµ‹è¯•ä»“åº“åˆ›å»ºè¶…æ—¶")
            except Exception as e:
                error_messages.append(f"æµ‹è¯•ä»“åº“åˆ›å»ºå¼‚å¸¸: {e}")
            
            duration = time.time() - start_time
            success = tests_passed >= (total_tests * 0.75)  # 75%é€šè¿‡ç‡è®¤ä¸ºæˆåŠŸ
            
            if success:
                return TestResult(
                    "test-environmentå®Œæ•´æµ‹è¯•", True, duration, 
                    f"é€šè¿‡ {tests_passed}/{total_tests} é¡¹æµ‹è¯•", "full"
                )
            else:
                return TestResult(
                    "test-environmentå®Œæ•´æµ‹è¯•", False, duration,
                    f"ä»…é€šè¿‡ {tests_passed}/{total_tests} é¡¹æµ‹è¯•: " + "; ".join(error_messages[:3]), "full"
                )
                
        except Exception as e:
            return TestResult(
                "test-environmentå®Œæ•´æµ‹è¯•", False, time.time() - start_time,
                f"æ‰§è¡Œå¼‚å¸¸: {e}", "full"
            )
            
    def _run_performance_test(self) -> TestResult:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        start_time = time.time()
        
        try:
            performance_script = self.project_root / "test_performance_optimization.py"
            if performance_script.exists():
                result = subprocess.run([
                    sys.executable, str(performance_script)
                ], cwd=self.project_root, capture_output=True, text=True, timeout=300)
                
                duration = time.time() - start_time
                
                if result.returncode == 0:
                    return TestResult(
                        "æ€§èƒ½ä¼˜åŒ–æµ‹è¯•", True, duration, "", "performance"
                    )
                else:
                    return TestResult(
                        "æ€§èƒ½ä¼˜åŒ–æµ‹è¯•", False, duration,
                        result.stderr or "æ€§èƒ½æµ‹è¯•å¤±è´¥", "performance"
                    )
            else:
                return TestResult(
                    "æ€§èƒ½ä¼˜åŒ–æµ‹è¯•", False, 0, "æ€§èƒ½æµ‹è¯•è„šæœ¬ä¸å­˜åœ¨", "performance"
                )
                
        except Exception as e:
            return TestResult(
                "æ€§èƒ½ä¼˜åŒ–æµ‹è¯•", False, time.time() - start_time,
                f"æ‰§è¡Œå¼‚å¸¸: {e}", "performance"
            )
            
    def _run_builtin_mock_tests(self) -> TestResult:
        """è¿è¡Œå†…ç½®æ¨¡æ‹Ÿæµ‹è¯•ï¼ˆé™çº§æ¨¡å¼ï¼‰"""
        start_time = time.time()
        
        try:
            # ç®€å•çš„æ¨¡æ‹Ÿæµ‹è¯•ï¼ŒéªŒè¯æ ¸å¿ƒåŠŸèƒ½
            from core.git_operations import GitOperations
            from utils.config_manager import ProjectConfigManager
            
            # åŸºæœ¬å¯¼å…¥æµ‹è¯•
            git_ops = GitOperations(".")
            config_mgr = ProjectConfigManager(".")
            
            duration = time.time() - start_time
            return TestResult(
                "å†…ç½®æ¨¡æ‹Ÿæµ‹è¯•", True, duration, "", "mock"
            )
            
        except Exception as e:
            return TestResult(
                "å†…ç½®æ¨¡æ‹Ÿæµ‹è¯•", False, time.time() - start_time,
                f"æ¨¡æ‹Ÿæµ‹è¯•å¤±è´¥: {e}", "mock"
            )
            
    def generate_report(self, test_mode: str) -> UnifiedTestReport:
        """ç”Ÿæˆç»Ÿä¸€æµ‹è¯•æŠ¥å‘Š"""
        total_duration = time.time() - self.start_time
        passed_tests = sum(1 for r in self.test_results if r.passed)
        failed_tests = len(self.test_results) - passed_tests
        success_rate = (passed_tests / len(self.test_results) * 100) if self.test_results else 0
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations()
        
        return UnifiedTestReport(
            timestamp=datetime.now().isoformat(),
            total_tests=len(self.test_results),
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            success_rate=success_rate,
            total_duration=total_duration,
            environment_status=self.env_status,
            test_results=self.test_results,
            test_mode=test_mode,
            recommendations=recommendations
        )
        
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæµ‹è¯•å»ºè®®"""
        recommendations = []
        
        # åŸºäºç¯å¢ƒçŠ¶æ€çš„å»ºè®®
        if not self.env_status.test_environment_available:
            recommendations.append("å»ºè®®åˆå§‹åŒ–test-environment submoduleä»¥è·å¾—å®Œæ•´æµ‹è¯•è¦†ç›–")
            
        if not self.env_status.git_available:
            recommendations.append("å»ºè®®å®‰è£…Gitå·¥å…·ä»¥æ”¯æŒå®Œæ•´çš„ç‰ˆæœ¬æ§åˆ¶æµ‹è¯•")
            
        # åŸºäºæµ‹è¯•ç»“æœçš„å»ºè®®  
        failed_results = [r for r in self.test_results if not r.passed]
        if failed_results:
            recommendations.append(f"éœ€è¦ä¿®å¤{len(failed_results)}ä¸ªå¤±è´¥çš„æµ‹è¯•é¡¹")
            
        success_rate = (sum(1 for r in self.test_results if r.passed) / len(self.test_results) * 100) if self.test_results else 0
        if success_rate < 80:
            recommendations.append("æµ‹è¯•æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ ¸å¿ƒåŠŸèƒ½å®ç°")
        elif success_rate < 95:
            recommendations.append("æµ‹è¯•æˆåŠŸç‡è‰¯å¥½ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æå‡ç¨³å®šæ€§")
        else:
            recommendations.append("æµ‹è¯•è¦†ç›–ä¼˜ç§€ï¼Œç³»ç»ŸçŠ¶æ€è‰¯å¥½")
            
        return recommendations
        
    def print_detailed_report(self, report: UnifiedTestReport):
        """æ‰“å°è¯¦ç»†æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ¯ ç»Ÿä¸€æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æµ‹è¯•æ¨¡å¼: {report.test_mode}")
        print(f"   æ€»æµ‹è¯•æ•°: {report.total_tests}")
        print(f"   é€šè¿‡æ•°: {report.passed_tests}")
        print(f"   å¤±è´¥æ•°: {report.failed_tests}")
        print(f"   æˆåŠŸç‡: {report.success_rate:.1f}%")
        print(f"   æ€»è€—æ—¶: {report.total_duration:.2f}ç§’")
        
        # ç¯å¢ƒçŠ¶æ€
        print(f"\nğŸ” ç¯å¢ƒçŠ¶æ€:")
        env_items = [
            ("ä¸»é¡¹ç›®æµ‹è¯•", report.environment_status.main_tests_available),
            ("test-environment", report.environment_status.test_environment_available),
            ("Gitå·¥å…·", report.environment_status.git_available),
            ("Pythonç¯å¢ƒ", report.environment_status.python_available),
            ("é¡¹ç›®ä¾èµ–", report.environment_status.dependencies_available)
        ]
        
        for name, status in env_items:
            icon = "âœ…" if status else "âŒ"
            print(f"   {icon} {name}")
            
        # è¯¦ç»†ç»“æœ
        if report.test_results:
            print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
            for result in report.test_results:
                icon = "âœ…" if result.passed else "âŒ"
                print(f"   {icon} {result.name} ({result.duration:.2f}s)")
                if not result.passed and result.error_message:
                    print(f"      é”™è¯¯: {result.error_message}")
                    
        # å»ºè®®
        if report.recommendations:
            print(f"\nğŸ’¡ å»ºè®®:")
            for rec in report.recommendations:
                print(f"   â€¢ {rec}")
                
        print("=" * 80)
        
    def save_report(self, report: UnifiedTestReport, file_path: str):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            report_dict = asdict(report)
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(report_dict, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {file_path}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Git Merge Orchestrator ç»Ÿä¸€æµ‹è¯•è¿è¡Œå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¿«é€ŸéªŒè¯ (< 30ç§’)
  python unified_test_runner.py --quick
  
  # å®Œæ•´æµ‹è¯•å¥—ä»¶ (10-15åˆ†é’Ÿ)
  python unified_test_runner.py --full
  
  # ç‰¹å®šåœºæ™¯æµ‹è¯•
  python unified_test_runner.py --scenario config,performance
  
  # ä»…ä¸»é¡¹ç›®æµ‹è¯• (test-environmentä¸å¯ç”¨æ—¶)
  python unified_test_runner.py --core-only
  
  # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
  python unified_test_runner.py --full --save-report test_report.json
        """
    )
    
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (< 30ç§’)")
    parser.add_argument("--full", action="store_true", help="å®Œæ•´æµ‹è¯•å¥—ä»¶ (10-15åˆ†é’Ÿ)")
    parser.add_argument("--core-only", action="store_true", help="ä»…ä¸»é¡¹ç›®æµ‹è¯• (é™çº§æ¨¡å¼)")
    parser.add_argument("--scenario", help="æŒ‡å®šåœºæ™¯æµ‹è¯•ï¼Œé€—å·åˆ†éš” (å¦‚: config,performance)")
    parser.add_argument("--save-report", help="ä¿å­˜æµ‹è¯•æŠ¥å‘Šåˆ°æ–‡ä»¶")
    parser.add_argument("--quiet", action="store_true", help="é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºç»“æœ")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = UnifiedTestRunner()
    
    if not args.quiet:
        print("ğŸš€ Git Merge Orchestrator ç»Ÿä¸€æµ‹è¯•ç³»ç»Ÿ")
        print("=" * 60)
        runner.print_environment_status()
    
    # æ ¹æ®å‚æ•°é€‰æ‹©æµ‹è¯•æ¨¡å¼
    if args.quick:
        test_mode = "quick"
        runner.test_results = runner.run_quick_test()
    elif args.full:
        test_mode = "full"
        runner.test_results = runner.run_full_test()
    elif args.core_only:
        test_mode = "core_only"
        runner.test_results = runner.run_core_only_test()
    elif args.scenario:
        scenarios = [s.strip() for s in args.scenario.split(",")]
        test_mode = f"scenario_{'+'.join(scenarios)}"
        runner.test_results = runner.run_scenario_test(scenarios)
    else:
        # é»˜è®¤ï¼šæ ¹æ®ç¯å¢ƒæ™ºèƒ½é€‰æ‹©
        if runner.env_status.test_environment_available:
            test_mode = "quick"
            runner.test_results = runner.run_quick_test()
        else:
            test_mode = "core_only"
            runner.test_results = runner.run_core_only_test()
            print("âš ï¸ test-environmentä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§æ¨¡å¼")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = runner.generate_report(test_mode)
    
    if not args.quiet:
        runner.print_detailed_report(report)
    else:
        # é™é»˜æ¨¡å¼åªæ˜¾ç¤ºå…³é”®ä¿¡æ¯
        success_rate = report.success_rate
        print(f"æµ‹è¯•ç»“æœ: {report.passed_tests}/{report.total_tests} é€šè¿‡ ({success_rate:.1f}%)")
        if report.failed_tests > 0:
            sys.exit(1)
    
    # ä¿å­˜æŠ¥å‘Š
    if args.save_report:
        runner.save_report(report, args.save_report)
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    if report.failed_tests > 0:
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿è¡Œå™¨å¼‚å¸¸: {e}")
        sys.exit(1)