"""
Git Merge Orchestrator - æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨
å®ç°LRUç¼“å­˜å’Œå¤šå±‚ç¼“å­˜ç­–ç•¥ä»¥ä¼˜åŒ–å¤§å‹é¡¹ç›®æ€§èƒ½
"""

import json
import hashlib
import threading
from collections import OrderedDict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, Union


class LRUCache:
    """LRU (Least Recently Used) ç¼“å­˜å®ç°"""

    def __init__(self, capacity: int = 1000):
        self.capacity = capacity
        self.cache = OrderedDict()
        self.lock = threading.Lock()

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        with self.lock:
            if key in self.cache:
                # ç§»åŠ¨åˆ°æœ€åï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
                self.cache.move_to_end(key)
                return self.cache[key]
            return None

    def put(self, key: str, value: Any) -> None:
        """è®¾ç½®ç¼“å­˜å€¼"""
        with self.lock:
            if key in self.cache:
                # æ›´æ–°ç°æœ‰å€¼
                self.cache.move_to_end(key)
            elif len(self.cache) >= self.capacity:
                # åˆ é™¤æœ€ä¹…æœªä½¿ç”¨çš„é¡¹
                self.cache.popitem(last=False)

            self.cache[key] = value

    def remove(self, key: str) -> bool:
        """ç§»é™¤ç¼“å­˜é¡¹"""
        with self.lock:
            return self.cache.pop(key, None) is not None

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()

    def size(self) -> int:
        """è·å–ç¼“å­˜å¤§å°"""
        with self.lock:
            return len(self.cache)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                "size": len(self.cache),
                "capacity": self.capacity,
                "usage_rate": len(self.cache) / self.capacity * 100,
            }


class SmartCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ - å¤šå±‚ç¼“å­˜ç­–ç•¥"""

    def __init__(self, repo_path: str):
        self.repo_path = Path(repo_path)
        self.cache_dir = self.repo_path / ".merge_work" / "cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # å¤šå±‚ç¼“å­˜
        self.memory_cache = LRUCache(1000)  # å†…å­˜ç¼“å­˜
        self.file_cache_path = self.cache_dir / "persistent_cache.json"

        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            "hits": 0,
            "misses": 0,
            "puts": 0,
            "memory_hits": 0,
            "file_hits": 0,
        }

        self.lock = threading.Lock()

    def _generate_cache_key(self, namespace: str, identifier: Union[str, Dict]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        if isinstance(identifier, dict):
            # å¯¹å­—å…¸è¿›è¡Œåºåˆ—åŒ–å’Œå“ˆå¸Œ
            content = json.dumps(identifier, sort_keys=True)
        else:
            content = str(identifier)

        return f"{namespace}:{hashlib.md5(content.encode()).hexdigest()}"

    def get(
        self, namespace: str, identifier: Union[str, Dict], max_age_hours: int = 24
    ) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_cache_key(namespace, identifier)

        # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
        memory_result = self.memory_cache.get(cache_key)
        if memory_result is not None:
            cache_data, timestamp = memory_result
            if self._is_cache_valid(timestamp, max_age_hours):
                with self.lock:
                    self.stats["hits"] += 1
                    self.stats["memory_hits"] += 1
                return cache_data

        # 2. æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
        file_result = self._get_from_file_cache(cache_key)
        if file_result is not None:
            cache_data, timestamp = file_result
            if self._is_cache_valid(timestamp, max_age_hours):
                # æå‡åˆ°å†…å­˜ç¼“å­˜
                self.memory_cache.put(cache_key, (cache_data, timestamp))
                with self.lock:
                    self.stats["hits"] += 1
                    self.stats["file_hits"] += 1
                return cache_data

        # ç¼“å­˜æœªå‘½ä¸­
        with self.lock:
            self.stats["misses"] += 1
        return None

    def put(self, namespace: str, identifier: Union[str, Dict], data: Any) -> None:
        """å­˜å‚¨ç¼“å­˜æ•°æ®"""
        cache_key = self._generate_cache_key(namespace, identifier)
        timestamp = datetime.now().isoformat()

        # å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜
        self.memory_cache.put(cache_key, (data, timestamp))

        # å¼‚æ­¥å­˜å‚¨åˆ°æ–‡ä»¶ç¼“å­˜
        self._put_to_file_cache_async(cache_key, data, timestamp)

        with self.lock:
            self.stats["puts"] += 1

    def _is_cache_valid(self, timestamp: str, max_age_hours: int) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        try:
            cached_time = datetime.fromisoformat(timestamp)
            age = datetime.now() - cached_time
            return age < timedelta(hours=max_age_hours)
        except:
            return False

    def _get_from_file_cache(self, cache_key: str) -> Optional[tuple]:
        """ä»æ–‡ä»¶ç¼“å­˜è·å–æ•°æ®"""
        try:
            if not self.file_cache_path.exists():
                return None

            with open(self.file_cache_path, "r", encoding="utf-8") as f:
                file_cache = json.load(f)

            if cache_key in file_cache:
                cache_entry = file_cache[cache_key]
                return cache_entry["data"], cache_entry["timestamp"]

        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶ç¼“å­˜è¯»å–å¤±è´¥: {e}")

        return None

    def _put_to_file_cache_async(
        self, cache_key: str, data: Any, timestamp: str
    ) -> None:
        """å¼‚æ­¥å­˜å‚¨åˆ°æ–‡ä»¶ç¼“å­˜"""

        def save_to_file():
            try:
                # è¯»å–ç°æœ‰ç¼“å­˜
                file_cache = {}
                if self.file_cache_path.exists():
                    with open(self.file_cache_path, "r", encoding="utf-8") as f:
                        file_cache = json.load(f)

                # æ·»åŠ æ–°ç¼“å­˜é¡¹
                file_cache[cache_key] = {"data": data, "timestamp": timestamp}

                # é™åˆ¶æ–‡ä»¶ç¼“å­˜å¤§å°
                if len(file_cache) > 5000:
                    # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                    sorted_items = sorted(
                        file_cache.items(), key=lambda x: x[1]["timestamp"]
                    )
                    # ä¿ç•™æœ€æ–°çš„4000é¡¹
                    file_cache = dict(sorted_items[-4000:])

                # å†™å›æ–‡ä»¶
                with open(self.file_cache_path, "w", encoding="utf-8") as f:
                    json.dump(file_cache, f, ensure_ascii=False, indent=2)

            except Exception as e:
                print(f"âš ï¸ æ–‡ä»¶ç¼“å­˜å†™å…¥å¤±è´¥: {e}")

        # ä½¿ç”¨çº¿ç¨‹å¼‚æ­¥æ‰§è¡Œ
        import threading

        threading.Thread(target=save_to_file, daemon=True).start()

    def clear_expired(self, max_age_hours: int = 24) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        cleared_count = 0

        try:
            if not self.file_cache_path.exists():
                return 0

            with open(self.file_cache_path, "r", encoding="utf-8") as f:
                file_cache = json.load(f)

            # è¿‡æ»¤æœ‰æ•ˆç¼“å­˜
            valid_cache = {}
            for key, entry in file_cache.items():
                if self._is_cache_valid(entry["timestamp"], max_age_hours):
                    valid_cache[key] = entry
                else:
                    cleared_count += 1

            # å†™å›æ–‡ä»¶
            with open(self.file_cache_path, "w", encoding="utf-8") as f:
                json.dump(valid_cache, f, ensure_ascii=False, indent=2)

        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")

        return cleared_count

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        memory_stats = self.memory_cache.get_stats()

        with self.lock:
            total_requests = self.stats["hits"] + self.stats["misses"]
            hit_rate = (
                (self.stats["hits"] / total_requests * 100) if total_requests > 0 else 0
            )

            return {
                "total_requests": total_requests,
                "hit_rate": round(hit_rate, 2),
                "memory_cache": memory_stats,
                "stats": dict(self.stats),
            }

    def warm_up_cache(self, file_paths: list, contributor_analyzer) -> None:
        """é¢„çƒ­ç¼“å­˜ - é¢„å…ˆè®¡ç®—å¸¸ç”¨æ•°æ®"""
        print(f"ğŸ”¥ å¼€å§‹ç¼“å­˜é¢„çƒ­ï¼Œå¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")

        # åˆ†æ‰¹é¢„çƒ­ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
        batch_size = 100
        warmed_count = 0

        for i in range(0, len(file_paths), batch_size):
            batch_files = file_paths[i : i + batch_size]

            # é¢„çƒ­æ–‡ä»¶è´¡çŒ®è€…ç¼“å­˜
            for file_path in batch_files:
                cache_key = f"file_contributors:{file_path}"
                if self.memory_cache.get(cache_key) is None:
                    try:
                        contributors = contributor_analyzer.get_file_contributors(
                            file_path
                        )
                        self.put("file_contributors", file_path, contributors)
                        warmed_count += 1
                    except Exception as e:
                        print(f"âš ï¸ é¢„çƒ­æ–‡ä»¶ {file_path} å¤±è´¥: {e}")

            if i % (batch_size * 5) == 0:  # æ¯500ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                print(
                    f"  ğŸ”¥ å·²é¢„çƒ­ {min(i + batch_size, len(file_paths))}/{len(file_paths)} ä¸ªæ–‡ä»¶..."
                )

        print(f"âœ… ç¼“å­˜é¢„çƒ­å®Œæˆï¼ŒæˆåŠŸé¢„çƒ­ {warmed_count} ä¸ªæ–‡ä»¶")


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
_cache_managers = {}


def get_cache_manager(repo_path: str) -> SmartCacheManager:
    """è·å–ç¼“å­˜ç®¡ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    if repo_path not in _cache_managers:
        _cache_managers[repo_path] = SmartCacheManager(repo_path)
    return _cache_managers[repo_path]


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    cache = SmartCacheManager(".")

    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    cache.put("test", "key1", {"data": "value1"})
    result = cache.get("test", "key1")
    print(f"ç¼“å­˜æµ‹è¯•ç»“æœ: {result}")

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = cache.get_stats()
    print(f"ç¼“å­˜ç»Ÿè®¡: {stats}")
