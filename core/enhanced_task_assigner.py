"""
Git Merge Orchestrator - å¢å¼ºä»»åŠ¡åˆ†é…å™¨ï¼ˆv2.3ï¼‰
ç»“åˆå¢å¼ºçš„è´¡çŒ®è€…åˆ†æå™¨ï¼Œæ”¯æŒè¡Œæ•°æƒé‡åˆ†æçš„æ™ºèƒ½ä»»åŠ¡åˆ†é…
"""

from datetime import datetime
from config import (
    DEFAULT_MAX_TASKS_PER_PERSON,
    DEFAULT_ACTIVE_MONTHS,
    ENHANCED_CONTRIBUTOR_ANALYSIS,
)
from utils.performance_monitor import performance_monitor, global_performance_stats
from .enhanced_contributor_analyzer import EnhancedContributorAnalyzer


class EnhancedTaskAssigner:
    """å¢å¼ºçš„ä»»åŠ¡åˆ†é…å™¨ - æ”¯æŒå¤šç»´åº¦æƒé‡åˆ†æ"""

    def __init__(self, git_ops, fallback_assigner=None):
        self.git_ops = git_ops
        self.enhanced_analyzer = EnhancedContributorAnalyzer(git_ops)
        self.fallback_assigner = fallback_assigner  # å¯é€‰çš„å›é€€åˆ†é…å™¨

        # æ£€æŸ¥å¢å¼ºåŠŸèƒ½æ˜¯å¦å¯ç”¨
        self.enhanced_enabled = self.enhanced_analyzer.is_enabled()

        if not self.enhanced_enabled:
            print("âš ï¸  å¢å¼ºä»»åŠ¡åˆ†é…å™¨æœªå¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€åˆ†é…é€»è¾‘")

    def is_enhanced_enabled(self):
        """æ£€æŸ¥å¢å¼ºåŠŸèƒ½æ˜¯å¦å¯ç”¨"""
        return self.enhanced_enabled

    @performance_monitor("å¢å¼ºæ™ºèƒ½ä»»åŠ¡åˆ†é…")
    def enhanced_auto_assign_tasks(
        self,
        plan,
        exclude_authors=None,
        max_tasks_per_person=None,
        enable_line_analysis=True,
        include_fallback=True,
    ):
        """
        å¢å¼ºçš„æ™ºèƒ½ä»»åŠ¡åˆ†é…
        
        Args:
            plan: åˆå¹¶è®¡åˆ’å¯¹è±¡
            exclude_authors: æ’é™¤çš„ä½œè€…åˆ—è¡¨
            max_tasks_per_person: æ¯äººæœ€å¤§ä»»åŠ¡æ•°
            enable_line_analysis: æ˜¯å¦å¯ç”¨è¡Œæ•°æƒé‡åˆ†æ
            include_fallback: æ˜¯å¦åŒ…å«å›é€€åˆ†é…
            
        Returns:
            tuple: (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡, åˆ†é…ç»Ÿè®¡)
        """
        exclude_authors = exclude_authors or []
        max_tasks_per_person = max_tasks_per_person or DEFAULT_MAX_TASKS_PER_PERSON
        start_time = datetime.now()

        print("ğŸš€ å¯åŠ¨å¢å¼ºæ™ºèƒ½ä»»åŠ¡åˆ†é…...")

        # æ£€æŸ¥åŠŸèƒ½çŠ¶æ€
        if not self.enhanced_enabled:
            print("âš ï¸  å¢å¼ºåŠŸèƒ½æœªå¯ç”¨ï¼Œä½¿ç”¨å›é€€åˆ†é…å™¨")
            if self.fallback_assigner:
                return self.fallback_assigner.turbo_auto_assign_tasks(
                    plan, exclude_authors, max_tasks_per_person, include_fallback
                )
            else:
                return self._basic_assignment_fallback(
                    plan, exclude_authors, max_tasks_per_person
                )

        # æ˜¾ç¤ºç®—æ³•ä¿¡æ¯
        algorithm_config = self.enhanced_analyzer.get_algorithm_config()
        algorithm_type = ENHANCED_CONTRIBUTOR_ANALYSIS.get(
            "assignment_algorithm", "comprehensive"
        )
        print(f"ğŸ§  ä½¿ç”¨ {algorithm_type} ç®—æ³•è¿›è¡Œæ™ºèƒ½åˆ†æ")
        print(f"âš¡ è¡Œæ•°æƒé‡åˆ†æ: {'å¯ç”¨' if enable_line_analysis else 'ç¦ç”¨'}")

        # ç‰¹æ€§è¯´æ˜
        features = []
        if algorithm_config.get("use_line_weight", False):
            features.append("è¡Œæ•°æƒé‡")
        if algorithm_config.get("use_time_weight", False):
            features.append("æ—¶é—´è¡°å‡")
        if algorithm_config.get("use_consistency_weight", False):
            features.append("ä¸€è‡´æ€§è¯„åˆ†")
        if algorithm_config.get("use_file_relationship", False):
            features.append("æ–‡ä»¶å…³è”")

        if features:
            print(f"ğŸ¯ å¯ç”¨ç‰¹æ€§: {', '.join(features)}")

        # è·å–æ´»è·ƒè´¡çŒ®è€…
        active_contributors = self.git_ops.get_active_contributors(
            DEFAULT_ACTIVE_MONTHS
        )

        # å¤„ç†ä¸åŒçš„å¤„ç†æ¨¡å¼ï¼ˆå…¼å®¹å­—å…¸å’Œå¯¹è±¡ï¼‰
        if isinstance(plan, dict):
            processing_mode = plan.get("processing_mode", "file_level")
        else:
            processing_mode = getattr(plan, "processing_mode", "file_level")

        if processing_mode == "file_level":
            return self._assign_file_level_enhanced(
                plan,
                exclude_authors,
                max_tasks_per_person,
                enable_line_analysis,
                active_contributors,
                start_time,
                include_fallback,
            )
        else:
            return self._assign_group_level_enhanced(
                plan,
                exclude_authors,
                max_tasks_per_person,
                enable_line_analysis,
                active_contributors,
                start_time,
                include_fallback,
            )

    def _assign_file_level_enhanced(
        self,
        plan,
        exclude_authors,
        max_tasks_per_person,
        enable_line_analysis,
        active_contributors,
        start_time,
        include_fallback=True,
    ):
        """æ–‡ä»¶çº§å¢å¼ºåˆ†é…"""
        # å…¼å®¹å­—å…¸å’Œå¯¹è±¡ä¸¤ç§æ•°æ®ç»“æ„
        if isinstance(plan, dict):
            files = plan.get("files", [])
        else:
            files = getattr(plan, "files", [])
            
        if not files:
            print("âŒ æ— æ–‡ä»¶éœ€è¦åˆ†é…")
            return 0, 0, {}

        print(f"ğŸ“ å‡†å¤‡åˆ†é… {len(files)} ä¸ªæ–‡ä»¶...")

        # æå–æ–‡ä»¶è·¯å¾„
        file_paths = [file_info.get("path", "") for file_info in files]
        file_paths = [path for path in file_paths if path]

        if not file_paths:
            print("âŒ æ— æœ‰æ•ˆæ–‡ä»¶è·¯å¾„")
            return 0, 0, {}

        # å¯¼å…¥datetimeæ¨¡å—
        from datetime import datetime
        
        # ä¸€æ¬¡æ€§è·å–æ´»è·ƒè´¡çŒ®è€…åˆ—è¡¨ï¼ˆé¿å…N+1æŸ¥è¯¢ï¼‰
        active_contributors_start = datetime.now()
        active_contributors_set = set(active_contributors)
        active_contributors_time = (datetime.now() - active_contributors_start).total_seconds()
        print(f"âš¡ æ´»è·ƒè´¡çŒ®è€…åˆ—è¡¨å‡†å¤‡: {active_contributors_time:.3f}s ({len(active_contributors_set)} äºº)")

        # é˜¶æ®µ1: æ‰¹é‡åˆ†ææ–‡ä»¶è´¡çŒ®è€…
        analysis_start = datetime.now()
        print(f"ğŸ” æ­£åœ¨è¿›è¡Œæ‰¹é‡å¢å¼ºè´¡çŒ®è€…åˆ†æ... ({len(file_paths)} ä¸ªæ–‡ä»¶)")
        print("âš¡ å¯ç”¨ç‰¹æ€§: è¡Œæ•°æƒé‡ã€æ—¶é—´è¡°å‡ã€ä¸€è‡´æ€§è¯„åˆ†")
        
        batch_contributors = self.enhanced_analyzer.analyze_contributors_batch(
            file_paths, enable_line_analysis=enable_line_analysis
        )
        
        analysis_time = (datetime.now() - analysis_start).total_seconds()
        print(f"âœ… å¢å¼ºè´¡çŒ®è€…åˆ†æå®Œæˆ: {analysis_time:.2f}s ({analysis_time/len(file_paths)*1000:.1f}ms/æ–‡ä»¶)")

        # é˜¶æ®µ2: æ‰¹é‡å†³ç­–é¢„è®¡ç®—
        decision_start = datetime.now()
        print(f"ğŸ¯ æ­£åœ¨è¿›è¡Œæ‰¹é‡å†³ç­–é¢„è®¡ç®—...")
        
        decisions = self.enhanced_analyzer.compute_final_decision_batch(
            batch_contributors, active_contributors_set
        )
        
        decision_time = (datetime.now() - decision_start).total_seconds()
        print(f"âœ… æ‰¹é‡å†³ç­–é¢„è®¡ç®—å®Œæˆ: {decision_time:.2f}s")

        # é˜¶æ®µ3: è´Ÿè½½å‡è¡¡åˆ†é…
        assignment_start = datetime.now()
        print(f"âš–ï¸ å¼€å§‹è´Ÿè½½å‡è¡¡åˆ†é…...")
        
        final_assignments, person_workload, load_balance_stats = self.apply_load_balanced_assignment(
            decisions, exclude_authors, max_tasks_per_person
        )
        
        assignment_time = (datetime.now() - assignment_start).total_seconds()
        print(f"âœ… è´Ÿè½½å‡è¡¡åˆ†é…å®Œæˆ: {assignment_time:.2f}s")

        # é˜¶æ®µ4: åº”ç”¨åˆ†é…ç»“æœåˆ°æ–‡ä»¶å¯¹è±¡
        application_start = datetime.now()
        success_count = 0
        failed_count = 0
        
        assignment_stats = {
            "total_files": len(files),
            "analyzed_files": len(batch_contributors),
            "active_contributors": len(active_contributors),
            "assignment_reasons": {},
            "algorithm_type": ENHANCED_CONTRIBUTOR_ANALYSIS.get(
                "assignment_algorithm", "comprehensive"
            ),
        }

        for file_info in files:
            file_path = file_info.get("path", "")
            if not file_path:
                failed_count += 1
                continue
                
            if file_path in final_assignments:
                assigned_author, assignment_reason = final_assignments[file_path]
                
                if assigned_author:
                    file_info["assignee"] = assigned_author
                    file_info["status"] = "assigned"
                    file_info["assignment_reason"] = assignment_reason
                    assignment_stats["assignment_reasons"][file_path] = assignment_reason
                    success_count += 1
                else:
                    file_info["assignee"] = "æœªåˆ†é…"
                    file_info["status"] = "pending"
                    file_info["assignment_reason"] = assignment_reason
                    failed_count += 1
            else:
                file_info["assignee"] = "æœªåˆ†é…"
                file_info["status"] = "pending"
                file_info["assignment_reason"] = "åˆ†é…å¤„ç†å¼‚å¸¸"
                failed_count += 1
        
        application_time = (datetime.now() - application_start).total_seconds()
        print(f"âœ… åˆ†é…ç»“æœåº”ç”¨å®Œæˆ: {application_time:.2f}s")

        # æ€»ä½“ç»Ÿè®¡å’Œæ€§èƒ½è®°å½•
        elapsed = (datetime.now() - start_time).total_seconds()
        
        # æ„å»ºè¯¦ç»†æ€§èƒ½è®°å½•ï¼ˆæ–°æ¶æ„çš„è¯¦ç»†åˆ†è§£ï¼‰
        perf_log = {
            # æ–°æ¶æ„çš„è¯¦ç»†é˜¶æ®µæ—¶é—´
            'analysis_phase_time': analysis_time,
            'decision_phase_time': decision_time,
            'assignment_phase_time': assignment_time, 
            'application_phase_time': application_time,
            'active_contributors_prep_time': active_contributors_time,
            'total_execution_time': elapsed,
            'other_processing_time': elapsed - analysis_time - decision_time - assignment_time - application_time,
            
            # æ–‡ä»¶å¤„ç†ç»Ÿè®¡
            'total_files': len(files),
            'files_to_process': len([f for f in files if not f.get("assignee", "").strip()]),
            'success_count': success_count,
            'failed_count': failed_count,
            
            # è´¡çŒ®è€…ç»Ÿè®¡ï¼ˆä½¿ç”¨æ–°çš„å·¥ä½œè´Ÿè½½åˆ†å¸ƒï¼‰
            'contributors_count': len(person_workload),
            'workload_distribution': dict(person_workload),
            
            # æ–°æ¶æ„æ€§èƒ½æŒ‡æ ‡
            'avg_time_per_file_ms': (elapsed / max(success_count + failed_count, 1)) * 1000,
            'decision_to_analysis_ratio': decision_time / analysis_time if analysis_time > 0 else 0,
            'assignment_to_decision_ratio': assignment_time / decision_time if decision_time > 0 else 0,
            'success_rate': success_count / max(success_count + failed_count, 1) * 100,
            
            # è´Ÿè½½å‡è¡¡ç»Ÿè®¡
            'load_balance_stats': load_balance_stats,
            'architecture_version': '2.3_optimized'
        }
        
        # ä¿å­˜æ€§èƒ½æ—¥å¿—
        self._save_enhanced_performance_log(perf_log)

        print(f"\nâœ… å¢å¼ºä»»åŠ¡åˆ†é…å®Œæˆ (æ–°ä¼˜åŒ–æ¶æ„v2.3)!")
        print(f"ğŸ“Š åˆ†é…ç»Ÿè®¡: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}, ç”¨æ—¶ {elapsed:.2f}s")
        print(f"ğŸ‘¥ æ¶‰åŠ {len(person_workload)} ä½è´¡çŒ®è€…")
        
        # æ–°æ¶æ„è¯¦ç»†æ€§èƒ½åˆ†æ
        if elapsed > 10:
            print(f"\nğŸ” è¯¦ç»†æ€§èƒ½åˆ†æ (æ€»æ—¶é—´ {elapsed:.1f}s):")
            print(f"  ğŸ§ª åˆ†æé˜¶æ®µ: {analysis_time:.1f}s ({analysis_time/elapsed*100:.1f}%)")
            print(f"  ğŸ¯ å†³ç­–è®¡ç®—: {decision_time:.1f}s ({decision_time/elapsed*100:.1f}%)")
            print(f"  âš–ï¸ è´Ÿè½½å‡è¡¡: {assignment_time:.1f}s ({assignment_time/elapsed*100:.1f}%)")
            print(f"  ğŸ“‹ ç»“æœåº”ç”¨: {application_time:.1f}s ({application_time/elapsed*100:.1f}%)")
            
            other_time = perf_log['other_processing_time']
            if other_time > 0.5:
                print(f"  ğŸ“¦ å…¶ä»–å¤„ç†: {other_time:.1f}s ({other_time/elapsed*100:.1f}%)")
            
            # æ–°æ¶æ„æ€§èƒ½å»ºè®®
            if decision_time > analysis_time:
                print(f"  ğŸ’¡ å»ºè®®: å†³ç­–è®¡ç®—è€—æ—¶è¾ƒå¤šï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥ç¼“å­˜ä¼˜åŒ–")
            if assignment_time > decision_time * 2:
                print(f"  ğŸ’¡ å»ºè®®: è´Ÿè½½å‡è¡¡ç®—æ³•å¯è¿›ä¸€æ­¥ä¼˜åŒ–")
            if perf_log['avg_time_per_file_ms'] > 20:
                print(f"  ğŸ’¡ å»ºè®®: å¹³å‡æ–‡ä»¶å¤„ç†æ—¶é—´ ({perf_log['avg_time_per_file_ms']:.1f}ms) ä»æœ‰ä¼˜åŒ–ç©ºé—´")
            else:
                print(f"  âœ¨ æ€§èƒ½è¡¨ç°: å¹³å‡æ–‡ä»¶å¤„ç†æ—¶é—´ {perf_log['avg_time_per_file_ms']:.1f}ms (ä¼˜ç§€)")

        # æ˜¾ç¤ºè´Ÿè½½åˆ†å¸ƒï¼ˆä½¿ç”¨æ–°çš„æ•°æ®æºï¼‰
        self._show_workload_distribution(person_workload)

        assignment_stats.update(
            {
                "success_count": success_count,
                "failed_count": failed_count,
                "elapsed_time": elapsed,
                "contributors_involved": len(person_workload),
                "workload_distribution": person_workload,
                "architecture_version": "2.3_optimized",
                "performance_breakdown": {
                    "analysis_time": analysis_time,
                    "decision_time": decision_time,
                    "assignment_time": assignment_time,
                    "application_time": application_time
                }
            }
        )

        return success_count, failed_count, assignment_stats

    def _assign_group_level_enhanced(
        self,
        plan,
        exclude_authors,
        max_tasks_per_person,
        enable_line_analysis,
        active_contributors,
        start_time,
        include_fallback=True,
    ):
        """ç»„çº§å¢å¼ºåˆ†é…ï¼ˆå‘åå…¼å®¹ï¼‰"""
        # å…¼å®¹å­—å…¸å’Œå¯¹è±¡ä¸¤ç§æ•°æ®ç»“æ„
        if isinstance(plan, dict):
            groups = plan.get("groups", [])
        else:
            groups = getattr(plan, "groups", [])
            
        if not groups:
            print("âŒ æ— åˆ†ç»„éœ€è¦åˆ†é…")
            return 0, 0, {}

        print(f"ğŸ“ å‡†å¤‡åˆ†é… {len(groups)} ä¸ªç»„...")

        success_count = 0
        failed_count = 0
        assignment_stats = {
            "total_groups": len(groups),
            "active_contributors": len(active_contributors),
            "assignment_reasons": {},
            "algorithm_type": ENHANCED_CONTRIBUTOR_ANALYSIS.get(
                "assignment_algorithm", "comprehensive"
            ),
        }

        person_task_count = {}

        for group in groups:
            group_name = group.get("name", "")
            group_files = group.get("files", [])

            if not group_files:
                failed_count += 1
                continue

            # åˆ†æç»„å†…æ–‡ä»¶çš„è´¡çŒ®è€…
            print(f"ğŸ” åˆ†æç»„ {group_name}: {len(group_files)} ä¸ªæ–‡ä»¶...")
            batch_contributors = self.enhanced_analyzer.analyze_contributors_batch(
                group_files, enable_line_analysis=enable_line_analysis
            )

            # åˆå¹¶ç»„çº§è´¡çŒ®è€…ç»Ÿè®¡
            group_contributors = self._merge_group_contributors(batch_contributors)

            # è·å–æœ€ä½³åˆ†é…å¯¹è±¡
            best_author, author_info, reason = self.enhanced_analyzer.get_best_assignee(
                group_contributors, exclude_inactive=True
            )

            if not best_author or best_author in exclude_authors:
                if include_fallback:
                    best_author, reason = self._fallback_group_assignment(
                        group_name, exclude_authors
                    )

                if not best_author:
                    group["assignee"] = "æœªåˆ†é…"
                    group["status"] = "pending"
                    group["assignment_reason"] = "æ— å¯ç”¨è´¡çŒ®è€…"
                    failed_count += 1
                    continue

            # æ£€æŸ¥ä»»åŠ¡æ•°é‡é™åˆ¶
            current_tasks = person_task_count.get(best_author, 0)
            if current_tasks >= max_tasks_per_person:
                alternative_assigned = self._find_alternative_assignee(
                    group_contributors,
                    exclude_authors,
                    person_task_count,
                    max_tasks_per_person,
                )

                if alternative_assigned:
                    best_author, reason = alternative_assigned
                else:
                    group["assignee"] = "æœªåˆ†é…"
                    group["status"] = "pending"
                    group["assignment_reason"] = "è¶…å‡ºä»»åŠ¡é™é¢"
                    failed_count += 1
                    continue

            # æ‰§è¡Œåˆ†é…
            group["assignee"] = best_author
            group["status"] = "assigned"
            group["assignment_reason"] = reason

            person_task_count[best_author] = person_task_count.get(best_author, 0) + 1
            assignment_stats["assignment_reasons"][group_name] = reason
            success_count += 1

        # å®Œæˆç»Ÿè®¡
        elapsed = (datetime.now() - start_time).total_seconds()

        print(f"\nâœ… å¢å¼ºç»„çº§ä»»åŠ¡åˆ†é…å®Œæˆ!")
        print(f"ğŸ“Š åˆ†é…ç»Ÿè®¡: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}, ç”¨æ—¶ {elapsed:.2f}s")
        print(f"ğŸ‘¥ æ¶‰åŠ {len(person_task_count)} ä½è´¡çŒ®è€…")

        self._show_workload_distribution(person_task_count)

        assignment_stats.update(
            {
                "success_count": success_count,
                "failed_count": failed_count,
                "elapsed_time": elapsed,
                "contributors_involved": len(person_task_count),
                "workload_distribution": person_task_count,
            }
        )

        return success_count, failed_count, assignment_stats

    def apply_load_balanced_assignment(self, decisions, exclude_authors=None, max_tasks_per_person=None):
        """
        åº”ç”¨è´Ÿè½½å‡è¡¡çš„æ™ºèƒ½åˆ†é…
        
        Args:
            decisions: æ‰¹é‡å†³ç­–ç»“æœå­—å…¸
            exclude_authors: æ’é™¤çš„ä½œè€…åˆ—è¡¨
            max_tasks_per_person: æ¯äººæœ€å¤§ä»»åŠ¡æ•°
            
        Returns:
            tuple: (æœ€ç»ˆåˆ†é…ç»“æœ, å·¥ä½œè´Ÿè½½åˆ†å¸ƒ, åˆ†é…ç»Ÿè®¡)
        """
        from datetime import datetime
        
        exclude_authors = exclude_authors or []
        max_tasks_per_person = max_tasks_per_person or DEFAULT_MAX_TASKS_PER_PERSON
        
        start_time = datetime.now()
        print(f"âš–ï¸ å¼€å§‹è´Ÿè½½å‡è¡¡åˆ†é…: {len(decisions)} ä¸ªæ–‡ä»¶...")
        
        person_workload = {}
        final_assignments = {}
        assignment_stats = {
            'total_files': len(decisions),
            'primary_assignments': 0,
            'alternative_assignments': 0,
            'failed_assignments': 0,
            'exclude_count': 0,
            'overload_count': 0
        }
        
        # åˆ›å»ºæŒ‰ä¼˜å…ˆçº§æ’åºçš„æ–‡ä»¶åˆ—è¡¨ï¼ˆåŸºäºä¸»è¦å€™é€‰äººçš„åˆ†æ•°ï¼‰
        prioritized_files = []
        for file_path, decision in decisions.items():
            if decision['primary']:
                primary_score = decision['primary'][1].get('enhanced_score', 0)
                prioritized_files.append((file_path, decision, primary_score))
            else:
                # æ— å¯åˆ†é…å¯¹è±¡çš„æ–‡ä»¶æ”¾åœ¨æœ€å
                prioritized_files.append((file_path, decision, -1))
        
        # æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
        prioritized_files.sort(key=lambda x: x[2], reverse=True)
        
        print(f"ğŸ“Š ä¼˜å…ˆçº§æ’åºå®Œæˆ: {len([f for f in prioritized_files if f[2] > 0])} ä¸ªæ–‡ä»¶æœ‰å¯åˆ†é…å¯¹è±¡")
        
        # æ‰§è¡Œæ™ºèƒ½åˆ†é…
        processed_count = 0
        for file_path, decision, score in prioritized_files:
            assigned = False
            assignment_reason = None
            selected_author = None
            
            # å°è¯•ä¸»è¦å€™é€‰äºº
            if decision['primary']:
                primary_author, primary_info = decision['primary']
                
                if primary_author in exclude_authors:
                    assignment_stats['exclude_count'] += 1
                elif person_workload.get(primary_author, 0) >= max_tasks_per_person:
                    assignment_stats['overload_count'] += 1
                else:
                    # å¯ä»¥ä½¿ç”¨ä¸»è¦å€™é€‰äºº
                    selected_author = primary_author
                    assignment_reason = decision['reason']
                    assignment_stats['primary_assignments'] += 1
                    assigned = True
            
            # å¦‚æœä¸»è¦å€™é€‰äººä¸å¯ç”¨ï¼Œå°è¯•å¤‡é€‰å€™é€‰äºº
            if not assigned and decision['alternatives']:
                for alt_author, alt_info in decision['alternatives']:
                    if alt_author not in exclude_authors and person_workload.get(alt_author, 0) < max_tasks_per_person:
                        selected_author = alt_author
                        assignment_reason = self.enhanced_analyzer._generate_assignment_reason(alt_author, alt_info) + " (è´Ÿè½½å‡è¡¡)"
                        assignment_stats['alternative_assignments'] += 1
                        assigned = True
                        break
            
            # è®°å½•åˆ†é…ç»“æœ
            if assigned:
                final_assignments[file_path] = (selected_author, assignment_reason)
                person_workload[selected_author] = person_workload.get(selected_author, 0) + 1
            else:
                # å°è¯•æœ€åçš„å›é€€åˆ†é… - ä½¿ç”¨ä»»åŠ¡æœ€å°‘çš„è´¡çŒ®è€…
                fallback_author = self._find_least_loaded_contributor(person_workload, max_tasks_per_person)
                if fallback_author:
                    final_assignments[file_path] = (fallback_author, "è´Ÿè½½å‡è¡¡å›é€€åˆ†é…")
                    person_workload[fallback_author] = person_workload.get(fallback_author, 0) + 1
                    assignment_stats['alternative_assignments'] += 1
                    assigned = True
                else:
                    final_assignments[file_path] = (None, decision.get('reason', 'æ— å¯ç”¨åˆ†é…å¯¹è±¡'))
                    assignment_stats['failed_assignments'] += 1
            
            processed_count += 1
            
            # è¿›åº¦æ˜¾ç¤ºï¼ˆæ¯10%æ˜¾ç¤ºä¸€æ¬¡ï¼‰
            if processed_count % max(1, len(prioritized_files) // 10) == 0:
                progress = (processed_count / len(prioritized_files)) * 100
                elapsed = (datetime.now() - start_time).total_seconds()
                print(f"ğŸ”„ è´Ÿè½½å‡è¡¡è¿›åº¦: {processed_count}/{len(prioritized_files)} ({progress:.1f}%) - ç”¨æ—¶ {elapsed:.1f}s")
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        print(f"âœ… è´Ÿè½½å‡è¡¡åˆ†é…å®Œæˆ: {total_time:.2f}s")
        print(f"ğŸ“Š åˆ†é…ç»“æœ: ä¸»è¦ {assignment_stats['primary_assignments']}, å¤‡é€‰ {assignment_stats['alternative_assignments']}, å¤±è´¥ {assignment_stats['failed_assignments']}")
        print(f"ğŸ‘¥ æ¶‰åŠ {len(person_workload)} ä½è´¡çŒ®è€…")
        
        # ä¿å­˜è´Ÿè½½å‡è¡¡æ€§èƒ½æ—¥å¿—
        self._save_load_balance_performance_log({
            'load_balance_time': total_time,
            'files_processed': len(decisions),
            'assignment_stats': assignment_stats,
            'workload_distribution': dict(person_workload),
            'avg_assignment_time_ms': (total_time / len(decisions)) * 1000,
            'load_balance_efficiency': (assignment_stats['primary_assignments'] + assignment_stats['alternative_assignments']) / len(decisions) * 100
        })
        
        return final_assignments, person_workload, assignment_stats

    def _merge_group_contributors(self, batch_contributors):
        """åˆå¹¶ç»„å†…æ–‡ä»¶çš„è´¡çŒ®è€…ç»Ÿè®¡"""
        merged_contributors = {}

        for file_path, contributors in batch_contributors.items():
            for author, info in contributors.items():
                if author not in merged_contributors:
                    merged_contributors[author] = {
                        "total_commits": 0,
                        "recent_commits": 0,
                        "total_changes": 0,
                        "total_additions": 0,
                        "total_deletions": 0,
                        "score": 0,
                        "enhanced_score": 0,
                        "files_contributed": [],
                    }

                # åˆå¹¶ç»Ÿè®¡
                merged = merged_contributors[author]
                merged["total_commits"] += info.get("total_commits", 0)
                merged["recent_commits"] += info.get("recent_commits", 0)
                merged["total_changes"] += info.get("total_changes", 0)
                merged["total_additions"] += info.get("total_additions", 0)
                merged["total_deletions"] += info.get("total_deletions", 0)
                merged["enhanced_score"] += info.get("enhanced_score", 0)
                merged["files_contributed"].append(file_path)

        return merged_contributors

    def _find_alternative_assignee(
        self, contributors, exclude_authors, person_task_count, max_tasks
    ):
        """å¯»æ‰¾æ›¿ä»£åˆ†é…å¯¹è±¡"""
        ranking = self.enhanced_analyzer.get_contributor_ranking(contributors)

        for author, info in ranking:
            if (
                author not in exclude_authors
                and person_task_count.get(author, 0) < max_tasks
            ):
                reason = (
                    self.enhanced_analyzer._generate_assignment_reason(author, info)
                    + "ï¼ˆè´Ÿè½½å‡è¡¡åˆ†é…ï¼‰"
                )
                return (author, reason)

        return None

    def _fallback_assignment(self, file_path, exclude_authors):
        """å•æ–‡ä»¶å›é€€åˆ†é…"""
        # å°è¯•ç›®å½•çº§åˆ†æ
        import os

        directory = os.path.dirname(file_path)

        if directory:
            dir_contributors = self.git_ops.get_directory_contributors(directory)
            if dir_contributors:
                # è·å–æœ€é«˜åˆ†ä¸”ä¸åœ¨æ’é™¤åˆ—è¡¨çš„è´¡çŒ®è€…
                sorted_contributors = sorted(
                    dir_contributors.items(),
                    key=lambda x: x[1].get("score", 0),
                    reverse=True,
                )

                for author, info in sorted_contributors:
                    if author not in exclude_authors:
                        return author, "ç›®å½•çº§å›é€€åˆ†é…"

        return None, "å›é€€åˆ†é…å¤±è´¥"

    def _fallback_group_assignment(self, group_name, exclude_authors):
        """ç»„çº§å›é€€åˆ†é…"""
        # ä½¿ç”¨ç»„åä½œä¸ºè·¯å¾„è¿›è¡Œç›®å½•çº§åˆ†æ
        dir_contributors = self.git_ops.get_directory_contributors(group_name)
        if dir_contributors:
            sorted_contributors = sorted(
                dir_contributors.items(),
                key=lambda x: x[1].get("score", 0),
                reverse=True,
            )

            for author, info in sorted_contributors:
                if author not in exclude_authors:
                    return author, "ç»„çº§å›é€€åˆ†é…"

        return None, "å›é€€åˆ†é…å¤±è´¥"

    def _show_workload_distribution(self, person_task_count):
        """æ˜¾ç¤ºå·¥ä½œè´Ÿè½½åˆ†å¸ƒ"""
        if not person_task_count:
            return

        print("\nğŸ‘¥ å·¥ä½œè´Ÿè½½åˆ†å¸ƒ:")
        sorted_workload = sorted(
            person_task_count.items(), key=lambda x: x[1], reverse=True
        )

        for author, count in sorted_workload[:10]:  # åªæ˜¾ç¤ºå‰10å
            print(f"  ğŸ“‹ {author}: {count} ä¸ªä»»åŠ¡")

        if len(sorted_workload) > 10:
            print(f"  ... å¦å¤– {len(sorted_workload) - 10} ä½è´¡çŒ®è€…")

    def _basic_assignment_fallback(self, plan, exclude_authors, max_tasks_per_person):
        """åŸºç¡€åˆ†é…å›é€€ï¼ˆå½“å¢å¼ºåŠŸèƒ½ä¸å¯ç”¨æ—¶ï¼‰"""
        print("âš ï¸  ä½¿ç”¨åŸºç¡€åˆ†é…é€»è¾‘")

        # è¿™é‡Œå¯ä»¥è°ƒç”¨åŸæœ‰çš„åŸºç¡€åˆ†é…é€»è¾‘
        # æˆ–è€…è¿”å›æœ€å°åŒ–çš„åˆ†é…ç»“æœ

        # å…¼å®¹å­—å…¸å’Œå¯¹è±¡ä¸¤ç§æ•°æ®ç»“æ„
        if isinstance(plan, dict):
            processing_mode = plan.get("processing_mode", "file_level")
            items = plan.get("files" if processing_mode == "file_level" else "groups", [])
        else:
            processing_mode = getattr(plan, "processing_mode", "file_level")
            items = getattr(plan, "files" if processing_mode == "file_level" else "groups", [])

        # ç®€å•çš„è½®è¯¢åˆ†é…
        active_contributors = self.git_ops.get_active_contributors(
            DEFAULT_ACTIVE_MONTHS
        )
        available_contributors = [
            c for c in active_contributors if c not in exclude_authors
        ]

        if not available_contributors:
            return 0, len(items), {"error": "æ— å¯ç”¨è´¡çŒ®è€…"}

        success_count = 0
        for i, item in enumerate(items):
            assignee = available_contributors[i % len(available_contributors)]
            item["assignee"] = assignee
            item["status"] = "assigned"
            item["assignment_reason"] = "åŸºç¡€è½®è¯¢åˆ†é…"
            success_count += 1

        return (
            success_count,
            0,
            {"basic_assignment": True, "contributors": len(available_contributors)},
        )

    def get_assignment_analysis_report(self, plan):
        """è·å–åˆ†é…åˆ†ææŠ¥å‘Š"""
        # å…¼å®¹å­—å…¸å’Œå¯¹è±¡ä¸¤ç§æ•°æ®ç»“æ„
        if isinstance(plan, dict):
            processing_mode = plan.get("processing_mode", "file_level")
            items = plan.get("files" if processing_mode == "file_level" else "groups", [])
        else:
            processing_mode = getattr(plan, "processing_mode", "file_level")
            items = getattr(plan, "files" if processing_mode == "file_level" else "groups", [])

        report = {
            "total_items": len(items),
            "assigned_items": 0,
            "unassigned_items": 0,
            "contributors_involved": set(),
            "assignment_reasons": {},
            "enhanced_analysis_used": self.enhanced_enabled,
        }

        for item in items:
            assignee = item.get("assignee")
            if assignee and assignee != "æœªåˆ†é…":
                report["assigned_items"] += 1
                report["contributors_involved"].add(assignee)
            else:
                report["unassigned_items"] += 1

            reason = item.get("assignment_reason", "unknown")
            report["assignment_reasons"][reason] = (
                report["assignment_reasons"].get(reason, 0) + 1
            )

        report["contributors_involved"] = len(report["contributors_involved"])

        return report
        
    def _save_enhanced_performance_log(self, perf_log):
        """ä¿å­˜å¢å¼ºä»»åŠ¡åˆ†é…å™¨çš„è¯¦ç»†æ€§èƒ½æ—¥å¿—"""
        try:
            import json
            from pathlib import Path
            from datetime import datetime
            
            # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
            if hasattr(self.git_ops, 'repo_path'):
                repo_path = Path(self.git_ops.repo_path)
            else:
                repo_path = Path(".")
                
            log_file = repo_path / ".merge_work" / "enhanced_performance_log.json"
            log_file.parent.mkdir(exist_ok=True)
            
            # æ„å»ºæ—¥å¿—æ¡ç›®
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'component': 'EnhancedTaskAssigner',
                'version': '2.3',
                'performance_breakdown': perf_log,
                'detailed_summary': {
                    'total_time': perf_log.get('total_execution_time', 0),
                    'analysis_phase_time': perf_log.get('analysis_phase_time', 0),
                    'assignment_phase_time': perf_log.get('assignment_phase_time', 0),
                    'other_processing_time': perf_log.get('other_processing_time', 0),
                    'total_files': perf_log.get('total_files', 0),
                    'files_to_process': perf_log.get('files_to_process', 0),
                    'success_count': perf_log.get('success_count', 0),
                    'failed_count': perf_log.get('failed_count', 0),
                    'success_rate': perf_log.get('success_rate', 0),
                    'avg_time_per_file_ms': perf_log.get('avg_time_per_file_ms', 0),
                    'analysis_to_assignment_ratio': perf_log.get('analysis_to_assignment_ratio', 0),
                    'contributors_involved': perf_log.get('contributors_count', 0)
                },
                'performance_insights': self._generate_performance_insights(perf_log)
            }
            
            # åŠ è½½ç°æœ‰æ—¥å¿—
            logs = []
            if log_file.exists():
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        logs = json.load(f)
                except:
                    logs = []
            
            # æ·»åŠ æ–°æ—¥å¿—
            logs.append(log_entry)
            
            # ä¿æŒæœ€è¿‘50æ¡è®°å½•
            if len(logs) > 50:
                logs = logs[-50:]
                
            # å†™å…¥æ–‡ä»¶
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(logs, f, indent=2, ensure_ascii=False)
                
            print(f"ğŸ“‹ æ€§èƒ½æ—¥å¿—å·²ä¿å­˜: {log_file}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ€§èƒ½æ—¥å¿—å¤±è´¥: {e}")
    
    def _generate_performance_insights(self, perf_log):
        """ç”Ÿæˆæ€§èƒ½æ´å¯Ÿå»ºè®®"""
        insights = []
        
        total_time = perf_log.get('total_execution_time', 0)
        analysis_time = perf_log.get('analysis_phase', 0)
        assignment_time = perf_log.get('total_assignment_loop_time', 0)
        
        # åˆ†æå„é˜¶æ®µè€—æ—¶
        if total_time > 30:
            insights.append(f"æ€»è€—æ—¶è¾ƒé•¿ ({total_time:.1f}s), éœ€è¦ä¼˜åŒ–")
            
        if assignment_time > analysis_time * 1.5:
            insights.append(f"åˆ†é…é€»è¾‘è€—æ—¶è¾ƒå¤š ({assignment_time:.1f}s vs {analysis_time:.1f}s), å¯è€ƒè™‘ç®—æ³•ä¼˜åŒ–")
            
        if perf_log.get('total_decision_time', 0) > assignment_time * 0.4:
            insights.append("å†³ç­–è®¡ç®—è€—æ—¶è¾ƒå¤š, å¯è€ƒè™‘ç¼“å­˜ä¼˜åŒ–")
            
        if perf_log.get('fallback_operations', 0) > assignment_time * 0.2:
            insights.append("å›é€€æ“ä½œé¢‘ç¹, å¯è€ƒè™‘ä¼˜åŒ–ä¸»è¦åˆ†é…ç®—æ³•")
            
        avg_time = perf_log.get('avg_time_per_file_ms', 0)
        if avg_time > 50:  # 50ms per file
            insights.append(f"å¹³å‡æ–‡ä»¶å¤„ç†æ—¶é—´è¾ƒé•¿ ({avg_time:.1f}ms), å¯è€ƒè™‘æ‰¹é‡ä¼˜åŒ–")
            
        if not insights:
            insights.append("æ€§èƒ½è¡¨ç°è‰¯å¥½")
            
        return insights
    
    def _save_load_balance_performance_log(self, perf_data):
        """ä¿å­˜è´Ÿè½½å‡è¡¡æ€§èƒ½è¯¦ç»†æ—¥å¿—"""
        try:
            import json
            from pathlib import Path
            from datetime import datetime
            
            # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
            if hasattr(self.git_ops, 'repo_path'):
                repo_path = Path(self.git_ops.repo_path)
            else:
                repo_path = Path(".")
                
            log_file = repo_path / ".merge_work" / "load_balance_performance.json"
            log_file.parent.mkdir(exist_ok=True)
            
            # æ„å»ºæ—¥å¿—æ¡ç›®
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'component': 'EnhancedTaskAssigner.apply_load_balanced_assignment',
                'version': '2.3',
                'performance_data': perf_data,
                'efficiency_insights': self._generate_load_balance_insights(perf_data)
            }
            
            # åŠ è½½ç°æœ‰æ—¥å¿—
            logs = []
            if log_file.exists():
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        logs = json.load(f)
                except:
                    logs = []
            
            # æ·»åŠ æ–°æ—¥å¿—
            logs.append(log_entry)
            
            # ä¿æŒæœ€è¿‘20æ¡è®°å½•
            if len(logs) > 20:
                logs = logs[-20:]
                
            # å†™å…¥æ–‡ä»¶
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(logs, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è´Ÿè½½å‡è¡¡æ€§èƒ½æ—¥å¿—å¤±è´¥: {e}")
    
    def _generate_load_balance_insights(self, perf_data):
        """ç”Ÿæˆè´Ÿè½½å‡è¡¡æ€§èƒ½æ´å¯Ÿ"""
        insights = []
        
        stats = perf_data.get('assignment_stats', {})
        efficiency = perf_data.get('load_balance_efficiency', 0)
        workload = perf_data.get('workload_distribution', {})
        
        # åˆ†é…æ•ˆç‡åˆ†æ
        if efficiency >= 95:
            insights.append("åˆ†é…æ•ˆç‡ä¼˜ç§€")
        elif efficiency >= 80:
            insights.append("åˆ†é…æ•ˆç‡è‰¯å¥½")
        else:
            insights.append(f"åˆ†é…æ•ˆç‡éœ€è¦æ”¹å–„ ({efficiency:.1f}%)")
        
        # è´Ÿè½½å‡è¡¡åˆ†æ
        if workload:
            workload_values = list(workload.values())
            max_load = max(workload_values)
            min_load = min(workload_values)
            avg_load = sum(workload_values) / len(workload_values)
            
            if max_load - min_load <= 2:
                insights.append("è´Ÿè½½åˆ†å¸ƒéå¸¸å‡è¡¡")
            elif max_load - min_load <= 5:
                insights.append("è´Ÿè½½åˆ†å¸ƒè¾ƒä¸ºå‡è¡¡")
            else:
                insights.append(f"è´Ÿè½½åˆ†å¸ƒä¸å‡è¡¡ (æœ€å¤§{max_load}vsæœ€å°{min_load})")
        
        # æ€§èƒ½åˆ†æ
        avg_time = perf_data.get('avg_assignment_time_ms', 0)
        if avg_time < 1:
            insights.append("åˆ†é…æ€§èƒ½ä¼˜ç§€")
        elif avg_time < 5:
            insights.append("åˆ†é…æ€§èƒ½è‰¯å¥½")
        else:
            insights.append(f"åˆ†é…æ€§èƒ½éœ€ä¼˜åŒ– ({avg_time:.1f}ms/æ–‡ä»¶)")
        
        return insights
    
    def _find_least_loaded_contributor(self, person_workload, max_tasks_per_person):
        """æ‰¾åˆ°å½“å‰è´Ÿè½½æœ€è½»çš„è´¡çŒ®è€…ä½œä¸ºå›é€€åˆ†é…ç›®æ ‡"""
        # è·å–æ‰€æœ‰æ´»è·ƒè´¡çŒ®è€…
        active_contributors = self.git_ops.get_active_contributors(DEFAULT_ACTIVE_MONTHS)
        
        if not active_contributors:
            return None
        
        # æ‰¾åˆ°è´Ÿè½½æœ€è½»ä¸”æœªè¶…è¿‡é™åˆ¶çš„è´¡çŒ®è€…
        min_workload = float('inf')
        least_loaded = None
        
        for contributor in active_contributors:
            current_load = person_workload.get(contributor, 0)
            if current_load < max_tasks_per_person and current_load < min_workload:
                min_workload = current_load
                least_loaded = contributor
        
        return least_loaded
