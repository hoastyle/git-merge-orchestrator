"""
Git Merge Orchestrator - åŠ é€Ÿä¼˜åŒ–è´¡çŒ®è€…åˆ†æå™¨
ä¿®å¤æ‰¹é‡åˆ†æé€»è¾‘ï¼Œç¡®ä¿ç»“æœå‡†ç¡®æ€§
"""

import json
import hashlib
import threading
from datetime import datetime, timedelta
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from config import (
    SCORING_WEIGHTS,
    DEFAULT_ANALYSIS_MONTHS,
    DEFAULT_ACTIVE_MONTHS,
    ENABLE_PERFORMANCE_MONITORING,
)
from core.ultra_fast_analyzer import UltraFastAnalyzer
from utils.performance_monitor import (
    performance_monitor,
    timing_context,
    global_performance_stats,
)
from utils.smart_cache import get_cache_manager


class OptimizedContributorAnalyzer:
    """ä¼˜åŒ–è´¡çŒ®è€…åˆ†æå™¨ - ä¿®å¤æ‰¹é‡åˆ†æé€»è¾‘"""

    def __init__(self, git_ops):
        self.git_ops = git_ops
        self.cache_file = (
            Path(git_ops.repo_path) / ".merge_work" / "contributor_cache.json"
        )
        self.cache_lock = threading.Lock()

        # æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨
        self.smart_cache = get_cache_manager(git_ops.repo_path)
        
        # è¶…é«˜é€Ÿåˆ†æå™¨
        self.ultra_fast_analyzer = UltraFastAnalyzer(git_ops.repo_path)

        # å†…å­˜ç¼“å­˜ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰
        self._file_contributors_cache = {}
        self._directory_contributors_cache = {}
        self._active_contributors_cache = None
        self._all_contributors_cache = None

        # æ‰¹é‡æ•°æ®ç¼“å­˜
        self._batch_file_data = {}
        self._batch_computed = False

    def load_persistent_cache(self):
        """åŠ è½½æŒä¹…åŒ–ç¼“å­˜"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    cache_data = json.load(f)

                # æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§ï¼ˆ24å°æ—¶ï¼‰
                cache_time = datetime.fromisoformat(
                    cache_data.get("created_at", "2000-01-01")
                )
                if datetime.now() - cache_time < timedelta(hours=24):
                    self._file_contributors_cache = cache_data.get(
                        "file_contributors", {}
                    )
                    self._directory_contributors_cache = cache_data.get(
                        "directory_contributors", {}
                    )
                    print(f"âœ… å·²åŠ è½½è´¡çŒ®è€…ç¼“å­˜ ({len(self._file_contributors_cache)} æ–‡ä»¶)")
                    return True
                else:
                    print("â° ç¼“å­˜å·²è¿‡æœŸï¼Œå°†é‡æ–°è®¡ç®—")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return False

    def save_persistent_cache(self):
        """ä¿å­˜æŒä¹…åŒ–ç¼“å­˜"""
        try:
            cache_data = {
                "created_at": datetime.now().isoformat(),
                "file_contributors": self._file_contributors_cache,
                "directory_contributors": self._directory_contributors_cache,
            }

            self.cache_file.parent.mkdir(exist_ok=True)
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ å·²ä¿å­˜è´¡çŒ®è€…ç¼“å­˜")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    @performance_monitor("æ‰¹é‡åˆ†æ")
    def batch_analyze_all_files(self, file_list, use_ultra_fast=True):
        """æ‰¹é‡åˆ†ææ‰€æœ‰æ–‡ä»¶çš„è´¡çŒ®è€…ä¿¡æ¯
        
        Args:
            file_list: æ–‡ä»¶åˆ—è¡¨
            use_ultra_fast: æ˜¯å¦ä½¿ç”¨è¶…é«˜é€Ÿåˆ†æï¼ˆé»˜è®¤Trueï¼Œ>=10ä¸ªæ–‡ä»¶æ—¶è‡ªåŠ¨å¯ç”¨ï¼‰
        """
        if self._batch_computed:
            return self._batch_file_data

        # æ™ºèƒ½é€‰æ‹©åˆ†ææ¨¡å¼
        should_use_ultra = use_ultra_fast and len(file_list) >= 10
        
        if should_use_ultra:
            return self._ultra_fast_batch_analysis(file_list)
        else:
            return self._traditional_batch_analysis(file_list)
    
    def _ultra_fast_batch_analysis(self, file_list):
        """è¶…é«˜é€Ÿæ‰¹é‡åˆ†æ"""
        main_start = datetime.now()
        print(f"ğŸš€ [PERF] ä½¿ç”¨è¶…é«˜é€Ÿåˆ†ææ¨¡å¼å¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶... (å¼€å§‹æ—¶é—´: {main_start.timestamp():.3f})")
        
        # ä½¿ç”¨è¶…é«˜é€Ÿåˆ†æå™¨
        ultra_start = datetime.now()
        ultra_results = self.ultra_fast_analyzer.analyze_contributors_ultra_fast(
            file_list, months=DEFAULT_ANALYSIS_MONTHS
        )
        ultra_time = (datetime.now() - ultra_start).total_seconds()
        print(f"â±ï¸  [PERF] è¶…é«˜é€Ÿåˆ†æå™¨æ‰§è¡Œ: {ultra_time:.3f}s")
        
        # è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼å¹¶ç¼“å­˜
        convert_start = datetime.now()
        for file_path, contributors in ultra_results.items():
            self._batch_file_data[file_path] = contributors
            # åŒæ—¶æ›´æ–°æ–‡ä»¶ç¼“å­˜
            cache_key = self._get_file_cache_key(file_path)
            self._file_contributors_cache[cache_key] = contributors
        
        convert_time = (datetime.now() - convert_start).total_seconds()
        print(f"â±ï¸  [PERF] æ ¼å¼è½¬æ¢å’Œç¼“å­˜: {convert_time:.3f}s")
        
        total_time = (datetime.now() - main_start).total_seconds()
        print(f"âœ… [PERF] è¶…é«˜é€Ÿåˆ†ææ€»å®Œæˆæ—¶é—´: {total_time:.3f}s")
        print(f"ğŸ“Š [PERF] å¤„ç†ç»Ÿè®¡: æ€»è®¡{len(file_list)}ä¸ªæ–‡ä»¶, å¹³å‡{total_time/len(file_list)*1000:.1f}ms/æ–‡ä»¶")
        
        # ä¿å­˜æ€§èƒ½æ—¥å¿—
        self._save_performance_log(file_list, total_time, {
            'ultra_analysis': ultra_time,
            'format_conversion': convert_time,
            'mode': 'optimized_ultra_fast'
        })
        
        self._batch_computed = True
        return self._batch_file_data
    
    def _traditional_batch_analysis(self, file_list):
        """ä¼ ç»Ÿæ‰¹é‡åˆ†æï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰"""
        main_start = datetime.now()
        print(f"ğŸ“Š [PERF] ä½¿ç”¨ä¼ ç»Ÿä¼˜åŒ–æ¨¡å¼å¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶... (å¼€å§‹æ—¶é—´: {main_start.timestamp():.3f})")
        print(f"âš¡ [PERF] æ€§èƒ½ä¼˜åŒ–æç¤ºï¼šæ­£åœ¨åº”ç”¨æ™ºèƒ½æ–‡ä»¶åˆ†ç±»ç­–ç•¥...")
        
        # ç»§ç»­ä½¿ç”¨åŸæœ‰çš„ä¼ ç»Ÿåˆ†æé€»è¾‘ï¼Œä½†æ·»åŠ è¯¦ç»†è®¡æ—¶
        print("ğŸ” [PERF] å¼€å§‹ä¼ ç»Ÿåˆ†ææµç¨‹...")

        # ç»§ç»­ä½¿ç”¨åŸæœ‰çš„ä¼ ç»Ÿåˆ†æé€»è¾‘
        # æ£€æŸ¥ç¼“å­˜
        cached_files = set()
        uncached_files = []

        for file_path in file_list:
            cache_key = self._get_file_cache_key(file_path)
            if cache_key in self._file_contributors_cache:
                self._batch_file_data[file_path] = self._file_contributors_cache[
                    cache_key
                ]
                cached_files.add(file_path)
            else:
                uncached_files.append(file_path)

        if cached_files:
            print(f"ğŸ“¦ ä»ç¼“å­˜è·å– {len(cached_files)} ä¸ªæ–‡ä»¶çš„è´¡çŒ®è€…ä¿¡æ¯")

        if not uncached_files:
            print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½æœ‰ç¼“å­˜ï¼Œè·³è¿‡Gitåˆ†æ")
            self._batch_computed = True
            return self._batch_file_data

        print(f"ğŸ” éœ€è¦åˆ†æ {len(uncached_files)} ä¸ªæ–°æ–‡ä»¶")

        # åˆ†ç¦»å¯èƒ½éœ€è¦followçš„æ–‡ä»¶å’Œæ™®é€šæ–‡ä»¶
        classification_start = datetime.now()
        critical_files, regular_files = self._classify_files_for_analysis(
            uncached_files
        )
        classification_time = (datetime.now() - classification_start).total_seconds()

        # æ€§èƒ½ç»Ÿè®¡
        total_analysis_files = len(uncached_files)
        critical_ratio = (
            (len(critical_files) / total_analysis_files * 100)
            if total_analysis_files > 0
            else 0
        )

        print(f"ğŸ“Š æ™ºèƒ½æ–‡ä»¶åˆ†ç±»å®Œæˆ ({classification_time:.3f}s):")
        print(
            f"   ğŸ¯ å…³é”®æ–‡ä»¶: {len(critical_files)} ä¸ª ({critical_ratio:.1f}%) - ä½¿ç”¨æ·±åº¦åˆ†æï¼ˆ--followï¼‰"
        )
        print(f"   ğŸ“‹ æ™®é€šæ–‡ä»¶: {len(regular_files)} ä¸ª ({100-critical_ratio:.1f}%) - ä½¿ç”¨æ‰¹é‡åˆ†æ")

        # æ˜¾ç¤ºä¼˜åŒ–è¯¦æƒ…
        if critical_files:
            print(f"   ğŸ“ å…³é”®æ–‡ä»¶ç±»å‹åˆ†æ:")
            cpp_files = [
                f for f in critical_files if f.endswith((".cpp", ".h", ".hpp", ".c"))
            ]
            config_files = [
                f
                for f in critical_files
                if f.endswith((".json", ".yml", ".yaml", ".txt"))
            ]
            root_files = [f for f in critical_files if "/" not in f]

            if cpp_files:
                print(f"      â€¢ C++æ ¸å¿ƒæ–‡ä»¶: {len(cpp_files)} ä¸ª")
            if config_files:
                print(f"      â€¢ é…ç½®æ–‡ä»¶: {len(config_files)} ä¸ª")
            if root_files:
                print(f"      â€¢ æ ¹ç›®å½•æ–‡ä»¶: {len(root_files)} ä¸ª")

        if critical_ratio > 20:
            print(f"âš ï¸  æ€§èƒ½è­¦å‘Šï¼š{critical_ratio:.1f}%çš„æ–‡ä»¶å°†ä½¿ç”¨æ·±åº¦åˆ†æï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        elif critical_ratio > 10:
            print(f"ğŸ’¡ æ€§èƒ½æç¤ºï¼š{critical_ratio:.1f}%çš„æ–‡ä»¶ä½¿ç”¨æ·±åº¦åˆ†æï¼Œæ€§èƒ½ä¸­ç­‰")
        else:
            print(f"âœ… æ€§èƒ½ä¼˜ç§€ï¼šä»…{critical_ratio:.1f}%çš„æ–‡ä»¶ä½¿ç”¨æ·±åº¦åˆ†æï¼Œé¢„æœŸå¿«é€Ÿå®Œæˆ")

        # å¯¹å…³é”®æ–‡ä»¶ä½¿ç”¨æ·±åº¦åˆ†æ
        if critical_files:
            deep_analysis_start = datetime.now()
            for file_path in critical_files:
                contributors = self._analyze_single_file_with_follow(file_path)
                cache_key = self._get_file_cache_key(file_path)
                self._file_contributors_cache[cache_key] = contributors
                self._batch_file_data[file_path] = contributors
            deep_analysis_time = (datetime.now() - deep_analysis_start).total_seconds()
            avg_time_per_deep_file = deep_analysis_time / len(critical_files)
            print(
                f"ğŸ¯ æ·±åº¦åˆ†æå®Œæˆ: {len(critical_files)} ä¸ªæ–‡ä»¶, è€—æ—¶ {deep_analysis_time:.2f}s (å¹³å‡ {avg_time_per_deep_file:.3f}s/æ–‡ä»¶)"
            )

        # å¯¹æ™®é€šæ–‡ä»¶ä½¿ç”¨æ‰¹é‡åˆ†æ
        if regular_files:
            batch_analysis_start = datetime.now()
            batch_results = self._fixed_batch_analyze_files(regular_files)
            for file_path, contributors in batch_results.items():
                cache_key = self._get_file_cache_key(file_path)
                self._file_contributors_cache[cache_key] = contributors
                self._batch_file_data[file_path] = contributors
            batch_analysis_time = (
                datetime.now() - batch_analysis_start
            ).total_seconds()
            avg_time_per_batch_file = (
                batch_analysis_time / len(regular_files) if regular_files else 0
            )
            print(
                f"ğŸ“Š æ‰¹é‡åˆ†æå®Œæˆ: {len(regular_files)} ä¸ªæ–‡ä»¶, è€—æ—¶ {batch_analysis_time:.2f}s (å¹³å‡ {avg_time_per_batch_file:.3f}s/æ–‡ä»¶)"
            )

            # æ€§èƒ½å¯¹æ¯”
            if critical_files and regular_files:
                speedup_ratio = avg_time_per_deep_file / max(
                    avg_time_per_batch_file, 0.001
                )
                print(f"âš¡ æ€§èƒ½å¯¹æ¯”ï¼šæ‰¹é‡åˆ†ææ¯”æ·±åº¦åˆ†æå¿« {speedup_ratio:.1f}x")

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âš¡ æ‰¹é‡åˆ†æå®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f} ç§’")
        print(
            f"ğŸ“Š å¤„ç†ç»Ÿè®¡: æ€»è®¡{len(file_list)}, ç¼“å­˜{len(cached_files)}, æ·±åº¦åˆ†æ{len(critical_files)}, æ‰¹é‡åˆ†æ{len(regular_files)}"
        )

        # æ€§èƒ½ç›‘æ§æŠ¥å‘Š - å¦‚æœå¯ç”¨äº†æ€§èƒ½ç›‘æ§
        if ENABLE_PERFORMANCE_MONITORING:
            analysis_stats = {
                "total_time": elapsed,
                "batch_time": batch_analysis_time if regular_files else 0,
                "single_time": deep_analysis_time if critical_files else 0,
                "total_files": len(file_list),
                "batch_files": len(regular_files),
                "single_files": len(critical_files),
                "critical_ratio": len(critical_files) / max(len(file_list), 1) * 100,
                "follow_usage": len(critical_files) / max(len(file_list), 1) * 100,
            }
            performance_report = self._generate_performance_report(analysis_stats)
            print("\n" + performance_report)

        # ä¿å­˜ç¼“å­˜
        self.save_persistent_cache()
        
        # ä¿å­˜æ€§èƒ½æ—¥å¿—
        total_time = (datetime.now() - main_start).total_seconds()
        self._save_performance_log(file_list, total_time, {
            'cache_check': cache_check_time if 'cache_check_time' in locals() else 0,
            'file_classification': classification_time if 'classification_time' in locals() else 0,
            'batch_analysis': batch_analysis_time if 'batch_analysis_time' in locals() else 0,
            'deep_analysis': deep_analysis_time if 'deep_analysis_time' in locals() else 0,
            'mode': 'optimized_traditional'
        })
        
        self._batch_computed = True
        return self._batch_file_data

    def _classify_files_for_analysis(self, file_list):
        """åˆ†ç±»æ–‡ä»¶ï¼šå“ªäº›éœ€è¦æ·±åº¦åˆ†æï¼ˆ--followï¼‰ï¼Œå“ªäº›å¯ä»¥æ‰¹é‡åˆ†æ
        
        ä¼˜åŒ–ç­–ç•¥ï¼šå¤§å¹…å‡å°‘æ·±åº¦åˆ†ææ–‡ä»¶æ•°é‡ï¼Œåªå¯¹çœŸæ­£å¯èƒ½é‡å‘½åçš„æ ¸å¿ƒæ–‡ä»¶ä½¿ç”¨--follow
        """
        critical_files = []
        regular_files = []

        # æ›´ä¸¥æ ¼çš„å…³é”®æ–‡ä»¶åˆ¤æ–­æ¡ä»¶ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨æ·±åº¦åˆ†æ
        for file_path in file_list:
            is_critical = False

            # 1. åªæœ‰æå°‘æ•°æ ¸å¿ƒæ–‡ä»¶æ‰éœ€è¦æ·±åº¦åˆ†æ
            # Python/JavaScriptæ ¸å¿ƒæ–‡ä»¶
            python_js_patterns = [
                "main.py",
                "index.js",
                "index.ts",
                "app.py",
                "server.py",
                "__init__.py",
                "setup.py",
                "manage.py",
                "wsgi.py",
                "asgi.py",
            ]

            # C++æ ¸å¿ƒæ–‡ä»¶
            cpp_patterns = [
                "main.cpp",
                "main.c",
                "main.h",
                "main.hpp",
                "app.cpp",
                "application.cpp",
                "core.cpp",
                "core.h",
                "engine.cpp",
                "engine.h",
                "manager.cpp",
                "manager.h",
            ]

            # å…¶ä»–æ ¸å¿ƒæ–‡ä»¶
            other_patterns = [
                "CMakeLists.txt",
                "Makefile",
                "configure",
                "config.h",
                "package.json",
                "pom.xml",
                "build.gradle",
            ]

            all_core_patterns = python_js_patterns + cpp_patterns + other_patterns

            file_name = file_path.split("/")[-1]
            if file_name in all_core_patterns:
                is_critical = True

            # 2. æ ¹ç›®å½•çš„é‡è¦é…ç½®æ–‡ä»¶
            elif "/" not in file_path and file_path.endswith(
                (".py", ".js", ".ts", ".json", ".yml", ".yaml", ".cpp", ".h", ".hpp")
            ):
                is_critical = True

            # 3. æ˜ç¡®çš„æ¡†æ¶å…¥å£æ–‡ä»¶
            elif any(
                pattern in file_path.lower()
                for pattern in ["main/", "/main.", "entry", "bootstrap"]
            ):
                is_critical = True

            # 4. æ•°æ®æ–‡ä»¶å’ŒäºŒè¿›åˆ¶æ–‡ä»¶æ°¸è¿œä¸éœ€è¦æ·±åº¦åˆ†æ
            elif self._is_data_or_binary_file(file_path):
                is_critical = False  # æ˜ç¡®æ ‡è®°ä¸ºéå…³é”®æ–‡ä»¶

            # 5. æ£€æŸ¥æ˜¯å¦å¯èƒ½æœ‰é‡å‘½åå†å²ï¼ˆæ›´ä¸¥æ ¼çš„æ¡ä»¶ï¼‰
            elif self._should_check_rename_history_strict(file_path):
                is_critical = True

            if is_critical:
                critical_files.append(file_path)
            else:
                regular_files.append(file_path)

        # å®‰å…¨é™åˆ¶ï¼šå³ä½¿æ»¡è¶³æ¡ä»¶ï¼Œä¹Ÿé™åˆ¶æ·±åº¦åˆ†ææ–‡ä»¶çš„æ¯”ä¾‹
        total_files = len(file_list)
        max_critical_files = max(
            1, min(10, total_files // 10)
        )  # æœ€å¤š10%çš„æ–‡ä»¶ä½¿ç”¨æ·±åº¦åˆ†æï¼Œè‡³å°‘1ä¸ªï¼Œæœ€å¤š10ä¸ª

        if len(critical_files) > max_critical_files:
            # æŒ‰é‡è¦æ€§æ’åºï¼Œåªä¿ç•™æœ€é‡è¦çš„æ–‡ä»¶
            sorted_critical = self._sort_files_by_importance(critical_files)
            demoted_files = sorted_critical[max_critical_files:]
            critical_files = sorted_critical[:max_critical_files]
            regular_files.extend(demoted_files)

            print(
                f"âš¡ æ€§èƒ½ä¼˜åŒ–ï¼šé™åˆ¶æ·±åº¦åˆ†ææ–‡ä»¶æ•°é‡ä» {len(sorted_critical)} é™è‡³ {len(critical_files)} ä¸ª"
            )

        return critical_files, regular_files

    def _is_data_or_binary_file(self, file_path):
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ•°æ®æ–‡ä»¶æˆ–äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œè¿™äº›æ–‡ä»¶é€šå¸¸ä¸éœ€è¦--followåˆ†æ"""
        data_extensions = [
            # æ•°æ®æ–‡ä»¶
            ".pcd",
            ".ply",
            ".las",
            ".laz",  # ç‚¹äº‘æ•°æ®
            ".bag",
            ".rosbag",  # ROSæ•°æ®
            ".csv",
            ".xlsx",
            ".xls",
            ".db",
            ".sqlite",  # è¡¨æ ¼æ•°æ®
            ".json",
            ".xml",
            ".yaml",
            ".yml",  # é…ç½®æ•°æ®ï¼ˆä½†ä¸åœ¨æ ¹ç›®å½•ï¼‰
            # åª’ä½“æ–‡ä»¶
            ".png",
            ".jpg",
            ".jpeg",
            ".gif",
            ".bmp",
            ".svg",  # å›¾ç‰‡
            ".mp4",
            ".avi",
            ".mkv",
            ".mov",  # è§†é¢‘
            ".mp3",
            ".wav",
            ".ogg",  # éŸ³é¢‘
            # äºŒè¿›åˆ¶æ–‡ä»¶
            ".so",
            ".dll",
            ".dylib",  # åº“æ–‡ä»¶
            ".exe",
            ".bin",
            ".out",  # å¯æ‰§è¡Œæ–‡ä»¶
            ".zip",
            ".tar",
            ".gz",
            ".7z",
            ".rar",  # å‹ç¼©æ–‡ä»¶
            ".pdf",
            ".doc",
            ".docx",  # æ–‡æ¡£æ–‡ä»¶
            # ç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶
            ".cache",
            ".tmp",
            ".temp",
            ".log",
            ".pyc",
            ".pyo",
            ".class",  # ç¼–è¯‘æ–‡ä»¶
        ]

        file_lower = file_path.lower()

        # ç‰¹æ®Šå¤„ç†ï¼šæ ¹ç›®å½•çš„é…ç½®æ–‡ä»¶ä¸åº”è¯¥è¢«è§†ä¸ºæ•°æ®æ–‡ä»¶
        if "/" not in file_path and file_path.lower().endswith(
            (".json", ".yaml", ".yml", ".xml")
        ):
            return False

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        if any(file_lower.endswith(ext) for ext in data_extensions):
            return True

        # æ£€æŸ¥æ˜¯å¦åœ¨æ•°æ®ç›®å½•ä¸­
        data_dirs = [
            "data/",
            "dataset/",
            "assets/",
            "resources/",
            "media/",
            "cache/",
            "tmp/",
        ]
        if any(data_dir in file_path.lower() for data_dir in data_dirs):
            return True

        return False

    def _should_check_rename_history_strict(self, file_path):
        """æ›´ä¸¥æ ¼çš„é‡å‘½åå†å²æ£€æŸ¥ï¼Œå‡å°‘è¯¯åˆ¤"""
        try:
            # åªæœ‰çœŸæ­£å¯èƒ½è¢«é‡å‘½åçš„æ–‡ä»¶æ‰éœ€è¦--follow
            refactor_indicators = [
                "legacy",
                "old",
                "deprecated",
                "migration",
                "refactor",
                "v1",
                "v2",
                "v3",
                "backup",
                "temp",
                "new",
                "tmp",
            ]

            path_lower = file_path.lower()
            # æ£€æŸ¥æ–‡ä»¶åæˆ–ç›®å½•åä¸­æ˜¯å¦æœ‰é‡æ„æ ‡è¯†
            path_parts = path_lower.split("/")
            for part in path_parts:
                if any(indicator in part for indicator in refactor_indicators):
                    return True

            # å¤§å¹…é™ä½æ·±åº¦é˜ˆå€¼ - åªæœ‰ææ·±çš„ç›®å½•æ‰å¯èƒ½æœ‰ç§»åŠ¨å†å²
            depth = file_path.count("/")
            if depth >= 6:  # ä»4æé«˜åˆ°6ï¼Œå‡å°‘è¯¯åˆ¤
                # ä½†æ’é™¤æ˜æ˜¾çš„æ•°æ®ç›®å½•
                if any(
                    data_dir in path_lower
                    for data_dir in [
                        "data/",
                        "dataset/",
                        "test/",
                        "tests/",
                        "examples/",
                    ]
                ):
                    return False
                return True

            return False
        except:
            return False

    def _should_check_rename_history(self, file_path):
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å¯èƒ½æœ‰é‡å‘½åå†å²ï¼Œéœ€è¦ä½¿ç”¨--follow"""
        try:
            # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶è·¯å¾„åŒ…å«å¸¸è§çš„é‡æ„æ ‡è¯†ï¼Œå¯èƒ½éœ€è¦è·Ÿè¸ªé‡å‘½å
            refactor_indicators = [
                "legacy",
                "old",
                "deprecated",
                "migration",
                "refactor",
                "v1",
                "v2",
                "backup",
                "temp",
                "new",
            ]

            path_lower = file_path.lower()
            if any(indicator in path_lower for indicator in refactor_indicators):
                return True

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æ·±å±‚ç›®å½•ï¼ˆå¯èƒ½è¢«ç§»åŠ¨è¿‡ï¼‰
            depth = file_path.count("/")
            if depth >= 4:  # æ·±åº¦>=4çš„æ–‡ä»¶æ›´å¯èƒ½æœ‰ç§»åŠ¨å†å²
                return True

            return False
        except:
            return False

    def _sort_files_by_importance(self, file_list):
        """æŒ‰é‡è¦æ€§å¯¹æ–‡ä»¶æ’åºï¼Œç”¨äºé™åˆ¶æ·±åº¦åˆ†ææ•°é‡æ—¶çš„ä¼˜å…ˆçº§"""

        def importance_score(file_path):
            score = 0
            file_name = file_path.split("/")[-1]

            # æ ¸å¿ƒæ–‡ä»¶å¾—åˆ†æœ€é«˜
            if file_name in ["main.py", "index.js", "index.ts", "app.py"]:
                score += 100

            # æ ¹ç›®å½•æ–‡ä»¶å¾—åˆ†è¾ƒé«˜
            if "/" not in file_path:
                score += 50

            # ç‰¹å®šå…³é”®è¯åŠ åˆ†
            if any(
                keyword in file_path.lower()
                for keyword in ["main", "core", "entry", "bootstrap"]
            ):
                score += 30

            # é…ç½®æ–‡ä»¶åŠ åˆ†
            if file_path.endswith((".json", ".yml", ".yaml", ".cfg", ".ini")):
                score += 20

            return score

        return sorted(file_list, key=importance_score, reverse=True)

    def _analyze_single_file_with_follow(self, filepath):
        """å•æ–‡ä»¶æ·±åº¦åˆ†æï¼ˆæ”¯æŒé‡å‘½åè·Ÿè¸ªï¼‰- ä½¿ç”¨æ™ºèƒ½ç¼“å­˜"""
        # å…ˆæ£€æŸ¥æ™ºèƒ½ç¼“å­˜
        cache_result = self.smart_cache.get(
            "file_contributors", filepath, max_age_hours=12
        )
        if cache_result is not None:
            return cache_result

        try:
            contributors = {}

            # è·å–ä¸€å¹´å†…çš„è´¡çŒ®ç»Ÿè®¡ï¼ˆä½¿ç”¨--followï¼‰
            one_year_ago = (
                datetime.now() - timedelta(days=DEFAULT_ANALYSIS_MONTHS * 30)
            ).strftime("%Y-%m-%d")

            recent_cmd = f'git log --follow --since="{one_year_ago}" --format="%an" -- "{filepath}"'
            recent_result = self.git_ops.run_command(recent_cmd)

            if recent_result:
                recent_authors = recent_result.split("\n")
                recent_author_counts = {}
                for author in recent_authors:
                    if author.strip():
                        recent_author_counts[author] = (
                            recent_author_counts.get(author, 0) + 1
                        )

                for author, count in recent_author_counts.items():
                    contributors[author] = {
                        "total_commits": count,
                        "recent_commits": count,
                        "score": count * SCORING_WEIGHTS["recent_commits"],
                    }

            # è·å–æ€»ä½“è´¡çŒ®ç»Ÿè®¡ï¼ˆä½¿ç”¨--followï¼‰
            total_cmd = f'git log --follow --format="%an" -- "{filepath}"'
            total_result = self.git_ops.run_command(total_cmd)

            if total_result:
                authors = total_result.split("\n")
                author_counts = {}
                for author in authors:
                    if author.strip():
                        author_counts[author] = author_counts.get(author, 0) + 1

                for author, count in author_counts.items():
                    if author in contributors:
                        contributors[author]["total_commits"] = count
                        contributors[author]["score"] = (
                            contributors[author]["recent_commits"]
                            * SCORING_WEIGHTS["recent_commits"]
                            + count * SCORING_WEIGHTS["total_commits"]
                        )
                    else:
                        contributors[author] = {
                            "total_commits": count,
                            "recent_commits": 0,
                            "score": count * SCORING_WEIGHTS["total_commits"],
                        }

            # å­˜å‚¨åˆ°æ™ºèƒ½ç¼“å­˜
            self.smart_cache.put("file_contributors", filepath, contributors)
            return contributors
        except Exception as e:
            print(f"æ·±åº¦åˆ†ææ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
            return {}

    def _fixed_batch_analyze_files(self, file_list):
        """æ‰¹é‡æ–‡ä»¶åˆ†ææ–¹æ³•"""
        print(f"ğŸ“Š å¼€å§‹æ‰¹é‡åˆ†æ {len(file_list)} ä¸ªæ–‡ä»¶")

        batch_results = {}
        one_year_ago = (
            datetime.now() - timedelta(days=DEFAULT_ANALYSIS_MONTHS * 30)
        ).strftime("%Y-%m-%d")

        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å‘½ä»¤è¡Œè¿‡é•¿
        batch_size = 500  # å¢å¤§æ‰¹é‡å¤§å°ä»¥æé«˜æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§

        for i in range(0, len(file_list), batch_size):
            batch_files = file_list[i : i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(file_list) + batch_size - 1) // batch_size

            print(f"  ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_files)} æ–‡ä»¶)")

            # è·å–è¿‘æœŸè´¡çŒ®è€…ï¼ˆç®€åŒ–æ ¼å¼ï¼‰
            recent_contributors = self._get_batch_contributors(
                batch_files, one_year_ago
            )

            # è·å–å†å²è´¡çŒ®è€…ï¼ˆç®€åŒ–æ ¼å¼ï¼‰
            total_contributors = self._get_batch_contributors(batch_files, None)

            # åˆå¹¶ç»“æœå¹¶è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„è´¡çŒ®è€…ç»Ÿè®¡
            for file_path in batch_files:
                contributors = {}

                # å¤„ç†è¿‘æœŸè´¡çŒ®
                recent_authors = recent_contributors.get(file_path, {})
                for author, count in recent_authors.items():
                    contributors[author] = {
                        "total_commits": count,
                        "recent_commits": count,
                        "score": count * SCORING_WEIGHTS["recent_commits"],
                    }

                # å¤„ç†å†å²è´¡çŒ®
                total_authors = total_contributors.get(file_path, {})
                for author, count in total_authors.items():
                    if author in contributors:
                        contributors[author]["total_commits"] = count
                        contributors[author]["score"] = (
                            contributors[author]["recent_commits"]
                            * SCORING_WEIGHTS["recent_commits"]
                            + count * SCORING_WEIGHTS["total_commits"]
                        )
                    else:
                        contributors[author] = {
                            "total_commits": count,
                            "recent_commits": 0,
                            "score": count * SCORING_WEIGHTS["total_commits"],
                        }

                batch_results[file_path] = contributors

        return batch_results

    def _get_batch_contributors(self, file_list, since_date=None):
        """è·å–æ‰¹é‡æ–‡ä»¶çš„è´¡çŒ®è€…ä¿¡æ¯ - å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
        max_retries = 2

        for attempt in range(max_retries + 1):
            try:
                # å°è¯•ä½¿ç”¨git_operationsä¸­çš„æ‰¹é‡æ–¹æ³•
                batch_results = self.git_ops.get_contributors_batch(
                    file_list, since_date
                )

                # éªŒè¯ç»“æœè´¨é‡
                if self._validate_batch_results(batch_results, file_list):
                    if attempt > 0:
                        print(f"   âœ… é‡è¯•æˆåŠŸ (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
                    return batch_results
                else:
                    if attempt < max_retries:
                        print(f"   âš ï¸ æ‰¹é‡ç»“æœè´¨é‡ä¸ä½³ï¼Œå‡†å¤‡é‡è¯• (ç¬¬{attempt + 1}æ¬¡)")
                        continue
                    else:
                        print(f"   âš ï¸ å¤šæ¬¡é‡è¯•åç»“æœä»ä¸ç†æƒ³ï¼Œä½¿ç”¨fallbackæ–¹æ³•")

            except Exception as e:
                if attempt < max_retries:
                    print(f"   âš ï¸ æ‰¹é‡å¤„ç†å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {str(e)[:100]}...")
                    print(f"   ğŸ”„ æ­£åœ¨é‡è¯•...")
                    continue
                else:
                    print(f"   âŒ æ‰¹é‡å¤„ç†å¤šæ¬¡å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {str(e)[:100]}...")

        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨fallbackæ–¹æ³•
        return self._get_batch_contributors_fallback(file_list, since_date)

    def _validate_batch_results(self, batch_results, file_list):
        """éªŒè¯æ‰¹é‡åˆ†æç»“æœçš„è´¨é‡"""
        if not batch_results:
            return False

        # æ£€æŸ¥ç»“æœè¦†ç›–ç‡
        covered_files = len(
            [f for f in file_list if f in batch_results and batch_results[f]]
        )
        coverage_rate = covered_files / len(file_list) if file_list else 0

        # è‡³å°‘åº”è¯¥è¦†ç›–30%çš„æ–‡ä»¶ï¼ˆè€ƒè™‘åˆ°æœ‰äº›æ–‡ä»¶å¯èƒ½æ²¡æœ‰æäº¤å†å²ï¼‰
        return coverage_rate >= 0.3

    def _get_batch_contributors_fallback(self, file_list, since_date=None):
        """æ‰¹é‡è´¡çŒ®è€…è·å–çš„å›é€€æ–¹æ³• - å¢å¼ºç‰ˆ"""
        print(f"   ğŸ“¦ ä½¿ç”¨fallbackæ–¹æ³•åˆ†æ {len(file_list)} ä¸ªæ–‡ä»¶")
        contributors_by_file = defaultdict(lambda: defaultdict(int))

        # å¦‚æœæ–‡ä»¶å¤ªå¤šï¼Œåˆ†æ‰¹å¤„ç†
        batch_size = 200  # å‡å°æ‰¹é‡å¤§å°ï¼Œé¿å…å‘½ä»¤è¡Œè¿‡é•¿
        for i in range(0, len(file_list), batch_size):
            batch_files = file_list[i : i + batch_size]
            batch_contributors = self._process_file_batch_fallback(
                batch_files, since_date
            )

            # åˆå¹¶ç»“æœ
            for file_path, contributors in batch_contributors.items():
                for author, count in contributors.items():
                    contributors_by_file[file_path][author] += count

        return dict(contributors_by_file)

    def _process_file_batch_fallback(self, file_list, since_date=None):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„æ–‡ä»¶"""
        contributors_by_file = defaultdict(lambda: defaultdict(int))

        try:
            # æ„å»ºæ–‡ä»¶å‚æ•°ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼
            files_arg = " ".join([f'"{f}"' for f in file_list])

            # æ„å»ºGitå‘½ä»¤
            if since_date:
                cmd = f'git log --since="{since_date}" --format="COMMIT:%an" --name-only -- {files_arg}'
            else:
                cmd = f'git log --format="COMMIT:%an" --name-only -- {files_arg}'

            result = self.git_ops.run_command(cmd)
            if not result:
                return dict(contributors_by_file)

            # è§£æè¾“å‡º
            lines = result.strip().split("\n")
            current_author = None

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                if line.startswith("COMMIT:"):
                    # æå–ä½œè€…å
                    current_author = line[7:].strip()  # å»æ‰ 'COMMIT:' å‰ç¼€å¹¶æ¸…ç†
                elif current_author and line in file_list:
                    # è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶åè¡Œï¼Œä¸”æ˜¯æˆ‘ä»¬å…³æ³¨çš„æ–‡ä»¶
                    contributors_by_file[line][current_author] += 1

        except Exception as e:
            print(f"   âš ï¸ æ‰¹æ¬¡å¤„ç†å‡ºé”™: {str(e)[:50]}...")
            # å¦‚æœæ‰¹é‡ä¹Ÿå¤±è´¥äº†ï¼Œå°è¯•å•ä¸ªæ–‡ä»¶åˆ†æ
            for file_path in file_list:
                try:
                    single_result = self._analyze_single_file_simple(
                        file_path, since_date
                    )
                    if single_result:
                        contributors_by_file[file_path] = single_result
                except:
                    # å•ä¸ªæ–‡ä»¶ä¹Ÿå¤±è´¥äº†ï¼Œè·³è¿‡
                    continue

        return dict(contributors_by_file)

    def _analyze_single_file_simple(self, file_path, since_date=None):
        """ç®€å•çš„å•æ–‡ä»¶åˆ†æï¼Œä¸ä½¿ç”¨--follow"""
        try:
            if since_date:
                cmd = f'git log --since="{since_date}" --format="%an" -- "{file_path}"'
            else:
                cmd = f'git log --format="%an" -- "{file_path}"'

            result = self.git_ops.run_command(cmd)
            if not result:
                return {}

            contributors = defaultdict(int)
            for author in result.split("\n"):
                author = author.strip()
                if author:
                    contributors[author] += 1

            return dict(contributors)
        except:
            return {}

    def _get_file_cache_key(self, file_path):
        """ç”Ÿæˆæ–‡ä»¶ç¼“å­˜é”® - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé¿å…é¢å¤–Gitè°ƒç”¨"""
        # ç®€åŒ–ç¼“å­˜é”®ç”Ÿæˆï¼Œé¿å…æ¯ä¸ªæ–‡ä»¶éƒ½æ‰§è¡ŒGitå‘½ä»¤
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„ + å½“å‰æ—¥æœŸä½œä¸ºç¼“å­˜é”®ï¼Œä¾é æ—¶é—´è¿‡æœŸæœºåˆ¶ä¿è¯æ•°æ®æ–°é²œåº¦
        from datetime import datetime

        date_key = datetime.now().strftime("%Y-%m-%d")
        return f"{file_path}@{date_key}"

    @performance_monitor("å¹¶è¡Œåˆ†æç»„")
    def parallel_analyze_groups(self, groups):
        """å¹¶è¡Œåˆ†æå¤šä¸ªç»„çš„è´¡çŒ®è€…ä¿¡æ¯"""
        print(f"ğŸ”„ å¼€å§‹å¹¶è¡Œåˆ†æ {len(groups)} ä¸ªç»„...")
        start_time = datetime.now()

        # é¦–å…ˆç¡®ä¿æ‰¹é‡æ•°æ®å·²å‡†å¤‡å¥½
        all_files = []
        for group in groups:
            all_files.extend(group.get("files", []))

        # æ‰¹é‡è·å–æ‰€æœ‰æ–‡ä»¶çš„è´¡çŒ®è€…ä¿¡æ¯
        self.batch_analyze_all_files(all_files)

        # å¹¶è¡Œå¤„ç†æ¯ä¸ªç»„
        results = {}
        # åŠ¨æ€ç¡®å®šçº¿ç¨‹æ•°ï¼šåŸºäºCPUæ ¸å¿ƒæ•°å’Œä»»åŠ¡æ•°
        import multiprocessing

        max_workers = min(
            multiprocessing.cpu_count() * 2,  # I/Oå¯†é›†ä»»åŠ¡å¯ä»¥ä½¿ç”¨æ›´å¤šçº¿ç¨‹
            len(groups),
            12,  # é¿å…åˆ›å»ºè¿‡å¤šçº¿ç¨‹
        )

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_group = {
                executor.submit(self._analyze_single_group, group): group
                for group in groups
            }

            for future in as_completed(future_to_group):
                group = future_to_group[future]
                try:
                    group_name = group["name"]
                    main_contributor, all_contributors = future.result()
                    results[group_name] = {
                        "main_contributor": main_contributor,
                        "all_contributors": all_contributors,
                    }
                except Exception as e:
                    print(f"âš ï¸ åˆ†æç»„ {group.get('name', 'Unknown')} æ—¶å‡ºé”™: {e}")
                    results[group.get("name", "Unknown")] = {
                        "main_contributor": None,
                        "all_contributors": {},
                    }

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âš¡ å¹¶è¡Œåˆ†æå®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f} ç§’")
        return results

    def _analyze_single_group(self, group):
        """åˆ†æå•ä¸ªç»„çš„ä¸»è¦è´¡çŒ®è€…ï¼ˆä½¿ç”¨ç¼“å­˜æ•°æ®ï¼‰"""
        files = group.get("files", [])
        all_contributors = {}

        for file in files:
            # ä»æ‰¹é‡æ•°æ®ä¸­è·å–è´¡çŒ®è€…ä¿¡æ¯
            contributors = self._batch_file_data.get(file, {})

            for author, stats in contributors.items():
                if author not in all_contributors:
                    all_contributors[author] = {
                        "total_commits": 0,
                        "recent_commits": 0,
                        "score": 0,
                        "file_count": 0,
                    }

                all_contributors[author]["total_commits"] += stats["total_commits"]
                all_contributors[author]["recent_commits"] += stats["recent_commits"]
                all_contributors[author]["score"] += stats["score"]
                all_contributors[author]["file_count"] += 1

        if not all_contributors:
            return None, {}

        # è¿”å›ç»¼åˆå¾—åˆ†æœ€é«˜çš„ä½œè€…
        main_contributor = max(all_contributors.items(), key=lambda x: x[1]["score"])
        return main_contributor[0], all_contributors

    @performance_monitor("æ‰¹é‡åˆ†æç›®å½•")
    def batch_analyze_directories(self, directory_paths):
        """æ‰¹é‡åˆ†æç›®å½•è´¡çŒ®è€…ä¿¡æ¯"""
        print(f"ğŸ“ å¼€å§‹æ‰¹é‡åˆ†æ {len(directory_paths)} ä¸ªç›®å½•...")

        results = {}
        uncached_dirs = []

        # æ£€æŸ¥ç¼“å­˜
        for dir_path in directory_paths:
            if dir_path in self._directory_contributors_cache:
                results[dir_path] = self._directory_contributors_cache[dir_path]
            else:
                uncached_dirs.append(dir_path)

        if not uncached_dirs:
            return results

        # æ‰¹é‡åˆ†ææœªç¼“å­˜çš„ç›®å½•
        one_year_ago = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")

        # åŠ¨æ€çº¿ç¨‹æ± é…ç½®
        import multiprocessing

        max_workers = min(
            multiprocessing.cpu_count() * 2, len(uncached_dirs), 8  # ç›®å½•åˆ†æçš„åˆç†ä¸Šé™
        )

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_dir = {
                executor.submit(
                    self._analyze_single_directory, dir_path, one_year_ago
                ): dir_path
                for dir_path in uncached_dirs
            }

            for future in as_completed(future_to_dir):
                dir_path = future_to_dir[future]
                try:
                    contributors = future.result()
                    results[dir_path] = contributors
                    self._directory_contributors_cache[dir_path] = contributors
                except Exception as e:
                    print(f"âš ï¸ åˆ†æç›®å½• {dir_path} æ—¶å‡ºé”™: {e}")
                    results[dir_path] = {}

        return results

    def _analyze_single_directory(self, directory_path, one_year_ago):
        """åˆ†æå•ä¸ªç›®å½•çš„è´¡çŒ®è€…"""
        contributors = {}

        # è·å–ä¸€å¹´å†…çš„è´¡çŒ®ç»Ÿè®¡ï¼ˆç›®å½•åˆ†æé€šå¸¸ä¸éœ€è¦è·Ÿè¸ªé‡å‘½åï¼‰
        recent_cmd = (
            f'git log --since="{one_year_ago}" --format="%an" -- "{directory_path}"'
        )
        recent_result = self.git_ops.run_command(recent_cmd)

        if recent_result:
            recent_authors = recent_result.split("\n")
            recent_author_counts = {}
            for author in recent_authors:
                if author.strip():
                    recent_author_counts[author] = (
                        recent_author_counts.get(author, 0) + 1
                    )

            for author, count in recent_author_counts.items():
                contributors[author] = {
                    "total_commits": count,
                    "recent_commits": count,
                    "score": count * 3,
                }

        # è·å–æ€»ä½“è´¡çŒ®ç»Ÿè®¡ï¼ˆç›®å½•åˆ†æé€šå¸¸ä¸éœ€è¦è·Ÿè¸ªé‡å‘½åï¼‰
        cmd = f'git log --format="%an" -- "{directory_path}"'
        total_result = self.git_ops.run_command(cmd)

        if total_result:
            authors = total_result.split("\n")
            author_counts = {}
            for author in authors:
                if author.strip():
                    author_counts[author] = author_counts.get(author, 0) + 1

            for author, count in author_counts.items():
                if author in contributors:
                    contributors[author]["total_commits"] = count
                    contributors[author]["score"] = (
                        contributors[author]["recent_commits"] * 3 + count
                    )
                else:
                    contributors[author] = {
                        "total_commits": count,
                        "recent_commits": 0,
                        "score": count,
                    }

        return contributors

    def get_active_contributors(self, months=DEFAULT_ACTIVE_MONTHS):
        """è·å–æ´»è·ƒè´¡çŒ®è€…ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        if self._active_contributors_cache is not None:
            return self._active_contributors_cache

        print(f"ğŸ” æ­£åœ¨åˆ†æè¿‘{months}ä¸ªæœˆçš„æ´»è·ƒè´¡çŒ®è€…...")
        active_contributors = self.git_ops.get_active_contributors(months)

        self._active_contributors_cache = active_contributors
        print(f"ğŸ“Š å‘ç° {len(active_contributors)} ä½è¿‘{months}ä¸ªæœˆæ´»è·ƒçš„è´¡çŒ®è€…")
        return active_contributors

    @performance_monitor("ä¼˜åŒ–å¤‡é€‰åˆ†é…æŸ¥æ‰¾")
    def optimized_find_fallback_assignee(self, file_paths, active_contributors):
        """ä¼˜åŒ–çš„å¤‡é€‰åˆ†é…æŸ¥æ‰¾"""
        print(f"ğŸ” æ­£åœ¨ä¸ºæœªåˆ†é…ç»„å¯»æ‰¾å¤‡é€‰è´Ÿè´£äºº...")

        # æ”¶é›†éœ€è¦åˆ†æçš„ç›®å½•
        directories_to_check = set()
        for file_path in file_paths:
            path_parts = file_path.split("/")
            for i in range(len(path_parts) - 1, 0, -1):
                parent_dir = "/".join(path_parts[:i])
                if parent_dir:
                    directories_to_check.add(parent_dir)

        # æŒ‰ç›®å½•å±‚çº§æ’åº
        sorted_dirs = sorted(
            directories_to_check, key=lambda x: x.count("/"), reverse=True
        )

        # æ‰¹é‡åˆ†æç›®å½•
        dir_contributors = self.batch_analyze_directories(sorted_dirs)

        # æŸ¥æ‰¾æœ€ä½³å€™é€‰äºº
        for directory in sorted_dirs:
            print(f"  æ£€æŸ¥ç›®å½•: {directory}")
            contributors = dir_contributors.get(directory, {})

            if contributors:
                active_dir_contributors = {
                    author: stats
                    for author, stats in contributors.items()
                    if author in active_contributors
                }

                if active_dir_contributors:
                    best_contributor = max(
                        active_dir_contributors.items(), key=lambda x: x[1]["score"]
                    )
                    print(
                        f"  âœ… åœ¨ç›®å½• {directory} æ‰¾åˆ°å€™é€‰äºº: {best_contributor[0]} (å¾—åˆ†: {best_contributor[1]['score']})"
                    )
                    return best_contributor[0], best_contributor[1], directory

        # æ£€æŸ¥æ ¹ç›®å½•
        print("  æ£€æŸ¥æ ¹ç›®å½•...")
        root_contributors = dir_contributors.get(".", {})
        if root_contributors:
            active_root_contributors = {
                author: stats
                for author, stats in root_contributors.items()
                if author in active_contributors
            }

            if active_root_contributors:
                best_contributor = max(
                    active_root_contributors.items(), key=lambda x: x[1]["score"]
                )
                print(
                    f"  âœ… åœ¨æ ¹ç›®å½•æ‰¾åˆ°å€™é€‰äºº: {best_contributor[0]} (å¾—åˆ†: {best_contributor[1]['score']})"
                )
                return best_contributor[0], best_contributor[1], "æ ¹ç›®å½•"

        print("  âŒ æœªæ‰¾åˆ°åˆé€‚çš„å€™é€‰äºº")
        return None, None, None

    # ä¿æŒä¸åŸæ¥å£çš„å…¼å®¹æ€§
    def analyze_file_contributors(self, filepath, include_recent=True):
        """æ–‡ä»¶è´¡çŒ®è€…åˆ†æï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        if not self._batch_computed:
            # å¦‚æœæ²¡æœ‰æ‰¹é‡è®¡ç®—ï¼Œå•ç‹¬åˆ†æè¿™ä¸ªæ–‡ä»¶
            self.batch_analyze_all_files([filepath])

        return self._batch_file_data.get(filepath, {})

    def get_group_main_contributor(self, files):
        """è·å–ç»„ä¸»è¦è´¡çŒ®è€…ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        if not self._batch_computed:
            self.batch_analyze_all_files(files)

        group = {"files": files}
        return self._analyze_single_group(group)

    def find_fallback_assignee(self, file_paths, active_contributors):
        """å¤‡é€‰åˆ†é…ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        return self.optimized_find_fallback_assignee(file_paths, active_contributors)

    def get_all_contributors(self):
        """è·å–æ‰€æœ‰å†å²è´¡çŒ®è€…"""
        if self._all_contributors_cache is not None:
            return self._all_contributors_cache

        all_contributors = self.git_ops.get_all_contributors_global()
        self._all_contributors_cache = all_contributors
        return all_contributors

    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "cached_files": len(self._file_contributors_cache),
            "cached_directories": len(self._directory_contributors_cache),
            "batch_computed": self._batch_computed,
            "cache_file_exists": self.cache_file.exists(),
        }

    # è®¡ç®—å…¨å±€è´¡çŒ®è€…ç»Ÿè®¡ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
    def calculate_global_contributor_stats(self, plan):
        """è®¡ç®—å…¨å±€è´¡çŒ®è€…ç»Ÿè®¡"""
        all_contributors_global = {}

        for group in plan["groups"]:
            contributors = group.get("contributors", {})
            for author, stats in contributors.items():
                if author not in all_contributors_global:
                    all_contributors_global[author] = {
                        "total_commits": 0,
                        "recent_commits": 0,
                        "score": 0,
                        "groups_contributed": 0,
                        "groups_assigned": [],
                        "is_active": author in self.get_active_contributors(),
                    }

                if isinstance(stats, dict):
                    all_contributors_global[author]["recent_commits"] += stats[
                        "recent_commits"
                    ]
                    all_contributors_global[author]["total_commits"] += stats[
                        "total_commits"
                    ]
                    all_contributors_global[author]["score"] += stats["score"]
                else:
                    all_contributors_global[author]["total_commits"] += stats
                    all_contributors_global[author]["score"] += stats

                all_contributors_global[author]["groups_contributed"] += 1

                # æ£€æŸ¥æ˜¯å¦è¢«åˆ†é…åˆ°è¿™ä¸ªç»„
                if group.get("assignee") == author:
                    all_contributors_global[author]["groups_assigned"].append(
                        group["name"]
                    )

        return all_contributors_global

    def get_workload_distribution(self, plan):
        """è·å–è´Ÿè½½åˆ†å¸ƒç»Ÿè®¡"""
        if not plan or "groups" not in plan:
            return {}

        assignee_workload = {}
        for group in plan["groups"]:
            assignee = group.get("assignee")
            if assignee and assignee != "æœªåˆ†é…":
                if assignee not in assignee_workload:
                    assignee_workload[assignee] = {
                        "groups": 0,
                        "files": 0,
                        "completed": 0,
                        "fallback": 0,
                    }
                assignee_workload[assignee]["groups"] += 1
                assignee_workload[assignee]["files"] += group.get(
                    "file_count", len(group.get("files", []))
                )
                if group.get("status") == "completed":
                    assignee_workload[assignee]["completed"] += 1
                if group.get("fallback_reason"):
                    assignee_workload[assignee]["fallback"] += 1

        return assignee_workload

    def get_assignment_reason_stats(self, plan):
        """è·å–åˆ†é…åŸå› ç»Ÿè®¡"""
        from ui.display_helper import DisplayHelper

        reason_stats = {}
        for group in plan["groups"]:
            assignment_reason = group.get("assignment_reason", "æœªæŒ‡å®š")
            reason_type = DisplayHelper.categorize_assignment_reason(assignment_reason)

            if reason_type not in reason_stats:
                reason_stats[reason_type] = []
            reason_stats[reason_type].append(group)

        return reason_stats

    def _generate_performance_report(self, analysis_stats):
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        if not analysis_stats:
            return "âš ï¸ æ— æ€§èƒ½æ•°æ®å¯ç”¨"

        total_time = analysis_stats.get("total_time", 0)
        batch_time = analysis_stats.get("batch_time", 0)
        single_time = analysis_stats.get("single_time", 0)
        total_files = analysis_stats.get("total_files", 0)
        batch_files = analysis_stats.get("batch_files", 0)
        single_files = analysis_stats.get("single_files", 0)
        critical_ratio = analysis_stats.get("critical_ratio", 0)
        follow_usage = analysis_stats.get("follow_usage", 0)

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_time_per_file = total_time / max(total_files, 1)
        batch_efficiency = batch_time / max(batch_files, 1) if batch_files > 0 else 0
        single_efficiency = (
            single_time / max(single_files, 1) if single_files > 0 else 0
        )

        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        if avg_time_per_file < 0.1:
            perf_level = "ğŸš€ ä¼˜ç§€"
        elif avg_time_per_file < 0.5:
            perf_level = "âœ… è‰¯å¥½"
        elif avg_time_per_file < 1.0:
            perf_level = "âš ï¸ ä¸€èˆ¬"
        else:
            perf_level = "âŒ éœ€ä¼˜åŒ–"

        report = [
            f"âš¡ æ€§èƒ½åˆ†ææŠ¥å‘Š:",
            f"   â€¢ æ€»è€—æ—¶: {total_time:.2f}ç§’",
            f"   â€¢ å¹³å‡æ¯æ–‡ä»¶: {avg_time_per_file:.3f}ç§’",
            f"   â€¢ æ€§èƒ½ç­‰çº§: {perf_level}",
            f"   â€¢ å…³é”®æ–‡ä»¶æ¯”ä¾‹: {critical_ratio:.1f}%",
            f"   â€¢ --followä½¿ç”¨ç‡: {follow_usage:.1f}%",
            "",
            f"ğŸ“ˆ å¤„ç†åˆ†æ:",
            f"   â€¢ æ‰¹é‡å¤„ç†: {batch_files}ä¸ªæ–‡ä»¶ ({batch_time:.2f}ç§’)",
            f"   â€¢ å•ç‹¬å¤„ç†: {single_files}ä¸ªæ–‡ä»¶ ({single_time:.2f}ç§’)",
        ]

        if batch_files > 0:
            report.append(f"   â€¢ æ‰¹é‡æ•ˆç‡: {batch_efficiency:.3f}ç§’/æ–‡ä»¶")
        if single_files > 0:
            report.append(f"   â€¢ å•ç‹¬æ•ˆç‡: {single_efficiency:.3f}ç§’/æ–‡ä»¶")

        # ä¼˜åŒ–å»ºè®®
        suggestions = []
        if critical_ratio > 15:
            suggestions.append("å»ºè®®ï¼šå‡å°‘å…³é”®æ–‡ä»¶åˆ†ç±»ä»¥æå‡æ‰¹é‡å¤„ç†æ•ˆç‡")
        if follow_usage > 20:
            suggestions.append("å»ºè®®ï¼šä¼˜åŒ–æ–‡ä»¶åˆ†ç±»ç­–ç•¥ä»¥å‡å°‘--followä½¿ç”¨")
        if avg_time_per_file > 0.5:
            suggestions.append("å»ºè®®ï¼šæ£€æŸ¥Gitä»“åº“å¤§å°å’Œç½‘ç»œè¿æ¥")

        if suggestions:
            report.append("")
            report.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for suggestion in suggestions:
                report.append(f"   â€¢ {suggestion}")

        return "\n".join(report)
    
    def _save_performance_log(self, file_list, total_time, step_times):
        """ä¿å­˜æ€§èƒ½æ—¥å¿—åˆ°æ–‡ä»¶"""
        try:
            # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
            if hasattr(self.git_ops, 'repo_path'):
                repo_path = Path(self.git_ops.repo_path)
            else:
                repo_path = Path(".")
                
            log_file = repo_path / ".merge_work" / "performance_log.json"
            log_file.parent.mkdir(exist_ok=True)
            
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'file_count': len(file_list),
                'total_time': total_time,
                'avg_time_per_file': total_time / len(file_list) * 1000,  # ms
                'step_times': step_times,
                'mode': step_times.get('mode', 'optimized_traditional')
            }
            
            # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼ŒåŠ è½½ç°æœ‰æ—¥å¿—
            logs = []
            if log_file.exists():
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        logs = json.load(f)
                except:
                    logs = []
            
            # æ·»åŠ æ–°æ—¥å¿—ï¼ˆä¿ç•™æœ€è¿‘50æ¡ï¼‰
            logs.append(log_entry)
            logs = logs[-50:]
            
            # ä¿å­˜æ—¥å¿—
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(logs, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“ [PERF] æ€§èƒ½æ—¥å¿—å·²ä¿å­˜: {log_file}")
            
        except Exception as e:
            print(f"âš ï¸ [PERF] ä¿å­˜æ€§èƒ½æ—¥å¿—å¤±è´¥: {e}")
