"""
Git Merge Orchestrator - åŠ é€Ÿä¼˜åŒ–çš„è´¡çŒ®è€…åˆ†æå™¨
é€šè¿‡æ‰¹é‡Gitå‘½ä»¤ã€æ™ºèƒ½ç¼“å­˜å’Œå¹¶è¡Œå¤„ç†å¤§å¹…æå‡åˆ†æé€Ÿåº¦
"""

import json
import hashlib
import threading
from datetime import datetime, timedelta
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from config import SCORING_WEIGHTS, DEFAULT_ANALYSIS_MONTHS, DEFAULT_ACTIVE_MONTHS
from utils.performance_monitor import (
    performance_monitor,
    timing_context,
    global_performance_stats,
)


class OptimizedContributorAnalyzer:
    """ä¼˜åŒ–çš„è´¡çŒ®è€…åˆ†æå™¨ - å¤§å¹…æå‡æ€§èƒ½"""

    def __init__(self, git_ops):
        self.git_ops = git_ops
        self.cache_file = (
            Path(git_ops.repo_path) / ".merge_work" / "contributor_cache.json"
        )
        self.cache_lock = threading.Lock()

        # å†…å­˜ç¼“å­˜
        self._file_contributors_cache = {}
        self._directory_contributors_cache = {}
        self._active_contributors_cache = None
        self._all_contributors_cache = None

        # æ‰¹é‡æ•°æ®ç¼“å­˜
        self._batch_file_data = {}
        self._batch_computed = False

    def load_persistent_cache(self):
        """åŠ è½½æŒä¹…åŒ–ç¼“å­˜"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    cache_data = json.load(f)

                # æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§ï¼ˆ24å°æ—¶ï¼‰
                cache_time = datetime.fromisoformat(
                    cache_data.get("created_at", "2000-01-01")
                )
                if datetime.now() - cache_time < timedelta(hours=24):
                    self._file_contributors_cache = cache_data.get(
                        "file_contributors", {}
                    )
                    self._directory_contributors_cache = cache_data.get(
                        "directory_contributors", {}
                    )
                    print(f"âœ… å·²åŠ è½½è´¡çŒ®è€…ç¼“å­˜ ({len(self._file_contributors_cache)} æ–‡ä»¶)")
                    return True
                else:
                    print("â° ç¼“å­˜å·²è¿‡æœŸï¼Œå°†é‡æ–°è®¡ç®—")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return False

    def save_persistent_cache(self):
        """ä¿å­˜æŒä¹…åŒ–ç¼“å­˜"""
        try:
            cache_data = {
                "created_at": datetime.now().isoformat(),
                "file_contributors": self._file_contributors_cache,
                "directory_contributors": self._directory_contributors_cache,
            }

            self.cache_file.parent.mkdir(exist_ok=True)
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ å·²ä¿å­˜è´¡çŒ®è€…ç¼“å­˜")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    @performance_monitor("æ‰¹é‡åˆ†ææ‰€æœ‰æ–‡ä»¶")
    def batch_analyze_all_files(self, file_list):
        """æ‰¹é‡åˆ†ææ‰€æœ‰æ–‡ä»¶çš„è´¡çŒ®è€…ä¿¡æ¯ - æ ¸å¿ƒåŠ é€Ÿå‡½æ•°"""
        if self._batch_computed:
            return self._batch_file_data

        print(f"ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ {len(file_list)} ä¸ªæ–‡ä»¶çš„è´¡çŒ®è€…ä¿¡æ¯...")
        start_time = datetime.now()

        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜
        cached_files = set()
        uncached_files = []

        for file_path in file_list:
            cache_key = self._get_file_cache_key(file_path)
            if cache_key in self._file_contributors_cache:
                self._batch_file_data[file_path] = self._file_contributors_cache[
                    cache_key
                ]
                cached_files.add(file_path)
            else:
                uncached_files.append(file_path)

        if cached_files:
            print(f"ğŸ“¦ ä»ç¼“å­˜è·å– {len(cached_files)} ä¸ªæ–‡ä»¶çš„è´¡çŒ®è€…ä¿¡æ¯")

        if not uncached_files:
            print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½æœ‰ç¼“å­˜ï¼Œè·³è¿‡Gitåˆ†æ")
            self._batch_computed = True
            return self._batch_file_data

        print(f"ğŸ” éœ€è¦åˆ†æ {len(uncached_files)} ä¸ªæ–°æ–‡ä»¶")

        # æ‰¹é‡è·å–ä¸€å¹´å†…çš„è´¡çŒ®ç»Ÿè®¡ - ä¿®å¤ç‰ˆ
        one_year_ago = (
            datetime.now() - timedelta(days=DEFAULT_ANALYSIS_MONTHS * 30)
        ).strftime("%Y-%m-%d")

        # åˆ†æ‰¹å¤„ç†æ–‡ä»¶ï¼Œé¿å…å‘½ä»¤è¡Œè¿‡é•¿
        batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ªæ–‡ä»¶
        recent_contributors = {}
        total_contributors = {}

        for i in range(0, len(uncached_files), batch_size):
            batch_files = uncached_files[i : i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(uncached_files) + batch_size - 1) // batch_size

            print(f"  ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_files)} æ–‡ä»¶)")

            # æ„å»ºæ–‡ä»¶å‚æ•°ï¼Œå»æ‰ --follow é€‰é¡¹
            files_arg = " ".join([f'"{f}"' for f in batch_files])

            # ä¸€æ¬¡æ€§è·å–è¿™æ‰¹æ–‡ä»¶çš„è¿‘æœŸè´¡çŒ®è€…ï¼ˆå»æ‰--followï¼‰
            recent_cmd = f'git log --since="{one_year_ago}" --format="%an|%H|%s" --name-only -- {files_arg}'
            recent_result = self.git_ops.run_command(recent_cmd)

            # ä¸€æ¬¡æ€§è·å–è¿™æ‰¹æ–‡ä»¶çš„å†å²è´¡çŒ®è€…ï¼ˆå»æ‰--followï¼‰
            total_cmd = f'git log --format="%an|%H|%s" --name-only -- {files_arg}'
            total_result = self.git_ops.run_command(total_cmd)

            # è§£ææ‰¹é‡ç»“æœå¹¶åˆå¹¶
            batch_recent = self._parse_batch_git_output(recent_result, batch_files)
            batch_total = self._parse_batch_git_output(total_result, batch_files)

            recent_contributors.update(batch_recent)
            total_contributors.update(batch_total)

        # è®¡ç®—æ¯ä¸ªæ–‡ä»¶çš„è´¡çŒ®è€…ç»Ÿè®¡
        for file_path in uncached_files:
            contributors = {}

            # å¤„ç†è¿‘æœŸè´¡çŒ®
            recent_authors = recent_contributors.get(file_path, {})
            for author, count in recent_authors.items():
                contributors[author] = {
                    "total_commits": count,
                    "recent_commits": count,
                    "score": count * SCORING_WEIGHTS["recent_commits"],
                }

            # å¤„ç†å†å²è´¡çŒ®
            total_authors = total_contributors.get(file_path, {})
            for author, count in total_authors.items():
                if author in contributors:
                    contributors[author]["total_commits"] = count
                    contributors[author]["score"] = (
                        contributors[author]["recent_commits"]
                        * SCORING_WEIGHTS["recent_commits"]
                        + count * SCORING_WEIGHTS["total_commits"]
                    )
                else:
                    contributors[author] = {
                        "total_commits": count,
                        "recent_commits": 0,
                        "score": count * SCORING_WEIGHTS["total_commits"],
                    }

            # ç¼“å­˜ç»“æœ
            cache_key = self._get_file_cache_key(file_path)
            self._file_contributors_cache[cache_key] = contributors
            self._batch_file_data[file_path] = contributors

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âš¡ æ‰¹é‡åˆ†æå®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f} ç§’")
        print(f"ğŸ“Š å¤„ç†æ–‡ä»¶: {len(file_list)} ä¸ªï¼Œç¼“å­˜å‘½ä¸­: {len(cached_files)} ä¸ª")

        # ä¿å­˜ç¼“å­˜
        self.save_persistent_cache()
        self._batch_computed = True
        return self._batch_file_data

    # å¦‚æœéœ€è¦æ–‡ä»¶é‡å‘½åè·Ÿè¸ªåŠŸèƒ½ï¼Œæ·»åŠ å•ç‹¬çš„æ–¹æ³•
    def analyze_file_with_follow(self, filepath):
        """å•æ–‡ä»¶åˆ†æï¼ˆæ”¯æŒé‡å‘½åè·Ÿè¸ªï¼‰- ç”¨äºé‡è¦æ–‡ä»¶"""
        print(f"ğŸ” æ·±åº¦åˆ†ææ–‡ä»¶: {filepath} (æ”¯æŒé‡å‘½åè·Ÿè¸ª)")

        try:
            contributors = {}

            # è·å–ä¸€å¹´å†…çš„è´¡çŒ®ç»Ÿè®¡ï¼ˆä½¿ç”¨--followï¼‰
            one_year_ago = (
                datetime.now() - timedelta(days=DEFAULT_ANALYSIS_MONTHS * 30)
            ).strftime("%Y-%m-%d")
            recent_cmd = f'git log --follow --since="{one_year_ago}" --format="%an" -- "{filepath}"'
            recent_result = self.git_ops.run_command(recent_cmd)

            if recent_result:
                recent_authors = recent_result.split("\n")
                recent_author_counts = {}
                for author in recent_authors:
                    if author.strip():
                        recent_author_counts[author] = (
                            recent_author_counts.get(author, 0) + 1
                        )

                for author, count in recent_author_counts.items():
                    contributors[author] = {
                        "total_commits": count,
                        "recent_commits": count,
                        "score": count * SCORING_WEIGHTS["recent_commits"],
                    }

            # è·å–æ€»ä½“è´¡çŒ®ç»Ÿè®¡ï¼ˆä½¿ç”¨--followï¼‰
            total_cmd = f'git log --follow --format="%an" -- "{filepath}"'
            total_result = self.git_ops.run_command(total_cmd)

            if total_result:
                authors = total_result.split("\n")
                author_counts = {}
                for author in authors:
                    if author.strip():
                        author_counts[author] = author_counts.get(author, 0) + 1

                for author, count in author_counts.items():
                    if author in contributors:
                        contributors[author]["total_commits"] = count
                        contributors[author]["score"] = (
                            contributors[author]["recent_commits"]
                            * SCORING_WEIGHTS["recent_commits"]
                            + count * SCORING_WEIGHTS["total_commits"]
                        )
                    else:
                        contributors[author] = {
                            "total_commits": count,
                            "recent_commits": 0,
                            "score": count * SCORING_WEIGHTS["total_commits"],
                        }

            return contributors
        except Exception as e:
            print(f"æ·±åº¦åˆ†ææ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
            return {}

    def _parse_batch_git_output(self, git_output, target_files):
        """è§£ææ‰¹é‡Gitè¾“å‡ºï¼Œæå–æ¯ä¸ªæ–‡ä»¶çš„è´¡çŒ®è€…ç»Ÿè®¡"""
        if not git_output:
            return {}

        contributors_by_file = defaultdict(lambda: defaultdict(int))
        current_author = None
        current_files = []

        lines = git_output.strip().split("\n")
        for line in lines:
            line = line.strip()
            if not line:
                continue

            # è§£ææäº¤ä¿¡æ¯è¡Œ (æ ¼å¼: author|hash|subject)
            if "|" in line:
                parts = line.split("|", 2)
                if len(parts) >= 1:
                    current_author = parts[0]
                    current_files = []
            else:
                # è¿™æ˜¯æ–‡ä»¶åè¡Œ
                if current_author and line in target_files:
                    contributors_by_file[line][current_author] += 1

        return dict(contributors_by_file)

    def _get_file_cache_key(self, file_path):
        """ç”Ÿæˆæ–‡ä»¶ç¼“å­˜é”®ï¼ˆåŸºäºæ–‡ä»¶è·¯å¾„å’Œæœ€åä¿®æ”¹æ—¶é—´ï¼‰"""
        try:
            # è·å–æ–‡ä»¶çš„æœ€åæäº¤hashä½œä¸ºç¼“å­˜é”®çš„ä¸€éƒ¨åˆ†
            cmd = f'git log -1 --format="%H" -- "{file_path}"'
            last_commit = self.git_ops.run_command(cmd)
            if last_commit:
                return f"{file_path}:{last_commit}"
        except:
            pass

        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨æ–‡ä»¶è·¯å¾„
        return file_path

    @performance_monitor("å¹¶è¡Œåˆ†æç»„")
    def parallel_analyze_groups(self, groups):
        """å¹¶è¡Œåˆ†æå¤šä¸ªç»„çš„è´¡çŒ®è€…ä¿¡æ¯"""
        print(f"ğŸ”„ å¼€å§‹å¹¶è¡Œåˆ†æ {len(groups)} ä¸ªç»„...")
        start_time = datetime.now()

        # é¦–å…ˆç¡®ä¿æ‰¹é‡æ•°æ®å·²å‡†å¤‡å¥½
        all_files = []
        for group in groups:
            all_files.extend(group.get("files", []))

        # æ‰¹é‡è·å–æ‰€æœ‰æ–‡ä»¶çš„è´¡çŒ®è€…ä¿¡æ¯
        self.smart_batch_analyze_files(all_files)

        # å¹¶è¡Œå¤„ç†æ¯ä¸ªç»„
        results = {}
        with ThreadPoolExecutor(max_workers=min(4, len(groups))) as executor:
            future_to_group = {
                executor.submit(self._analyze_single_group, group): group
                for group in groups
            }

            for future in as_completed(future_to_group):
                group = future_to_group[future]
                try:
                    group_name = group["name"]
                    main_contributor, all_contributors = future.result()
                    results[group_name] = {
                        "main_contributor": main_contributor,
                        "all_contributors": all_contributors,
                    }
                except Exception as e:
                    print(f"âš ï¸ åˆ†æç»„ {group.get('name', 'Unknown')} æ—¶å‡ºé”™: {e}")
                    results[group.get("name", "Unknown")] = {
                        "main_contributor": None,
                        "all_contributors": {},
                    }

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âš¡ å¹¶è¡Œåˆ†æå®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f} ç§’")
        return results

    def _analyze_single_group(self, group):
        """åˆ†æå•ä¸ªç»„çš„ä¸»è¦è´¡çŒ®è€…ï¼ˆä½¿ç”¨ç¼“å­˜æ•°æ®ï¼‰"""
        files = group.get("files", [])
        all_contributors = {}

        for file in files:
            # ä»æ‰¹é‡æ•°æ®ä¸­è·å–è´¡çŒ®è€…ä¿¡æ¯
            contributors = self._batch_file_data.get(file, {})

            for author, stats in contributors.items():
                if author not in all_contributors:
                    all_contributors[author] = {
                        "total_commits": 0,
                        "recent_commits": 0,
                        "score": 0,
                        "file_count": 0,
                    }

                all_contributors[author]["total_commits"] += stats["total_commits"]
                all_contributors[author]["recent_commits"] += stats["recent_commits"]
                all_contributors[author]["score"] += stats["score"]
                all_contributors[author]["file_count"] += 1

        if not all_contributors:
            return None, {}

        # è¿”å›ç»¼åˆå¾—åˆ†æœ€é«˜çš„ä½œè€…
        main_contributor = max(all_contributors.items(), key=lambda x: x[1]["score"])
        return main_contributor[0], all_contributors

    @performance_monitor("æ‰¹é‡åˆ†æç›®å½•")
    def batch_analyze_directories(self, directory_paths):
        """æ‰¹é‡åˆ†æç›®å½•è´¡çŒ®è€…ä¿¡æ¯"""
        print(f"ğŸ“ å¼€å§‹æ‰¹é‡åˆ†æ {len(directory_paths)} ä¸ªç›®å½•...")

        results = {}
        uncached_dirs = []

        # æ£€æŸ¥ç¼“å­˜
        for dir_path in directory_paths:
            if dir_path in self._directory_contributors_cache:
                results[dir_path] = self._directory_contributors_cache[dir_path]
            else:
                uncached_dirs.append(dir_path)

        if not uncached_dirs:
            return results

        # æ‰¹é‡åˆ†ææœªç¼“å­˜çš„ç›®å½•
        one_year_ago = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")

        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_dir = {
                executor.submit(
                    self._analyze_single_directory, dir_path, one_year_ago
                ): dir_path
                for dir_path in uncached_dirs
            }

            for future in as_completed(future_to_dir):
                dir_path = future_to_dir[future]
                try:
                    contributors = future.result()
                    results[dir_path] = contributors
                    self._directory_contributors_cache[dir_path] = contributors
                except Exception as e:
                    print(f"âš ï¸ åˆ†æç›®å½• {dir_path} æ—¶å‡ºé”™: {e}")
                    results[dir_path] = {}

        return results

    def _analyze_single_directory(self, directory_path, one_year_ago):
        """åˆ†æå•ä¸ªç›®å½•çš„è´¡çŒ®è€…"""
        contributors = {}

        # è·å–ä¸€å¹´å†…çš„è´¡çŒ®ç»Ÿè®¡
        recent_cmd = f'git log --follow --since="{one_year_ago}" --format="%an" -- "{directory_path}"'
        recent_result = self.git_ops.run_command(recent_cmd)

        if recent_result:
            recent_authors = recent_result.split("\n")
            recent_author_counts = {}
            for author in recent_authors:
                if author.strip():
                    recent_author_counts[author] = (
                        recent_author_counts.get(author, 0) + 1
                    )

            for author, count in recent_author_counts.items():
                contributors[author] = {
                    "total_commits": count,
                    "recent_commits": count,
                    "score": count * 3,
                }

        # è·å–æ€»ä½“è´¡çŒ®ç»Ÿè®¡
        cmd = f'git log --follow --format="%an" -- "{directory_path}"'
        total_result = self.git_ops.run_command(cmd)

        if total_result:
            authors = total_result.split("\n")
            author_counts = {}
            for author in authors:
                if author.strip():
                    author_counts[author] = author_counts.get(author, 0) + 1

            for author, count in author_counts.items():
                if author in contributors:
                    contributors[author]["total_commits"] = count
                    contributors[author]["score"] = (
                        contributors[author]["recent_commits"] * 3 + count
                    )
                else:
                    contributors[author] = {
                        "total_commits": count,
                        "recent_commits": 0,
                        "score": count,
                    }

        return contributors

    def get_active_contributors(self, months=DEFAULT_ACTIVE_MONTHS):
        """è·å–æ´»è·ƒè´¡çŒ®è€…ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        if self._active_contributors_cache is not None:
            return self._active_contributors_cache

        print(f"ğŸ” æ­£åœ¨åˆ†æè¿‘{months}ä¸ªæœˆçš„æ´»è·ƒè´¡çŒ®è€…...")
        active_contributors = self.git_ops.get_active_contributors(months)

        self._active_contributors_cache = active_contributors
        print(f"ğŸ“Š å‘ç° {len(active_contributors)} ä½è¿‘{months}ä¸ªæœˆæ´»è·ƒçš„è´¡çŒ®è€…")
        return active_contributors

    @performance_monitor("ä¼˜åŒ–å¤‡é€‰åˆ†é…æŸ¥æ‰¾")
    def optimized_find_fallback_assignee(self, file_paths, active_contributors):
        """ä¼˜åŒ–çš„å¤‡é€‰åˆ†é…æŸ¥æ‰¾"""
        print(f"ğŸ” æ­£åœ¨ä¸ºæœªåˆ†é…ç»„å¯»æ‰¾å¤‡é€‰è´Ÿè´£äºº...")

        # æ”¶é›†éœ€è¦åˆ†æçš„ç›®å½•
        directories_to_check = set()
        for file_path in file_paths:
            path_parts = file_path.split("/")
            for i in range(len(path_parts) - 1, 0, -1):
                parent_dir = "/".join(path_parts[:i])
                if parent_dir:
                    directories_to_check.add(parent_dir)

        # æŒ‰ç›®å½•å±‚çº§æ’åº
        sorted_dirs = sorted(
            directories_to_check, key=lambda x: x.count("/"), reverse=True
        )

        # æ‰¹é‡åˆ†æç›®å½•
        dir_contributors = self.batch_analyze_directories(sorted_dirs)

        # æŸ¥æ‰¾æœ€ä½³å€™é€‰äºº
        for directory in sorted_dirs:
            print(f"  æ£€æŸ¥ç›®å½•: {directory}")
            contributors = dir_contributors.get(directory, {})

            if contributors:
                active_dir_contributors = {
                    author: stats
                    for author, stats in contributors.items()
                    if author in active_contributors
                }

                if active_dir_contributors:
                    best_contributor = max(
                        active_dir_contributors.items(), key=lambda x: x[1]["score"]
                    )
                    print(
                        f"  âœ… åœ¨ç›®å½• {directory} æ‰¾åˆ°å€™é€‰äºº: {best_contributor[0]} (å¾—åˆ†: {best_contributor[1]['score']})"
                    )
                    return best_contributor[0], best_contributor[1], directory

        # æ£€æŸ¥æ ¹ç›®å½•
        print("  æ£€æŸ¥æ ¹ç›®å½•...")
        root_contributors = dir_contributors.get(".", {})
        if root_contributors:
            active_root_contributors = {
                author: stats
                for author, stats in root_contributors.items()
                if author in active_contributors
            }

            if active_root_contributors:
                best_contributor = max(
                    active_root_contributors.items(), key=lambda x: x[1]["score"]
                )
                print(
                    f"  âœ… åœ¨æ ¹ç›®å½•æ‰¾åˆ°å€™é€‰äºº: {best_contributor[0]} (å¾—åˆ†: {best_contributor[1]['score']})"
                )
                return best_contributor[0], best_contributor[1], "æ ¹ç›®å½•"

        print("  âŒ æœªæ‰¾åˆ°åˆé€‚çš„å€™é€‰äºº")
        return None, None, None

    # ä¿æŒä¸åŸæ¥å£çš„å…¼å®¹æ€§
    def analyze_file_contributors(self, filepath, include_recent=True):
        """æ–‡ä»¶è´¡çŒ®è€…åˆ†æï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        if not self._batch_computed:
            # å¦‚æœæ²¡æœ‰æ‰¹é‡è®¡ç®—ï¼Œå•ç‹¬åˆ†æè¿™ä¸ªæ–‡ä»¶
            self.smart_batch_analyze_files([filepath])

        return self._batch_file_data.get(filepath, {})

    def get_group_main_contributor(self, files):
        """è·å–ç»„ä¸»è¦è´¡çŒ®è€…ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        if not self._batch_computed:
            self.smart_batch_analyze_files(files)

        group = {"files": files}
        return self._analyze_single_group(group)

    def find_fallback_assignee(self, file_paths, active_contributors):
        """å¤‡é€‰åˆ†é…ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        return self.optimized_find_fallback_assignee(file_paths, active_contributors)

    def get_all_contributors(self):
        """è·å–æ‰€æœ‰å†å²è´¡çŒ®è€…"""
        if self._all_contributors_cache is not None:
            return self._all_contributors_cache

        all_contributors = self.git_ops.get_all_contributors_global()
        self._all_contributors_cache = all_contributors
        return all_contributors

    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "cached_files": len(self._file_contributors_cache),
            "cached_directories": len(self._directory_contributors_cache),
            "batch_computed": self._batch_computed,
            "cache_file_exists": self.cache_file.exists(),
        }

    # æ··åˆç­–ç•¥ï¼šé‡è¦æ–‡ä»¶ä½¿ç”¨followï¼Œå…¶ä»–æ–‡ä»¶æ‰¹é‡å¤„ç†
    def smart_batch_analyze_files(self, file_list, important_files=None):
        """æ™ºèƒ½æ‰¹é‡åˆ†æï¼šé‡è¦æ–‡ä»¶æ·±åº¦åˆ†æï¼Œå…¶ä»–æ–‡ä»¶æ‰¹é‡å¤„ç†"""
        important_files = important_files or []

        # åˆ†ç¦»é‡è¦æ–‡ä»¶å’Œæ™®é€šæ–‡ä»¶
        important_set = set(important_files)
        regular_files = [f for f in file_list if f not in important_set]
        existing_important_files = [f for f in file_list if f in important_set]

        print(f"ğŸ¯ æ™ºèƒ½åˆ†æç­–ç•¥:")
        print(f"   é‡è¦æ–‡ä»¶ (æ·±åº¦åˆ†æ): {len(existing_important_files)} ä¸ª")
        print(f"   æ™®é€šæ–‡ä»¶ (æ‰¹é‡åˆ†æ): {len(regular_files)} ä¸ª")

        # æ‰¹é‡å¤„ç†æ™®é€šæ–‡ä»¶
        if regular_files:
            self.batch_analyze_all_files(regular_files)

        # æ·±åº¦åˆ†æé‡è¦æ–‡ä»¶
        for important_file in existing_important_files:
            if important_file not in self._batch_file_data:
                contributors = self.analyze_file_with_follow(important_file)
                self._batch_file_data[important_file] = contributors

                # æ›´æ–°ç¼“å­˜
                cache_key = self._get_file_cache_key(important_file)
                self._file_contributors_cache[cache_key] = contributors

        return self._batch_file_data
