"""
Ultra Fast Contributor Analyzer
é©å‘½æ€§æ€§èƒ½ä¼˜åŒ–ï¼šå…¨å±€åˆ†æ + æ™ºèƒ½æ¨æ–­
é¢„æœŸæ€§èƒ½æå‡ï¼š10-100å€
"""

import subprocess
import json
import time
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path


class UltraFastAnalyzer:
    """è¶…é«˜é€Ÿè´¡çŒ®è€…åˆ†æå™¨"""
    
    def __init__(self, repo_path="."):
        self.repo_path = Path(repo_path)
        self.cache_file = self.repo_path / ".merge_work" / "ultra_cache.json"
        self.cache_expiry_hours = 6  # 6å°æ—¶ç¼“å­˜è¿‡æœŸ
        
    def analyze_contributors_ultra_fast(self, file_list, months=6, force_incremental=False):
        """è¶…é«˜é€Ÿåˆ†æ - å…¨å±€åˆ†æ + æ™ºèƒ½æ¨æ–­ + å¢é‡æ›´æ–°"""
        print(f"ğŸš€ å¼€å§‹è¶…é«˜é€Ÿåˆ†æ {len(file_list)} ä¸ªæ–‡ä»¶...")
        start_time = time.time()
        
        # 1. æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨å¢é‡æ›´æ–°
        if not force_incremental and self._should_use_incremental_update(file_list):
            return self._incremental_update_analysis(file_list, months)
        
        # 2. æ£€æŸ¥ç¼“å­˜
        if self._is_cache_valid():
            print("âš¡ ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œç¬é—´å®Œæˆ")
            cached_data = self._load_cache()
            return self._extract_file_results(cached_data, file_list)
        
        # 3. å…¨å±€åˆ†æ - ä¸€æ¬¡Gitè°ƒç”¨è·å–æ‰€æœ‰ä¿¡æ¯
        since_date = (datetime.now() - timedelta(days=months * 30)).strftime("%Y-%m-%d")
        global_data = self._global_analysis(since_date)
        
        # 4. æ™ºèƒ½åˆ†é…
        results = self._intelligent_assignment(global_data, file_list)
        
        # 5. ä¿å­˜ç¼“å­˜
        self._save_cache(global_data)
        
        elapsed = time.time() - start_time
        print(f"âš¡ è¶…é«˜é€Ÿåˆ†æå®Œæˆ: {elapsed:.3f}s (å¹³å‡ {elapsed/len(file_list)*1000:.1f}ms/æ–‡ä»¶)")
        
        return results
    
    def _should_use_incremental_update(self, file_list):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¢é‡æ›´æ–°"""
        if not self.cache_file.exists():
            return False
            
        try:
            cached_data = self._load_cache()
            cached_files = set(cached_data['file_contributors'].keys())
            
            # å¦‚æœè¶…è¿‡80%çš„æ–‡ä»¶å·²åœ¨ç¼“å­˜ä¸­ï¼Œä¸”æ–‡ä»¶æ€»æ•°ä¸å°‘ï¼Œä½¿ç”¨å¢é‡æ›´æ–°
            overlap = len(set(file_list) & cached_files)
            overlap_ratio = overlap / len(file_list) if file_list else 0
            
            should_use = overlap_ratio > 0.8 and len(file_list) > 20
            if should_use:
                print(f"ğŸ“Š å¢é‡æ›´æ–°æ¡ä»¶æ»¡è¶³ï¼š{overlap}/{len(file_list)} ({overlap_ratio*100:.1f}%) æ–‡ä»¶å·²ç¼“å­˜")
            
            return should_use
        except:
            return False
    
    def _incremental_update_analysis(self, file_list, months):
        """å¢é‡æ›´æ–°åˆ†æ - åªåˆ†æå˜æ›´çš„æ–‡ä»¶"""
        print("ğŸ”„ ä½¿ç”¨å¢é‡æ›´æ–°æ¨¡å¼...")
        start_time = time.time()
        
        try:
            # 1. åŠ è½½ç°æœ‰ç¼“å­˜
            cached_data = self._load_cache()
            cached_files = set(cached_data['file_contributors'].keys())
            
            # 2. è¯†åˆ«éœ€è¦æ›´æ–°çš„æ–‡ä»¶
            new_files = [f for f in file_list if f not in cached_files]
            changed_files = self._get_changed_files_since_cache(cached_data['timestamp'])
            
            files_to_update = list(set(new_files + changed_files))
            
            print(f"   ğŸ“‹ éœ€è¦æ›´æ–°: {len(files_to_update)} ä¸ªæ–‡ä»¶ (æ–°å¢: {len(new_files)}, å˜æ›´: {len(changed_files)})")
            
            if files_to_update:
                # 3. åªåˆ†æéœ€è¦æ›´æ–°çš„æ–‡ä»¶
                since_date = (datetime.now() - timedelta(days=months * 30)).strftime("%Y-%m-%d")
                incremental_data = self._incremental_analysis(files_to_update, since_date)
                
                # 4. åˆå¹¶åˆ°ç°æœ‰ç¼“å­˜
                cached_data['file_contributors'].update(incremental_data['file_contributors'])
                cached_data['author_activity'].update(incremental_data['author_activity'])
                cached_data['timestamp'] = time.time()
                
                # 5. ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
                self._save_cache(cached_data)
            
            # 6. è¿”å›ç»“æœ
            results = self._extract_file_results(cached_data, file_list)
            
            elapsed = time.time() - start_time
            print(f"âš¡ å¢é‡åˆ†æå®Œæˆ: {elapsed:.3f}s (æ›´æ–°äº† {len(files_to_update)} ä¸ªæ–‡ä»¶)")
            
            return results
            
        except Exception as e:
            print(f"âš ï¸ å¢é‡æ›´æ–°å¤±è´¥ï¼Œå›é€€åˆ°å…¨é‡åˆ†æ: {e}")
            return self.analyze_contributors_ultra_fast(file_list, months, force_incremental=True)
    
    def _get_changed_files_since_cache(self, cache_timestamp):
        """è·å–è‡ªç¼“å­˜æ—¶é—´ä»¥æ¥å˜æ›´çš„æ–‡ä»¶"""
        try:
            # è½¬æ¢æ—¶é—´æˆ³ä¸ºæ—¥æœŸ
            cache_date = datetime.fromtimestamp(cache_timestamp).strftime("%Y-%m-%d %H:%M:%S")
            
            # è·å–è‡ªæŒ‡å®šæ—¶é—´ä»¥æ¥çš„å˜æ›´æ–‡ä»¶
            cmd = f'git log --since="{cache_date}" --name-only --format="" | sort -u'
            result = subprocess.run(
                cmd, shell=True, cwd=self.repo_path,
                capture_output=True, text=True, check=False
            )
            
            if result.stdout:
                changed_files = [f.strip() for f in result.stdout.split('\n') if f.strip()]
                return changed_files
            else:
                return []
                
        except Exception as e:
            print(f"âš ï¸ è·å–å˜æ›´æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def _incremental_analysis(self, file_list, since_date):
        """å¢é‡åˆ†æ - åªåˆ†ææŒ‡å®šæ–‡ä»¶"""
        if not file_list:
            return {'file_contributors': {}, 'author_activity': {}}
        
        print(f"   ğŸ” å¢é‡åˆ†æ {len(file_list)} ä¸ªæ–‡ä»¶...")
        
        # æ„å»ºåªé’ˆå¯¹è¿™äº›æ–‡ä»¶çš„Gitå‘½ä»¤
        files_arg = " ".join([f'"{path}"' for path in file_list])
        cmd = f'git log --since="{since_date}" --format="COMMIT:%H|%an|%ct" --name-only -- {files_arg}'
        
        result = subprocess.run(
            cmd, shell=True, cwd=self.repo_path,
            capture_output=True, text=True, check=True
        )
        
        # è§£æç»“æœï¼ˆä½¿ç”¨ä¸å…¨å±€åˆ†æç›¸åŒçš„é€»è¾‘ï¼‰
        file_contributors = defaultdict(lambda: defaultdict(int))
        author_activity = defaultdict(int)
        
        lines = result.stdout.strip().split('\n')
        current_commit = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if line.startswith('COMMIT:'):
                parts = line[7:].split('|', 2)
                if len(parts) >= 2:
                    commit_hash, author = parts[0], parts[1]
                    current_commit = {'author': author}
                    author_activity[author] += 1
            elif current_commit and line and not line.startswith('COMMIT:'):
                if line in file_list:  # åªå¤„ç†æˆ‘ä»¬å…³å¿ƒçš„æ–‡ä»¶
                    author = current_commit['author']
                    file_contributors[line][author] += 1
        
        return {
            'file_contributors': dict(file_contributors),
            'author_activity': dict(author_activity)
        }
    
    def _global_analysis(self, since_date):
        """ä¸€æ¬¡æ€§å…¨å±€åˆ†æ - æ ¸å¿ƒä¼˜åŒ–"""
        print("ğŸ“Š æ‰§è¡Œå…¨å±€åˆ†æ...")
        start = time.time()
        
        # å•ä¸ªGitå‘½ä»¤è·å–æ‰€æœ‰éœ€è¦çš„ä¿¡æ¯
        cmd = f'git log --since="{since_date}" --format="COMMIT:%H|%an|%ct" --name-only'
        
        result = subprocess.run(
            cmd, shell=True, cwd=self.repo_path, 
            capture_output=True, text=True, check=True
        )
        
        print(f"   GitæŸ¥è¯¢è€—æ—¶: {time.time() - start:.3f}s")
        
        # è§£æç»“æœ
        parse_start = time.time()
        commits = {}
        file_contributors = defaultdict(lambda: defaultdict(int))
        author_activity = defaultdict(int)
        
        lines = result.stdout.strip().split('\n')
        current_commit = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if line.startswith('COMMIT:'):
                # è§£ææäº¤ä¿¡æ¯: COMMIT:hash|author|timestamp
                parts = line[7:].split('|', 2)
                if len(parts) >= 2:
                    commit_hash, author = parts[0], parts[1]
                    timestamp = int(parts[2]) if len(parts) > 2 else int(time.time())
                    current_commit = {
                        'hash': commit_hash,
                        'author': author, 
                        'timestamp': timestamp
                    }
                    author_activity[author] += 1
            elif current_commit and line and not line.startswith('COMMIT:'):
                # è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶è·¯å¾„
                author = current_commit['author']
                file_contributors[line][author] += 1
        
        print(f"   æ•°æ®è§£æè€—æ—¶: {time.time() - parse_start:.3f}s")
        print(f"   å‘ç° {len(file_contributors)} ä¸ªæ–‡ä»¶, {len(author_activity)} ä¸ªä½œè€…")
        
        return {
            'file_contributors': dict(file_contributors),
            'author_activity': dict(author_activity),
            'timestamp': time.time()
        }
    
    def _intelligent_assignment(self, global_data, file_list):
        """æ™ºèƒ½åˆ†é…ç­–ç•¥"""
        print("ğŸ§  æ‰§è¡Œæ™ºèƒ½åˆ†é…...")
        
        file_contributors = global_data['file_contributors']
        author_activity = global_data['author_activity']
        
        results = {}
        direct_hits = 0
        fallback_assignments = 0
        
        for file_path in file_list:
            if file_path in file_contributors:
                # ç›´æ¥å‘½ä¸­
                contributors = file_contributors[file_path]
                results[file_path] = contributors
                direct_hits += 1
            else:
                # æ™ºèƒ½æ¨æ–­
                assigned = self._smart_fallback(file_path, file_contributors, author_activity)
                results[file_path] = assigned
                fallback_assignments += 1
        
        print(f"   ç›´æ¥å‘½ä¸­: {direct_hits}, æ™ºèƒ½æ¨æ–­: {fallback_assignments}")
        return results
    
    def _smart_fallback(self, file_path, file_contributors, author_activity):
        """å¢å¼ºçš„æ™ºèƒ½å›é€€åˆ†é…ç­–ç•¥"""
        # 1. ç²¾ç¡®ç›®å½•åŒ¹é…ï¼ˆåŒ…æ‹¬çˆ¶ç›®å½•ï¼‰
        dir_path = '/'.join(file_path.split('/')[:-1])
        if dir_path:
            # å°è¯•å®Œå…¨åŒ¹é…çš„ç›®å½•
            exact_dir_matches = defaultdict(int)
            for fp, contributors in file_contributors.items():
                fp_dir = '/'.join(fp.split('/')[:-1])
                if fp_dir == dir_path:
                    for author, count in contributors.items():
                        exact_dir_matches[author] += count * 3  # å®Œå…¨åŒ¹é…æƒé‡æ›´é«˜
            
            # å°è¯•çˆ¶ç›®å½•åŒ¹é…
            parent_dir_matches = defaultdict(int)
            for fp, contributors in file_contributors.items():
                if fp.startswith(dir_path + '/'):
                    for author, count in contributors.items():
                        parent_dir_matches[author] += count
                        
            # åˆå¹¶ç›®å½•åŒ¹é…ç»“æœ
            combined_dir_matches = defaultdict(int)
            for author, count in exact_dir_matches.items():
                combined_dir_matches[author] += count
            for author, count in parent_dir_matches.items():
                combined_dir_matches[author] += count
                
            if combined_dir_matches:
                return dict(combined_dir_matches)
        
        # 2. å¢å¼ºçš„æ‰©å±•ååŒ¹é…
        file_ext = file_path.split('.')[-1] if '.' in file_path else ''
        if file_ext:
            ext_matches = defaultdict(int)
            similar_ext_matches = defaultdict(int)
            
            # ç›¸å…³æ‰©å±•åç»„
            ext_groups = {
                'py': ['py', 'pyw', 'pyi'],
                'js': ['js', 'jsx', 'ts', 'tsx'], 
                'cpp': ['cpp', 'c', 'cc', 'cxx'],
                'h': ['h', 'hpp', 'hxx'],
                'md': ['md', 'txt', 'rst'],
                'json': ['json', 'yaml', 'yml'],
            }
            
            # æŸ¥æ‰¾å½“å‰æ‰©å±•åç»„
            current_group = [file_ext]
            for group_key, extensions in ext_groups.items():
                if file_ext in extensions:
                    current_group = extensions
                    break
            
            for fp, contributors in file_contributors.items():
                fp_ext = fp.split('.')[-1] if '.' in fp else ''
                if fp_ext == file_ext:
                    # å®Œå…¨åŒ¹é…æ‰©å±•å
                    for author, count in contributors.items():
                        ext_matches[author] += count * 2
                elif fp_ext in current_group:
                    # ç›¸å…³æ‰©å±•ååŒ¹é…
                    for author, count in contributors.items():
                        similar_ext_matches[author] += count
                        
            # åˆå¹¶æ‰©å±•åç»“æœ
            combined_ext_matches = defaultdict(int)
            for author, count in ext_matches.items():
                combined_ext_matches[author] += count
            for author, count in similar_ext_matches.items():
                combined_ext_matches[author] += count
                
            if combined_ext_matches:
                return dict(combined_ext_matches)
        
        # 3. æ–‡ä»¶åæ¨¡å¼åŒ¹é…
        file_name = file_path.split('/')[-1].lower()
        name_matches = defaultdict(int)
        
        for fp, contributors in file_contributors.items():
            fp_name = fp.split('/')[-1].lower()
            
            # æ£€æŸ¥ç›¸ä¼¼çš„æ–‡ä»¶åæ¨¡å¼
            similarity_score = 0
            if file_name.startswith(fp_name[:3]) or fp_name.startswith(file_name[:3]):
                similarity_score += 1
            
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            keywords = ['test', 'main', 'config', 'utils', 'helper', 'core']
            for keyword in keywords:
                if keyword in file_name and keyword in fp_name:
                    similarity_score += 2
                    
            if similarity_score > 0:
                for author, count in contributors.items():
                    name_matches[author] += count * similarity_score
                    
        if name_matches:
            return dict(name_matches)
        
        # 4. ä½¿ç”¨åŠ æƒçš„å…¨å±€æ´»è·ƒä½œè€…
        if author_activity:
            # ä¸åªæ˜¯æœ€æ´»è·ƒçš„ä½œè€…ï¼Œè€Œæ˜¯æŒ‰æ´»è·ƒåº¦åˆ†é…æƒé‡
            total_activity = sum(author_activity.values())
            weighted_authors = {}
            
            for author, activity in author_activity.items():
                # æŒ‰æ´»è·ƒåº¦æ¯”ä¾‹åˆ†é…æƒé‡
                weight = max(1, int(activity * 5 / total_activity))
                weighted_authors[author] = weight
                
            return weighted_authors
        
        return {}
    
    def _extract_file_results(self, cached_data, file_list):
        """ä»ç¼“å­˜æ•°æ®æå–æ–‡ä»¶ç»“æœ"""
        file_contributors = cached_data['file_contributors']
        author_activity = cached_data['author_activity']
        
        results = {}
        for file_path in file_list:
            if file_path in file_contributors:
                results[file_path] = file_contributors[file_path]
            else:
                results[file_path] = self._smart_fallback(
                    file_path, file_contributors, author_activity
                )
        return results
    
    def _is_cache_valid(self):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not self.cache_file.exists():
            return False
            
        try:
            with open(self.cache_file, 'r') as f:
                data = json.load(f)
            
            cache_time = data.get('timestamp', 0)
            age_hours = (time.time() - cache_time) / 3600
            
            return age_hours < self.cache_expiry_hours
        except:
            return False
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        with open(self.cache_file, 'r') as f:
            return json.load(f)
    
    def _save_cache(self, data):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        self.cache_file.parent.mkdir(exist_ok=True)
        with open(self.cache_file, 'w') as f:
            json.dump(data, f, indent=2)


# æ€§èƒ½æµ‹è¯•å‡½æ•°
def performance_comparison_test():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    from core.optimized_contributor_analyzer import OptimizedContributorAnalyzer
    from core.git_operations import GitOperations
    
    # è·å–æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    import glob
    test_files = glob.glob("**/*.py", recursive=True)[:50]  # æµ‹è¯•50ä¸ªæ–‡ä»¶
    
    print(f"ğŸ§ª æ€§èƒ½å¯¹æ¯”æµ‹è¯• - {len(test_files)} ä¸ªæ–‡ä»¶")
    
    # 1. ä¼ ç»Ÿæ–¹æ³•
    git_ops = GitOperations()
    traditional = OptimizedContributorAnalyzer(git_ops)
    
    print("\nğŸ“Š ä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•:")
    start = time.time()
    traditional_results = traditional.analyze_contributors_batch(test_files)
    traditional_time = time.time() - start
    print(f"ä¼ ç»Ÿæ–¹æ³•è€—æ—¶: {traditional_time:.3f}s")
    
    # 2. è¶…é«˜é€Ÿæ–¹æ³•
    print("\nğŸš€ è¶…é«˜é€Ÿæ–¹æ³•:")
    ultra = UltraFastAnalyzer()
    start = time.time()
    ultra_results = ultra.analyze_contributors_ultra_fast(test_files)
    ultra_time = time.time() - start
    print(f"è¶…é«˜é€Ÿæ–¹æ³•è€—æ—¶: {ultra_time:.3f}s")
    
    # 3. æ€§èƒ½å¯¹æ¯”
    if traditional_time > 0:
        speedup = traditional_time / ultra_time
        print(f"\nâš¡ æ€§èƒ½æå‡: {speedup:.1f}å€")
        print(f"ğŸ’¡ æ•ˆç‡å¯¹æ¯”: {traditional_time/len(test_files)*1000:.1f}ms vs {ultra_time/len(test_files)*1000:.1f}ms æ¯æ–‡ä»¶")


if __name__ == "__main__":
    performance_comparison_test()