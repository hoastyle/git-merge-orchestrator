"""
Ultra Fast Contributor Analyzer
é©å‘½æ€§æ€§èƒ½ä¼˜åŒ–ï¼šå…¨å±€åˆ†æ + æ™ºèƒ½æ¨æ–­
é¢„æœŸæ€§èƒ½æå‡ï¼š10-100å€
"""

import subprocess
import json
import time
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path


class UltraFastAnalyzer:
    """è¶…é«˜é€Ÿè´¡çŒ®è€…åˆ†æå™¨"""
    
    def __init__(self, repo_path="."):
        self.repo_path = Path(repo_path)
        self.cache_file = self.repo_path / ".merge_work" / "ultra_cache.json"
        self.cache_expiry_hours = 6  # 6å°æ—¶ç¼“å­˜è¿‡æœŸ
        
    def analyze_contributors_ultra_fast(self, file_list, months=6, force_incremental=False):
        """è¶…é«˜é€Ÿåˆ†æ - å…¨å±€åˆ†æ + æ™ºèƒ½æ¨æ–­ + å¢é‡æ›´æ–°"""
        main_start = time.time()
        print(f"ğŸš€ [PERF] å¼€å§‹è¶…é«˜é€Ÿåˆ†æ {len(file_list)} ä¸ªæ–‡ä»¶... (å¼€å§‹æ—¶é—´: {main_start:.3f})")
        
        # 1. æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨å¢é‡æ›´æ–°
        step1_start = time.time()
        should_use_incremental = not force_incremental and self._should_use_incremental_update(file_list)
        step1_time = time.time() - step1_start
        print(f"â±ï¸  [PERF] æ­¥éª¤1-å¢é‡æ›´æ–°æ£€æŸ¥: {step1_time:.3f}s")
        
        if should_use_incremental:
            print("ğŸ”„ [PERF] ä½¿ç”¨å¢é‡æ›´æ–°æ¨¡å¼")
            return self._incremental_update_analysis(file_list, months)
        
        # 2. æ£€æŸ¥ç¼“å­˜
        step2_start = time.time()
        cache_valid = self._is_cache_valid()
        step2_time = time.time() - step2_start
        print(f"â±ï¸  [PERF] æ­¥éª¤2-ç¼“å­˜æ£€æŸ¥: {step2_time:.3f}s (æœ‰æ•ˆ: {cache_valid})")
        
        if cache_valid:
            cache_load_start = time.time()
            print("âš¡ [PERF] ä½¿ç”¨ç¼“å­˜æ•°æ®")
            cached_data = self._load_cache()
            cache_load_time = time.time() - cache_load_start
            print(f"â±ï¸  [PERF] ç¼“å­˜åŠ è½½: {cache_load_time:.3f}s")
            
            extract_start = time.time()
            results = self._extract_file_results(cached_data, file_list)
            extract_time = time.time() - extract_start
            print(f"â±ï¸  [PERF] ç»“æœæå–: {extract_time:.3f}s")
            
            total_time = time.time() - main_start
            print(f"âœ… [PERF] ç¼“å­˜æ¨¡å¼æ€»è€—æ—¶: {total_time:.3f}s")
            return results
        
        # 3. å…¨å±€åˆ†æ - ä¸€æ¬¡Gitè°ƒç”¨è·å–æ‰€æœ‰ä¿¡æ¯
        step3_start = time.time()
        since_date = (datetime.now() - timedelta(days=months * 30)).strftime("%Y-%m-%d")
        print(f"ğŸ“Š [PERF] å¼€å§‹å…¨å±€åˆ†æ (åˆ†ææ—¶é—´èŒƒå›´: {since_date} è‡³ä»Š)")
        global_data = self._global_analysis(since_date)
        step3_time = time.time() - step3_start
        print(f"â±ï¸  [PERF] æ­¥éª¤3-å…¨å±€åˆ†æ: {step3_time:.3f}s")
        
        # 4. æ™ºèƒ½åˆ†é…
        step4_start = time.time()
        results = self._intelligent_assignment(global_data, file_list)
        step4_time = time.time() - step4_start
        print(f"â±ï¸  [PERF] æ­¥éª¤4-æ™ºèƒ½åˆ†é…: {step4_time:.3f}s")
        
        # 5. ä¿å­˜ç¼“å­˜
        step5_start = time.time()
        self._save_cache(global_data)
        step5_time = time.time() - step5_start
        print(f"â±ï¸  [PERF] æ­¥éª¤5-ç¼“å­˜ä¿å­˜: {step5_time:.3f}s")
        
        total_time = time.time() - main_start
        print(f"âœ… [PERF] è¶…é«˜é€Ÿåˆ†ææ€»è€—æ—¶: {total_time:.3f}s (å¹³å‡ {total_time/len(file_list)*1000:.1f}ms/æ–‡ä»¶)")
        
        # ä¿å­˜æ€§èƒ½æ—¥å¿—
        self._save_performance_log(file_list, total_time, {
            'incremental_check': step1_time,
            'cache_check': step2_time, 
            'global_analysis': step3_time,
            'intelligent_assignment': step4_time,
            'cache_save': step5_time
        })
        
        return results
    
    def _should_use_incremental_update(self, file_list):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¢é‡æ›´æ–°"""
        if not self.cache_file.exists():
            return False
            
        try:
            cached_data = self._load_cache()
            cached_files = set(cached_data['file_contributors'].keys())
            
            # å¦‚æœè¶…è¿‡80%çš„æ–‡ä»¶å·²åœ¨ç¼“å­˜ä¸­ï¼Œä¸”æ–‡ä»¶æ€»æ•°ä¸å°‘ï¼Œä½¿ç”¨å¢é‡æ›´æ–°
            overlap = len(set(file_list) & cached_files)
            overlap_ratio = overlap / len(file_list) if file_list else 0
            
            should_use = overlap_ratio > 0.8 and len(file_list) > 20
            if should_use:
                print(f"ğŸ“Š å¢é‡æ›´æ–°æ¡ä»¶æ»¡è¶³ï¼š{overlap}/{len(file_list)} ({overlap_ratio*100:.1f}%) æ–‡ä»¶å·²ç¼“å­˜")
            
            return should_use
        except:
            return False
    
    def _incremental_update_analysis(self, file_list, months):
        """å¢é‡æ›´æ–°åˆ†æ - åªåˆ†æå˜æ›´çš„æ–‡ä»¶"""
        print("ğŸ”„ ä½¿ç”¨å¢é‡æ›´æ–°æ¨¡å¼...")
        start_time = time.time()
        
        try:
            # 1. åŠ è½½ç°æœ‰ç¼“å­˜
            cached_data = self._load_cache()
            cached_files = set(cached_data['file_contributors'].keys())
            
            # 2. è¯†åˆ«éœ€è¦æ›´æ–°çš„æ–‡ä»¶
            new_files = [f for f in file_list if f not in cached_files]
            changed_files = self._get_changed_files_since_cache(cached_data['timestamp'])
            
            files_to_update = list(set(new_files + changed_files))
            
            print(f"   ğŸ“‹ éœ€è¦æ›´æ–°: {len(files_to_update)} ä¸ªæ–‡ä»¶ (æ–°å¢: {len(new_files)}, å˜æ›´: {len(changed_files)})")
            
            if files_to_update:
                # 3. åªåˆ†æéœ€è¦æ›´æ–°çš„æ–‡ä»¶
                since_date = (datetime.now() - timedelta(days=months * 30)).strftime("%Y-%m-%d")
                incremental_data = self._incremental_analysis(files_to_update, since_date)
                
                # 4. åˆå¹¶åˆ°ç°æœ‰ç¼“å­˜
                cached_data['file_contributors'].update(incremental_data['file_contributors'])
                cached_data['author_activity'].update(incremental_data['author_activity'])
                cached_data['timestamp'] = time.time()
                
                # 5. ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
                self._save_cache(cached_data)
            
            # 6. è¿”å›ç»“æœ
            results = self._extract_file_results(cached_data, file_list)
            
            elapsed = time.time() - start_time
            print(f"âš¡ å¢é‡åˆ†æå®Œæˆ: {elapsed:.3f}s (æ›´æ–°äº† {len(files_to_update)} ä¸ªæ–‡ä»¶)")
            
            return results
            
        except Exception as e:
            print(f"âš ï¸ å¢é‡æ›´æ–°å¤±è´¥ï¼Œå›é€€åˆ°å…¨é‡åˆ†æ: {e}")
            return self.analyze_contributors_ultra_fast(file_list, months, force_incremental=True)
    
    def _get_changed_files_since_cache(self, cache_timestamp):
        """è·å–è‡ªç¼“å­˜æ—¶é—´ä»¥æ¥å˜æ›´çš„æ–‡ä»¶"""
        try:
            # è½¬æ¢æ—¶é—´æˆ³ä¸ºæ—¥æœŸ
            cache_date = datetime.fromtimestamp(cache_timestamp).strftime("%Y-%m-%d %H:%M:%S")
            
            # è·å–è‡ªæŒ‡å®šæ—¶é—´ä»¥æ¥çš„å˜æ›´æ–‡ä»¶
            cmd = f'git log --since="{cache_date}" --name-only --format="" | sort -u'
            result = subprocess.run(
                cmd, shell=True, cwd=self.repo_path,
                capture_output=True, text=True, check=False
            )
            
            if result.stdout:
                changed_files = [f.strip() for f in result.stdout.split('\n') if f.strip()]
                return changed_files
            else:
                return []
                
        except Exception as e:
            print(f"âš ï¸ è·å–å˜æ›´æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def _incremental_analysis(self, file_list, since_date):
        """å¢é‡åˆ†æ - åªåˆ†ææŒ‡å®šæ–‡ä»¶"""
        if not file_list:
            return {'file_contributors': {}, 'author_activity': {}}
        
        print(f"   ğŸ” å¢é‡åˆ†æ {len(file_list)} ä¸ªæ–‡ä»¶...")
        
        # æ„å»ºåªé’ˆå¯¹è¿™äº›æ–‡ä»¶çš„Gitå‘½ä»¤
        files_arg = " ".join([f'"{path}"' for path in file_list])
        cmd = f'git log --since="{since_date}" --format="COMMIT:%H|%an|%ct" --name-only -- {files_arg}'
        
        result = subprocess.run(
            cmd, shell=True, cwd=self.repo_path,
            capture_output=True, text=True, check=True
        )
        
        # è§£æç»“æœï¼ˆä½¿ç”¨ä¸å…¨å±€åˆ†æç›¸åŒçš„é€»è¾‘ï¼‰
        file_contributors = defaultdict(lambda: defaultdict(int))
        author_activity = defaultdict(int)
        
        lines = result.stdout.strip().split('\n')
        current_commit = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if line.startswith('COMMIT:'):
                parts = line[7:].split('|', 2)
                if len(parts) >= 2:
                    commit_hash, author = parts[0], parts[1]
                    current_commit = {'author': author}
                    author_activity[author] += 1
            elif current_commit and line and not line.startswith('COMMIT:'):
                if line in file_list:  # åªå¤„ç†æˆ‘ä»¬å…³å¿ƒçš„æ–‡ä»¶
                    author = current_commit['author']
                    file_contributors[line][author] += 1
        
        return {
            'file_contributors': dict(file_contributors),
            'author_activity': dict(author_activity)
        }
    
    def _global_analysis(self, since_date):
        """ä¸€æ¬¡æ€§å…¨å±€åˆ†æ - æ ¸å¿ƒä¼˜åŒ–"""
        analysis_start = time.time()
        print(f"ğŸ“Š [PERF] æ‰§è¡Œå…¨å±€åˆ†æ... (å¼€å§‹æ—¶é—´: {analysis_start:.3f})")
        
        # æ„å»ºGitå‘½ä»¤
        cmd_build_start = time.time()
        cmd = f'git log --since="{since_date}" --format="COMMIT:%H|%an|%ct" --name-only'
        cmd_build_time = time.time() - cmd_build_start
        print(f"â±ï¸  [PERF] Gitå‘½ä»¤æ„å»º: {cmd_build_time:.3f}s")
        print(f"ğŸ“ [PERF] Gitå‘½ä»¤: {cmd}")
        
        # æ‰§è¡ŒGitå‘½ä»¤
        git_start = time.time()
        result = subprocess.run(
            cmd, shell=True, cwd=self.repo_path, 
            capture_output=True, text=True, check=True
        )
        git_time = time.time() - git_start
        print(f"â±ï¸  [PERF] GitæŸ¥è¯¢æ‰§è¡Œ: {git_time:.3f}s")
        
        # ç»Ÿè®¡è¾“å‡ºå¤§å°
        output_size = len(result.stdout)
        output_lines = len(result.stdout.split('\n'))
        print(f"ğŸ“Š [PERF] Gitè¾“å‡º: {output_size} å­—ç¬¦, {output_lines} è¡Œ")
        
        # è§£æç»“æœ
        parse_start = time.time()
        commits = {}
        file_contributors = defaultdict(lambda: defaultdict(int))
        author_activity = defaultdict(int)
        
        lines = result.stdout.strip().split('\n')
        current_commit = None
        processed_lines = 0
        commit_count = 0
        file_lines = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            processed_lines += 1
            
            if line.startswith('COMMIT:'):
                commit_count += 1
                # è§£ææäº¤ä¿¡æ¯: COMMIT:hash|author|timestamp
                parts = line[7:].split('|', 2)
                if len(parts) >= 2:
                    commit_hash, author = parts[0], parts[1]
                    timestamp = int(parts[2]) if len(parts) > 2 else int(time.time())
                    current_commit = {
                        'hash': commit_hash,
                        'author': author, 
                        'timestamp': timestamp
                    }
                    author_activity[author] += 1
            elif current_commit and line and not line.startswith('COMMIT:'):
                file_lines += 1
                # è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶è·¯å¾„
                author = current_commit['author']
                file_contributors[line][author] += 1
        
        parse_time = time.time() - parse_start
        total_analysis_time = time.time() - analysis_start
        
        print(f"â±ï¸  [PERF] æ•°æ®è§£æ: {parse_time:.3f}s")
        print(f"ğŸ“Š [PERF] è§£æç»Ÿè®¡: {processed_lines} è¡Œå¤„ç†, {commit_count} ä¸ªæäº¤, {file_lines} ä¸ªæ–‡ä»¶è¡Œ")
        print(f"ğŸ“Š [PERF] å‘ç° {len(file_contributors)} ä¸ªæ–‡ä»¶, {len(author_activity)} ä¸ªä½œè€…")
        print(f"âœ… [PERF] å…¨å±€åˆ†ææ€»è€—æ—¶: {total_analysis_time:.3f}s")
        
        return {
            'file_contributors': dict(file_contributors),
            'author_activity': dict(author_activity),
            'timestamp': time.time(),
            '_perf_stats': {
                'cmd_build_time': cmd_build_time,
                'git_exec_time': git_time,
                'parse_time': parse_time,
                'total_time': total_analysis_time,
                'output_size': output_size,
                'output_lines': output_lines,
                'processed_lines': processed_lines,
                'commit_count': commit_count,
                'file_lines': file_lines
            }
        }
    
    def _intelligent_assignment(self, global_data, file_list):
        """æ™ºèƒ½åˆ†é…ç­–ç•¥"""
        print("ğŸ§  æ‰§è¡Œæ™ºèƒ½åˆ†é…...")
        
        file_contributors = global_data['file_contributors']
        author_activity = global_data['author_activity']
        
        results = {}
        direct_hits = 0
        fallback_assignments = 0
        
        for file_path in file_list:
            if file_path in file_contributors:
                # ç›´æ¥å‘½ä¸­
                contributors = file_contributors[file_path]
                results[file_path] = contributors
                direct_hits += 1
            else:
                # æ™ºèƒ½æ¨æ–­
                assigned = self._smart_fallback(file_path, file_contributors, author_activity)
                results[file_path] = assigned
                fallback_assignments += 1
        
        print(f"   ç›´æ¥å‘½ä¸­: {direct_hits}, æ™ºèƒ½æ¨æ–­: {fallback_assignments}")
        return results
    
    def _smart_fallback(self, file_path, file_contributors, author_activity):
        """å¢å¼ºçš„æ™ºèƒ½å›é€€åˆ†é…ç­–ç•¥"""
        # 1. ç²¾ç¡®ç›®å½•åŒ¹é…ï¼ˆåŒ…æ‹¬çˆ¶ç›®å½•ï¼‰
        dir_path = '/'.join(file_path.split('/')[:-1])
        if dir_path:
            # å°è¯•å®Œå…¨åŒ¹é…çš„ç›®å½•
            exact_dir_matches = defaultdict(int)
            for fp, contributors in file_contributors.items():
                fp_dir = '/'.join(fp.split('/')[:-1])
                if fp_dir == dir_path:
                    for author, count in contributors.items():
                        exact_dir_matches[author] += count * 3  # å®Œå…¨åŒ¹é…æƒé‡æ›´é«˜
            
            # å°è¯•çˆ¶ç›®å½•åŒ¹é…
            parent_dir_matches = defaultdict(int)
            for fp, contributors in file_contributors.items():
                if fp.startswith(dir_path + '/'):
                    for author, count in contributors.items():
                        parent_dir_matches[author] += count
                        
            # åˆå¹¶ç›®å½•åŒ¹é…ç»“æœ
            combined_dir_matches = defaultdict(int)
            for author, count in exact_dir_matches.items():
                combined_dir_matches[author] += count
            for author, count in parent_dir_matches.items():
                combined_dir_matches[author] += count
                
            if combined_dir_matches:
                return dict(combined_dir_matches)
        
        # 2. å¢å¼ºçš„æ‰©å±•ååŒ¹é…
        file_ext = file_path.split('.')[-1] if '.' in file_path else ''
        if file_ext:
            ext_matches = defaultdict(int)
            similar_ext_matches = defaultdict(int)
            
            # ç›¸å…³æ‰©å±•åç»„
            ext_groups = {
                'py': ['py', 'pyw', 'pyi'],
                'js': ['js', 'jsx', 'ts', 'tsx'], 
                'cpp': ['cpp', 'c', 'cc', 'cxx'],
                'h': ['h', 'hpp', 'hxx'],
                'md': ['md', 'txt', 'rst'],
                'json': ['json', 'yaml', 'yml'],
            }
            
            # æŸ¥æ‰¾å½“å‰æ‰©å±•åç»„
            current_group = [file_ext]
            for group_key, extensions in ext_groups.items():
                if file_ext in extensions:
                    current_group = extensions
                    break
            
            for fp, contributors in file_contributors.items():
                fp_ext = fp.split('.')[-1] if '.' in fp else ''
                if fp_ext == file_ext:
                    # å®Œå…¨åŒ¹é…æ‰©å±•å
                    for author, count in contributors.items():
                        ext_matches[author] += count * 2
                elif fp_ext in current_group:
                    # ç›¸å…³æ‰©å±•ååŒ¹é…
                    for author, count in contributors.items():
                        similar_ext_matches[author] += count
                        
            # åˆå¹¶æ‰©å±•åç»“æœ
            combined_ext_matches = defaultdict(int)
            for author, count in ext_matches.items():
                combined_ext_matches[author] += count
            for author, count in similar_ext_matches.items():
                combined_ext_matches[author] += count
                
            if combined_ext_matches:
                return dict(combined_ext_matches)
        
        # 3. æ–‡ä»¶åæ¨¡å¼åŒ¹é…
        file_name = file_path.split('/')[-1].lower()
        name_matches = defaultdict(int)
        
        for fp, contributors in file_contributors.items():
            fp_name = fp.split('/')[-1].lower()
            
            # æ£€æŸ¥ç›¸ä¼¼çš„æ–‡ä»¶åæ¨¡å¼
            similarity_score = 0
            if file_name.startswith(fp_name[:3]) or fp_name.startswith(file_name[:3]):
                similarity_score += 1
            
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            keywords = ['test', 'main', 'config', 'utils', 'helper', 'core']
            for keyword in keywords:
                if keyword in file_name and keyword in fp_name:
                    similarity_score += 2
                    
            if similarity_score > 0:
                for author, count in contributors.items():
                    name_matches[author] += count * similarity_score
                    
        if name_matches:
            return dict(name_matches)
        
        # 4. ä½¿ç”¨åŠ æƒçš„å…¨å±€æ´»è·ƒä½œè€…
        if author_activity:
            # ä¸åªæ˜¯æœ€æ´»è·ƒçš„ä½œè€…ï¼Œè€Œæ˜¯æŒ‰æ´»è·ƒåº¦åˆ†é…æƒé‡
            total_activity = sum(author_activity.values())
            weighted_authors = {}
            
            for author, activity in author_activity.items():
                # æŒ‰æ´»è·ƒåº¦æ¯”ä¾‹åˆ†é…æƒé‡
                weight = max(1, int(activity * 5 / total_activity))
                weighted_authors[author] = weight
                
            return weighted_authors
        
        return {}
    
    def _extract_file_results(self, cached_data, file_list):
        """ä»ç¼“å­˜æ•°æ®æå–æ–‡ä»¶ç»“æœ"""
        file_contributors = cached_data['file_contributors']
        author_activity = cached_data['author_activity']
        
        results = {}
        for file_path in file_list:
            if file_path in file_contributors:
                results[file_path] = file_contributors[file_path]
            else:
                results[file_path] = self._smart_fallback(
                    file_path, file_contributors, author_activity
                )
        return results
    
    def _is_cache_valid(self):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not self.cache_file.exists():
            return False
            
        try:
            with open(self.cache_file, 'r') as f:
                data = json.load(f)
            
            cache_time = data.get('timestamp', 0)
            age_hours = (time.time() - cache_time) / 3600
            
            return age_hours < self.cache_expiry_hours
        except:
            return False
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        with open(self.cache_file, 'r') as f:
            return json.load(f)
    
    def _save_cache(self, data):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        self.cache_file.parent.mkdir(exist_ok=True)
        with open(self.cache_file, 'w') as f:
            json.dump(data, f, indent=2)


# æ€§èƒ½æµ‹è¯•å‡½æ•°
def performance_comparison_test():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    from core.optimized_contributor_analyzer import OptimizedContributorAnalyzer
    from core.git_operations import GitOperations
    
    # è·å–æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    import glob
    test_files = glob.glob("**/*.py", recursive=True)[:50]  # æµ‹è¯•50ä¸ªæ–‡ä»¶
    
    print(f"ğŸ§ª æ€§èƒ½å¯¹æ¯”æµ‹è¯• - {len(test_files)} ä¸ªæ–‡ä»¶")
    
    # 1. ä¼ ç»Ÿæ–¹æ³•
    git_ops = GitOperations()
    traditional = OptimizedContributorAnalyzer(git_ops)
    
    print("\nğŸ“Š ä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•:")
    start = time.time()
    traditional_results = traditional.analyze_contributors_batch(test_files)
    traditional_time = time.time() - start
    print(f"ä¼ ç»Ÿæ–¹æ³•è€—æ—¶: {traditional_time:.3f}s")
    
    # 2. è¶…é«˜é€Ÿæ–¹æ³•
    print("\nğŸš€ è¶…é«˜é€Ÿæ–¹æ³•:")
    ultra = UltraFastAnalyzer()
    start = time.time()
    ultra_results = ultra.analyze_contributors_ultra_fast(test_files)
    ultra_time = time.time() - start
    print(f"è¶…é«˜é€Ÿæ–¹æ³•è€—æ—¶: {ultra_time:.3f}s")
    
    # 3. æ€§èƒ½å¯¹æ¯”
    if traditional_time > 0:
        speedup = traditional_time / ultra_time
        print(f"\nâš¡ æ€§èƒ½æå‡: {speedup:.1f}å€")
        print(f"ğŸ’¡ æ•ˆç‡å¯¹æ¯”: {traditional_time/len(test_files)*1000:.1f}ms vs {ultra_time/len(test_files)*1000:.1f}ms æ¯æ–‡ä»¶")


    def _save_performance_log(self, file_list, total_time, step_times):
        """ä¿å­˜æ€§èƒ½æ—¥å¿—åˆ°æ–‡ä»¶"""
        try:
            log_file = self.repo_path / ".merge_work" / "performance_log.json"
            log_file.parent.mkdir(exist_ok=True)
            
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'file_count': len(file_list),
                'total_time': total_time,
                'avg_time_per_file': total_time / len(file_list) * 1000,  # ms
                'step_times': step_times,
                'mode': 'ultra_fast'
            }
            
            # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼ŒåŠ è½½ç°æœ‰æ—¥å¿—
            logs = []
            if log_file.exists():
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        logs = json.load(f)
                except:
                    logs = []
            
            # æ·»åŠ æ–°æ—¥å¿—ï¼ˆä¿ç•™æœ€è¿‘50æ¡ï¼‰
            logs.append(log_entry)
            logs = logs[-50:]
            
            # ä¿å­˜æ—¥å¿—
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(logs, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“ [PERF] æ€§èƒ½æ—¥å¿—å·²ä¿å­˜: {log_file}")
            
        except Exception as e:
            print(f"âš ï¸ [PERF] ä¿å­˜æ€§èƒ½æ—¥å¿—å¤±è´¥: {e}")


if __name__ == "__main__":
    performance_comparison_test()