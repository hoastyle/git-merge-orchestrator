"""
Git Merge Orchestrator - å¢å¼ºè´¡çŒ®è€…åˆ†ææ¨¡å—ï¼ˆv2.3ï¼‰
è´Ÿè´£ä½¿ç”¨å¤šç»´åº¦æƒé‡ç®—æ³•åˆ†ææ–‡ä»¶å’Œç›®å½•çš„è´¡çŒ®è€…
æ”¯æŒè¡Œæ•°æƒé‡åˆ†æã€æ—¶é—´è¡°å‡æƒé‡ã€ä¸€è‡´æ€§è¯„åˆ†ç­‰é«˜çº§åŠŸèƒ½
"""

from datetime import datetime, timedelta
from config import (
    ENHANCED_CONTRIBUTOR_ANALYSIS,
    ALGORITHM_CONFIGS,
    DEFAULT_ANALYSIS_MONTHS,
    DEFAULT_ACTIVE_MONTHS,
)


class EnhancedContributorAnalyzer:
    """å¢å¼ºè´¡çŒ®è€…åˆ†æå™¨ï¼ˆæ”¯æŒå¤šç»´åº¦æƒé‡ï¼‰"""

    def __init__(self, git_ops):
        self.git_ops = git_ops
        self._active_contributors_cache = None
        self._all_contributors_cache = None
        self.config = ENHANCED_CONTRIBUTOR_ANALYSIS

        # æ£€æŸ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨
        self.enabled = self.config.get("enabled", True)

        if not self.enabled:
            print("âš ï¸  å¢å¼ºè´¡çŒ®è€…åˆ†æå·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€åˆ†æ")

    def is_enabled(self):
        """æ£€æŸ¥å¢å¼ºåˆ†ææ˜¯å¦å¯ç”¨"""
        return self.enabled

    def get_algorithm_config(self):
        """è·å–å½“å‰ç®—æ³•é…ç½®"""
        algorithm_type = self.config.get("assignment_algorithm", "comprehensive")
        return ALGORITHM_CONFIGS.get(algorithm_type, ALGORITHM_CONFIGS["comprehensive"])

    def analyze_file_contributors(
        self, filepath, months=None, enable_line_analysis=True
    ):
        """
        å¢å¼ºæ–‡ä»¶è´¡çŒ®è€…åˆ†æ
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            months: åˆ†ææ—¶é—´èŒƒå›´ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
            enable_line_analysis: æ˜¯å¦å¯ç”¨è¡Œæ•°æƒé‡åˆ†æ
            
        Returns:
            dict: å¢å¼ºçš„è´¡çŒ®è€…ä¿¡æ¯
        """
        if not self.enabled:
            # å›é€€åˆ°åŸºç¡€åˆ†æ
            return self._fallback_to_basic_analysis(filepath, months)

        months = months or self.config.get("analysis_months", DEFAULT_ANALYSIS_MONTHS)

        try:
            # ä½¿ç”¨å¢å¼ºGitæ—¥å¿—è§£æ
            contributors = self.git_ops.get_enhanced_file_contributors(
                filepath, months, enable_line_analysis
            )

            # åº”ç”¨æ´»è·ƒåº¦è¿‡æ»¤
            active_contributors = self._filter_active_contributors(contributors)

            # åº”ç”¨æœ€ä½åˆ†æ•°é˜ˆå€¼è¿‡æ»¤
            filtered_contributors = self._apply_score_threshold(active_contributors)

            # æ ‡å‡†åŒ–åˆ†æ•°
            normalized_contributors = self._normalize_scores(filtered_contributors)

            return normalized_contributors

        except Exception as e:
            print(f"å¢å¼ºåˆ†æå¤±è´¥: {e}")
            if self.config.get("fallback_to_simple", True):
                print("å›é€€åˆ°åŸºç¡€åˆ†æ...")
                return self._fallback_to_basic_analysis(filepath, months)
            return {}

    def analyze_contributors_batch(
        self, file_paths, months=None, enable_line_analysis=True
    ):
        """
        æ‰¹é‡å¢å¼ºè´¡çŒ®è€…åˆ†æ
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            months: åˆ†ææ—¶é—´èŒƒå›´
            enable_line_analysis: æ˜¯å¦å¯ç”¨è¡Œæ•°åˆ†æ
            
        Returns:
            dict: {æ–‡ä»¶è·¯å¾„: å¢å¼ºè´¡çŒ®è€…ä¿¡æ¯}
        """
        if not self.enabled:
            return self._fallback_to_basic_batch_analysis(file_paths, months)

        months = months or self.config.get("analysis_months", DEFAULT_ANALYSIS_MONTHS)

        try:
            from datetime import datetime
            
            # é˜¶æ®µ1: ä½¿ç”¨å¢å¼ºæ‰¹é‡Gitæ—¥å¿—è§£æ
            git_parsing_start = datetime.now()
            batch_contributors = self.git_ops.get_enhanced_contributors_batch(
                file_paths, months, enable_line_analysis
            )
            git_parsing_time = (datetime.now() - git_parsing_start).total_seconds()
            print(f"âš¡ Gitæ—¥å¿—è§£æå®Œæˆ: {git_parsing_time:.2f}s ({len(batch_contributors)} ä¸ªæ–‡ä»¶)")

            # é˜¶æ®µ2: å¯¹æ¯ä¸ªæ–‡ä»¶çš„ç»“æœè¿›è¡Œåå¤„ç†
            post_processing_start = datetime.now()
            processed_results = {}
            file_count = len(batch_contributors)
            
            print(f"ğŸ§ª å¼€å§‹åå¤„ç†: {file_count} ä¸ªæ–‡ä»¶...")
            
            # ä¼˜åŒ–: ä¸€æ¬¡æ€§è·å–æ´»è·ƒè´¡çŒ®è€…åˆ—è¡¨ï¼ˆé¿å…N+1æŸ¥è¯¢é—®é¢˜ï¼‰
            active_contributors_start = datetime.now()
            active_months = self.config.get("active_months", DEFAULT_ACTIVE_MONTHS)
            active_contributors_set = set(self.git_ops.get_active_contributors(active_months))
            active_contributors_time = (datetime.now() - active_contributors_start).total_seconds()
            print(f"âš¡ è·å–æ´»è·ƒè´¡çŒ®è€…åˆ—è¡¨: {active_contributors_time:.2f}s ({len(active_contributors_set)} äºº)")
            
            # è¯¦ç»†çš„åå¤„ç†ç»Ÿè®¡
            filtering_time = active_contributors_time  # åŒ…å«ä¸€æ¬¡æ€§è·å–æ—¶é—´
            scoring_time = 0
            normalization_time = 0
            
            for i, (file_path, contributors) in enumerate(batch_contributors.items(), 1):
                # åº”ç”¨æ´»è·ƒåº¦è¿‡æ»¤ - ä½¿ç”¨é¢„è·å–çš„æ´»è·ƒè´¡çŒ®è€…åˆ—è¡¨
                filter_start = datetime.now()
                active_contributors = self._filter_active_contributors_optimized(contributors, active_contributors_set)
                filtering_time += (datetime.now() - filter_start).total_seconds()

                # åº”ç”¨åˆ†æ•°é˜ˆå€¼è¿‡æ»¤
                score_start = datetime.now()
                filtered_contributors = self._apply_score_threshold(active_contributors)
                scoring_time += (datetime.now() - score_start).total_seconds()

                # æ ‡å‡†åŒ–åˆ†æ•°
                norm_start = datetime.now()
                normalized_contributors = self._normalize_scores(filtered_contributors)
                normalization_time += (datetime.now() - norm_start).total_seconds()

                processed_results[file_path] = normalized_contributors
                
                # è¿›åº¦æ˜¾ç¤ºï¼ˆæ¯10%æ˜¾ç¤ºä¸€æ¬¡ï¼‰
                if i % max(1, file_count // 10) == 0 or i == file_count:
                    progress = (i / file_count) * 100
                    elapsed = (datetime.now() - post_processing_start).total_seconds()
                    print(f"ğŸ”„ åå¤„ç†è¿›åº¦: {i}/{file_count} ({progress:.1f}%) - ç”¨æ—¶ {elapsed:.1f}s")
            
            post_processing_time = (datetime.now() - post_processing_start).total_seconds()
            print(f"âœ… åå¤„ç†å®Œæˆ: {post_processing_time:.2f}s")
            print(f"  â€¢ æ´»è·ƒåº¦è¿‡æ»¤: {filtering_time:.2f}s")
            print(f"  â€¢ åˆ†æ•°è¿‡æ»¤: {scoring_time:.2f}s")  
            print(f"  â€¢ åˆ†æ•°æ ‡å‡†åŒ–: {normalization_time:.2f}s")
            
            # ä¿å­˜è¯¦ç»†æ€§èƒ½è®°å½•
            self._save_analysis_performance_log({
                'git_parsing_time': git_parsing_time,
                'post_processing_time': post_processing_time,
                'filtering_time': filtering_time,
                'scoring_time': scoring_time,
                'normalization_time': normalization_time,
                'files_processed': file_count,
                'total_analysis_time': git_parsing_time + post_processing_time
            })

            return processed_results

        except Exception as e:
            print(f"æ‰¹é‡å¢å¼ºåˆ†æå¤±è´¥: {e}")
            if self.config.get("fallback_to_simple", True):
                print("å›é€€åˆ°åŸºç¡€æ‰¹é‡åˆ†æ...")
                return self._fallback_to_basic_batch_analysis(file_paths, months)
            return {}

    def get_contributor_ranking(self, contributors_dict):
        """
        è·å–è´¡çŒ®è€…æ’å
        
        Args:
            contributors_dict: è´¡çŒ®è€…ä¿¡æ¯å­—å…¸
            
        Returns:
            list: æ’åºåçš„è´¡çŒ®è€…åˆ—è¡¨ [(ä½œè€…, ä¿¡æ¯), ...]
        """
        if not contributors_dict:
            return []

        # æŒ‰å¢å¼ºåˆ†æ•°æ’åº
        sorted_contributors = sorted(
            contributors_dict.items(),
            key=lambda x: x[1].get("enhanced_score", x[1].get("score", 0)),
            reverse=True,
        )

        return sorted_contributors

    def get_best_assignee(self, contributors_dict, exclude_inactive=True):
        """
        è·å–æœ€ä½³ä»»åŠ¡åˆ†é…å¯¹è±¡
        
        Args:
            contributors_dict: è´¡çŒ®è€…ä¿¡æ¯å­—å…¸
            exclude_inactive: æ˜¯å¦æ’é™¤ä¸æ´»è·ƒçš„è´¡çŒ®è€…
            
        Returns:
            tuple: (æœ€ä½³ä½œè€…, ä½œè€…ä¿¡æ¯, æ¨èç†ç”±)
        """
        if not contributors_dict:
            return None, {}, "æ— è´¡çŒ®è€…ä¿¡æ¯"

        # è·å–æ´»è·ƒè´¡çŒ®è€…
        if exclude_inactive:
            active_months = self.config.get("active_months", DEFAULT_ACTIVE_MONTHS)
            active_contributors = self.git_ops.get_active_contributors(active_months)

            # è¿‡æ»¤å‡ºæ´»è·ƒçš„è´¡çŒ®è€…
            filtered_contributors = {
                author: info
                for author, info in contributors_dict.items()
                if author in active_contributors
            }

            if not filtered_contributors:
                # å¦‚æœæ²¡æœ‰æ´»è·ƒè´¡çŒ®è€…ï¼Œä½¿ç”¨æ‰€æœ‰è´¡çŒ®è€…ä½†æ·»åŠ è­¦å‘Š
                filtered_contributors = contributors_dict
                warning_suffix = "ï¼ˆæ³¨æ„ï¼šè¯¥æ–‡ä»¶è¿‘æœŸæ— æ´»è·ƒè´¡çŒ®è€…ï¼‰"
            else:
                warning_suffix = ""
        else:
            filtered_contributors = contributors_dict
            warning_suffix = ""

        # è·å–æœ€é«˜åˆ†è´¡çŒ®è€…
        ranking = self.get_contributor_ranking(filtered_contributors)
        if not ranking:
            return None, {}, "æ— å¯ç”¨è´¡çŒ®è€…"

        best_author, best_info = ranking[0]

        # ç”Ÿæˆæ¨èç†ç”±
        reason = (
            self._generate_assignment_reason(best_author, best_info) + warning_suffix
        )

        return best_author, best_info, reason

    def _filter_active_contributors(self, contributors_dict):
        """è¿‡æ»¤æ´»è·ƒè´¡çŒ®è€…"""
        if not contributors_dict:
            return {}

        active_months = self.config.get("active_months", DEFAULT_ACTIVE_MONTHS)
        active_contributors = self.git_ops.get_active_contributors(active_months)

        # å¦‚æœæ²¡æœ‰æ´»è·ƒè´¡çŒ®è€…ï¼Œè¿”å›åŸå§‹æ•°æ®
        if not active_contributors:
            return contributors_dict

        # è¿‡æ»¤æ´»è·ƒè´¡çŒ®è€…
        filtered = {}
        for author, info in contributors_dict.items():
            if author in active_contributors:
                info["is_active"] = True
                filtered[author] = info
            else:
                # æ ‡è®°ä¸ºä¸æ´»è·ƒä½†ä¿ç•™æ•°æ®
                info["is_active"] = False
                filtered[author] = info

        return filtered

    def _apply_score_threshold(self, contributors_dict):
        """åº”ç”¨æœ€ä½åˆ†æ•°é˜ˆå€¼è¿‡æ»¤"""
        if not contributors_dict:
            return {}

        min_threshold = self.config.get("minimum_score_threshold", 0.1)

        filtered = {}
        for author, info in contributors_dict.items():
            score = info.get("enhanced_score", info.get("score", 0))
            if score >= min_threshold:
                filtered[author] = info

        return filtered if filtered else contributors_dict  # å¦‚æœå…¨éƒ¨è¢«è¿‡æ»¤ï¼Œè¿”å›åŸå§‹æ•°æ®

    def _apply_score_threshold_relaxed(self, contributors_dict):
        """åº”ç”¨æ›´å®½æ¾çš„æœ€ä½åˆ†æ•°é˜ˆå€¼è¿‡æ»¤ï¼ˆç”¨äºæ‰¹é‡å†³ç­–ï¼‰"""
        if not contributors_dict:
            return {}

        # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„å€™é€‰äºº
        min_threshold = self.config.get("minimum_score_threshold", 0.1) * 0.5  # é™ä½50%
        
        # å¦‚æœæ‰€æœ‰è´¡çŒ®è€…çš„åˆ†æ•°éƒ½å¾ˆä½ï¼Œè¿›ä¸€æ­¥æ”¾å®½
        all_scores = [info.get("enhanced_score", info.get("score", 0)) for info in contributors_dict.values()]
        if all_scores and max(all_scores) < min_threshold:
            min_threshold = min(all_scores) * 0.8  # ä½¿ç”¨æœ€ä½åˆ†æ•°çš„80%
            print(f"ğŸ”§ è‡ªåŠ¨è°ƒæ•´åˆ†æ•°é˜ˆå€¼ä¸º {min_threshold:.3f} (åŸé˜ˆå€¼è¿‡ä¸¥)")

        filtered = {}
        for author, info in contributors_dict.items():
            score = info.get("enhanced_score", info.get("score", 0))
            if score >= min_threshold:
                filtered[author] = info

        return filtered if filtered else contributors_dict  # å¦‚æœå…¨éƒ¨è¢«è¿‡æ»¤ï¼Œè¿”å›åŸå§‹æ•°æ®

    def _normalize_scores(self, contributors_dict):
        """æ ‡å‡†åŒ–åˆ†æ•°"""
        if not contributors_dict:
            return {}

        normalization_method = self.config.get("score_normalization", "min_max")

        # æå–æ‰€æœ‰åˆ†æ•°
        scores = []
        for info in contributors_dict.values():
            score = info.get("enhanced_score", info.get("score", 0))
            scores.append(score)

        if not scores:
            return contributors_dict

        # åº”ç”¨æ ‡å‡†åŒ–
        if normalization_method == "min_max":
            min_score = min(scores)
            max_score = max(scores)
            score_range = max_score - min_score

            if score_range == 0:
                return contributors_dict  # æ‰€æœ‰åˆ†æ•°ç›¸åŒï¼Œæ— éœ€æ ‡å‡†åŒ–

            for author, info in contributors_dict.items():
                score = info.get("enhanced_score", info.get("score", 0))
                normalized_score = (score - min_score) / score_range
                info["normalized_score"] = normalized_score

        elif normalization_method == "z_score":
            import statistics

            try:
                mean_score = statistics.mean(scores)
                std_score = statistics.stdev(scores) if len(scores) > 1 else 1

                for author, info in contributors_dict.items():
                    score = info.get("enhanced_score", info.get("score", 0))
                    z_score = (score - mean_score) / std_score if std_score != 0 else 0
                    info["normalized_score"] = z_score
            except statistics.StatisticsError:
                # å›é€€åˆ°åŸå§‹åˆ†æ•°
                for author, info in contributors_dict.items():
                    info["normalized_score"] = info.get(
                        "enhanced_score", info.get("score", 0)
                    )

        elif normalization_method == "percentile":
            import statistics

            sorted_scores = sorted(scores)

            for author, info in contributors_dict.items():
                score = info.get("enhanced_score", info.get("score", 0))
                # è®¡ç®—ç™¾åˆ†ä½æ•°
                percentile = (sorted_scores.index(score) + 1) / len(sorted_scores)
                info["normalized_score"] = percentile

        return contributors_dict

    def _generate_assignment_reason(self, author, author_info):
        """ç”Ÿæˆåˆ†é…æ¨èç†ç”±"""
        reasons = []

        # æå–å…³é”®ä¿¡æ¯
        recent_commits = author_info.get("recent_commits", 0)
        total_commits = author_info.get("total_commits", 0)
        total_changes = author_info.get("total_changes", 0)
        enhanced_score = author_info.get("enhanced_score", 0)

        # åˆ†æ•°è¯¦ç»†ä¿¡æ¯
        score_breakdown = author_info.get("score_breakdown", {})

        # åŸºäºæäº¤æ•°é‡çš„ç†ç”±
        if recent_commits > 0:
            if recent_commits >= 5:
                reasons.append("é«˜é¢‘è¿‘æœŸè´¡çŒ®è€…")
            elif recent_commits >= 2:
                reasons.append("æ´»è·ƒè´¡çŒ®è€…")
            else:
                reasons.append("è¿‘æœŸæœ‰è´¡çŒ®")

        # åŸºäºè¡Œæ•°å˜æ›´çš„ç†ç”±
        if total_changes > 0:
            if total_changes >= 1000:
                reasons.append("å¤§è§„æ¨¡ä»£ç å˜æ›´ç»éªŒ")
            elif total_changes >= 100:
                reasons.append("ä¸­ç­‰è§„æ¨¡å˜æ›´ç»éªŒ")
            else:
                reasons.append("æœ‰ä»£ç å˜æ›´ç»éªŒ")

        # åŸºäºç®—æ³•ç±»å‹çš„ç‰¹æ®Šè¯´æ˜
        algorithm = self.config.get("assignment_algorithm", "comprehensive")
        if algorithm == "comprehensive":
            consistency_score = score_breakdown.get("consistency_score", 0)
            if consistency_score > 0.5:
                reasons.append("æŒç»­è´¡çŒ®æ¨¡å¼")

        # åŸºäºå¢å¼ºè¯„åˆ†çš„æ€»ä½“è¯„ä»·
        if enhanced_score >= 5.0:
            reasons.append("ç»¼åˆè¯„åˆ†ä¼˜ç§€")
        elif enhanced_score >= 2.0:
            reasons.append("ç»¼åˆè¯„åˆ†è‰¯å¥½")

        # ç”Ÿæˆæœ€ç»ˆç†ç”±
        if reasons:
            primary_reason = reasons[0]
            if len(reasons) > 1:
                return f"{primary_reason}ï¼Œ{', '.join(reasons[1:])}"
            return primary_reason

        return "åŸºäºè´¡çŒ®å†å²æ¨è"

    def _fallback_to_basic_analysis(self, filepath, months):
        """å›é€€åˆ°åŸºç¡€åˆ†æ"""
        from .contributor_analyzer import ContributorAnalyzer

        basic_analyzer = ContributorAnalyzer(self.git_ops)
        return basic_analyzer.analyze_file_contributors(filepath)

    def _fallback_to_basic_batch_analysis(self, file_paths, months):
        """å›é€€åˆ°åŸºç¡€æ‰¹é‡åˆ†æ"""
        result = {}
        for file_path in file_paths:
            result[file_path] = self._fallback_to_basic_analysis(file_path, months)
        return result

    def compute_final_decision_batch(self, files_contributors_dict, active_contributors_set=None):
        """
        æ‰¹é‡é¢„è®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„æœ€ä¼˜åˆ†é…å†³ç­–
        
        Args:
            files_contributors_dict: {æ–‡ä»¶è·¯å¾„: è´¡çŒ®è€…ä¿¡æ¯} å­—å…¸
            active_contributors_set: æ´»è·ƒè´¡çŒ®è€…é›†åˆï¼ˆå¦‚æœæœªæä¾›å°†è‡ªåŠ¨è·å–ï¼‰
            
        Returns:
            dict: {æ–‡ä»¶è·¯å¾„: {'primary': (ä½œè€…, ä¿¡æ¯), 'alternatives': [...], 'reason': ç†ç”±}}
        """
        from datetime import datetime
        
        if not files_contributors_dict:
            return {}
            
        start_time = datetime.now()
        
        # è·å–æ´»è·ƒè´¡çŒ®è€…é›†åˆï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if active_contributors_set is None:
            active_months = self.config.get("active_months", DEFAULT_ACTIVE_MONTHS)
            active_contributors_set = set(self.git_ops.get_active_contributors(active_months))
        
        print(f"ğŸ¯ å¼€å§‹æ‰¹é‡å†³ç­–é¢„è®¡ç®—: {len(files_contributors_dict)} ä¸ªæ–‡ä»¶...")
        
        decisions = {}
        processed_count = 0
        
        for file_path, contributors in files_contributors_dict.items():
            # åº”ç”¨æ´»è·ƒåº¦è¿‡æ»¤ï¼ˆä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
            filtered_contributors = self._filter_active_contributors_optimized(
                contributors, active_contributors_set
            )
            
            # å¦‚æœæ´»è·ƒåº¦è¿‡æ»¤åæ²¡æœ‰å€™é€‰äººï¼Œä½¿ç”¨åŸå§‹æ•°æ®
            if not filtered_contributors:
                print(f"âš ï¸ æ–‡ä»¶ {file_path} æ— æ´»è·ƒè´¡çŒ®è€…ï¼Œä½¿ç”¨æ‰€æœ‰è´¡çŒ®è€…")
                filtered_contributors = contributors
            
            # åº”ç”¨åˆ†æ•°é˜ˆå€¼è¿‡æ»¤ï¼ˆä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ï¼‰
            threshold_filtered = self._apply_score_threshold_relaxed(filtered_contributors)
            
            # å¦‚æœé˜ˆå€¼è¿‡æ»¤åæ²¡æœ‰å€™é€‰äººï¼Œä½¿ç”¨è¿‡æ»¤å‰çš„æ•°æ®
            if not threshold_filtered:
                threshold_filtered = filtered_contributors
            
            # æ ‡å‡†åŒ–åˆ†æ•°
            normalized_contributors = self._normalize_scores(threshold_filtered)
            
            # è·å–æ’åºåçš„å€™é€‰äººåˆ—è¡¨
            ranking = self.get_contributor_ranking(normalized_contributors)
            
            if ranking:
                primary_author, primary_info = ranking[0]
                alternatives = ranking[1:5]  # ä¿ç•™å‰5ä¸ªå¤‡é€‰
                reason = self._generate_assignment_reason(primary_author, primary_info)
                
                decisions[file_path] = {
                    'primary': (primary_author, primary_info),
                    'alternatives': alternatives,
                    'reason': reason,
                    'all_candidates': len(ranking),
                    'active_candidates': len([r for r in ranking if r[1].get('is_active', True)])
                }
            else:
                decisions[file_path] = {
                    'primary': None,
                    'alternatives': [],
                    'reason': 'æ— å¯ç”¨è´¡çŒ®è€…',
                    'all_candidates': 0,
                    'active_candidates': 0
                }
            
            processed_count += 1
            
            # è¿›åº¦æ˜¾ç¤ºï¼ˆæ¯10%æ˜¾ç¤ºä¸€æ¬¡ï¼‰
            if processed_count % max(1, len(files_contributors_dict) // 10) == 0:
                progress = (processed_count / len(files_contributors_dict)) * 100
                elapsed = (datetime.now() - start_time).total_seconds()
                print(f"ğŸ”„ å†³ç­–è®¡ç®—è¿›åº¦: {processed_count}/{len(files_contributors_dict)} ({progress:.1f}%) - ç”¨æ—¶ {elapsed:.1f}s")
        
        total_time = (datetime.now() - start_time).total_seconds()
        valid_decisions = len([d for d in decisions.values() if d['primary'] is not None])
        
        print(f"âœ… æ‰¹é‡å†³ç­–è®¡ç®—å®Œæˆ: {total_time:.2f}s")
        print(f"ğŸ“Š å†³ç­–ç»Ÿè®¡: {valid_decisions}/{len(decisions)} ä¸ªæ–‡ä»¶æœ‰å¯åˆ†é…å¯¹è±¡ ({valid_decisions/len(decisions)*100:.1f}%)")
        
        # ä¿å­˜å†³ç­–è®¡ç®—æ€§èƒ½æ—¥å¿—
        self._save_decision_performance_log({
            'decision_calculation_time': total_time,
            'files_processed': len(files_contributors_dict),
            'valid_decisions': valid_decisions,
            'avg_decision_time_ms': (total_time / len(files_contributors_dict)) * 1000,
            'active_contributors_count': len(active_contributors_set)
        })
        
        return decisions

    def get_analysis_statistics(self, contributors_dict):
        """è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        if not contributors_dict:
            return {}

        stats = {
            "total_contributors": len(contributors_dict),
            "active_contributors": 0,
            "avg_score": 0,
            "max_score": 0,
            "total_commits": 0,
            "total_changes": 0,
            "algorithm_used": self.config.get("assignment_algorithm", "comprehensive"),
        }

        scores = []
        for author, info in contributors_dict.items():
            if info.get("is_active", True):
                stats["active_contributors"] += 1

            score = info.get("enhanced_score", info.get("score", 0))
            scores.append(score)
            stats["max_score"] = max(stats["max_score"], score)
            stats["total_commits"] += info.get("total_commits", 0)
            stats["total_changes"] += info.get("total_changes", 0)

        if scores:
            stats["avg_score"] = sum(scores) / len(scores)

        return stats

    def export_detailed_analysis(self, filepath, contributors_dict, output_file=None):
        """å¯¼å‡ºè¯¦ç»†åˆ†æç»“æœ"""
        if not self.config.get("export_analysis_results", False):
            return False

        import json
        from pathlib import Path

        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_data = {
            "file_path": filepath,
            "analysis_timestamp": datetime.now().isoformat(),
            "algorithm_config": self.get_algorithm_config(),
            "contributors": {},
        }

        # å¤„ç†è´¡çŒ®è€…æ•°æ®
        for author, info in contributors_dict.items():
            export_data["contributors"][author] = {
                "basic_info": {
                    "total_commits": info.get("total_commits", 0),
                    "recent_commits": info.get("recent_commits", 0),
                    "total_changes": info.get("total_changes", 0),
                    "is_active": info.get("is_active", True),
                },
                "scoring": info.get("score_breakdown", {}),
                "final_scores": {
                    "enhanced_score": info.get("enhanced_score", 0),
                    "normalized_score": info.get("normalized_score", 0),
                },
            }

        # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
        if not output_file:
            safe_filename = filepath.replace("/", "_").replace(" ", "_")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = (
                f".merge_work/analysis_export_{safe_filename}_{timestamp}.json"
            )

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # å†™å…¥æ–‡ä»¶
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ“Š è¯¦ç»†åˆ†æç»“æœå·²å¯¼å‡º: {output_file}")
            return True

        except Exception as e:
            print(f"å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {e}")
            return False

    def debug_analysis(self, filepath, contributors_dict):
        """è°ƒè¯•åˆ†æè¿‡ç¨‹ï¼ˆä»…åœ¨è°ƒè¯•æ¨¡å¼ä¸‹å¯ç”¨ï¼‰"""
        if not self.config.get("debug_mode", False):
            return

        print(f"\nğŸ” è°ƒè¯•åˆ†æ: {filepath}")
        print(f"ç®—æ³•ç±»å‹: {self.config.get('assignment_algorithm', 'comprehensive')}")

        for author, info in contributors_dict.items():
            print(f"\nğŸ‘¤ {author}:")
            print(
                f"  åŸºç¡€ä¿¡æ¯: commits={info.get('total_commits', 0)}, changes={info.get('total_changes', 0)}"
            )

            if "score_breakdown" in info:
                breakdown = info["score_breakdown"]
                print(f"  è¯„åˆ†è¯¦æƒ…:")
                for key, value in breakdown.items():
                    print(f"    {key}: {value:.3f}")

            enhanced_score = info.get("enhanced_score", 0)
            normalized_score = info.get("normalized_score", 0)
            print(
                f"  æœ€ç»ˆåˆ†æ•°: enhanced={enhanced_score:.3f}, normalized={normalized_score:.3f}"
            )
            
    def _save_analysis_performance_log(self, perf_data):
        """ä¿å­˜åˆ†ææ€§èƒ½è¯¦ç»†æ—¥å¿—"""
        try:
            import json
            from pathlib import Path
            from datetime import datetime
            
            # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
            if hasattr(self.git_ops, 'repo_path'):
                repo_path = Path(self.git_ops.repo_path)
            else:
                repo_path = Path(".")
                
            log_file = repo_path / ".merge_work" / "enhanced_analysis_performance.json"
            log_file.parent.mkdir(exist_ok=True)
            
            # æ„å»ºæ—¥å¿—æ¡ç›®
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'component': 'EnhancedContributorAnalyzer',
                'version': '2.3',
                'detailed_breakdown': perf_data,
                'performance_insights': self._generate_analysis_insights(perf_data)
            }
            
            # åŠ è½½ç°æœ‰æ—¥å¿—
            logs = []
            if log_file.exists():
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        logs = json.load(f)
                except:
                    logs = []
            
            # æ·»åŠ æ–°æ—¥å¿—
            logs.append(log_entry)
            
            # ä¿æŒæœ€è¿‘30æ¡è®°å½•
            if len(logs) > 30:
                logs = logs[-30:]
                
            # å†™å…¥æ–‡ä»¶
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(logs, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜åˆ†ææ€§èƒ½æ—¥å¿—å¤±è´¥: {e}")
    
    def _generate_analysis_insights(self, perf_data):
        """ç”Ÿæˆåˆ†ææ€§èƒ½æ´å¯Ÿ"""
        insights = []
        
        total_time = perf_data.get('total_analysis_time', 0)
        git_time = perf_data.get('git_parsing_time', 0)
        post_time = perf_data.get('post_processing_time', 0)
        files_count = perf_data.get('files_processed', 0)
        
        # åˆ†ææ—¶é—´åˆ†å¸ƒ
        if post_time > git_time:
            insights.append(f"åå¤„ç†è€—æ—¶è¾ƒå¤š ({post_time:.1f}s vs {git_time:.1f}s Gitè§£æ)")
        
        # æ£€æŸ¥å•ä¸ªç»†åˆ†é˜¶æ®µ  
        filter_time = perf_data.get('filtering_time', 0)
        scoring_time = perf_data.get('scoring_time', 0)
        norm_time = perf_data.get('normalization_time', 0)
        
        if filter_time > post_time * 0.4:
            insights.append(f"æ´»è·ƒåº¦è¿‡æ»¤è€—æ—¶è¾ƒå¤š ({filter_time:.2f}s)")
        if scoring_time > post_time * 0.4:
            insights.append(f"åˆ†æ•°è¿‡æ»¤è€—æ—¶è¾ƒå¤š ({scoring_time:.2f}s)")
        if norm_time > post_time * 0.4:
            insights.append(f"åˆ†æ•°æ ‡å‡†åŒ–è€—æ—¶è¾ƒå¤š ({norm_time:.2f}s)")
            
        # æ€§èƒ½å»ºè®®
        if total_time > 30:
            insights.append(f"åˆ†ææ€»è€—æ—¶è¾ƒé•¿ ({total_time:.1f}s), å¯è€ƒè™‘ç¼“å­˜ä¼˜åŒ–")
            
        avg_time_per_file = total_time / files_count * 1000 if files_count > 0 else 0
        if avg_time_per_file > 25:  # 25ms per file
            insights.append(f"å¹³å‡æ–‡ä»¶åˆ†ææ—¶é—´è¾ƒé•¿ ({avg_time_per_file:.1f}ms)")
            
        if not insights:
            insights.append("åˆ†ææ€§èƒ½è¡¨ç°è‰¯å¥½")
            
        return insights
        
    def _filter_active_contributors_optimized(self, contributors_dict, active_contributors_set):
        """ä¼˜åŒ–ç‰ˆæ´»è·ƒåº¦è¿‡æ»¤ - ä½¿ç”¨é¢„è·å–çš„æ´»è·ƒè´¡çŒ®è€…é›†åˆ"""
        if not contributors_dict:
            return {}

        # å¦‚æœæ²¡æœ‰æ´»è·ƒè´¡çŒ®è€…ï¼Œè¿”å›åŸå§‹æ•°æ®
        if not active_contributors_set:
            return contributors_dict

        # ä½¿ç”¨é¢„è·å–çš„é›†åˆè¿›è¡Œå¿«é€Ÿè¿‡æ»¤
        filtered = {}
        inactive_count = 0
        
        for author, info in contributors_dict.items():
            if author in active_contributors_set:
                info["is_active"] = True
                filtered[author] = info
            else:
                # æ ‡è®°ä¸ºä¸æ´»è·ƒ
                info["is_active"] = False
                inactive_count += 1
                # åœ¨æ‰¹é‡å†³ç­–ä¸­ï¼Œå¦‚æœæ´»è·ƒè´¡çŒ®è€…å¤ªå°‘ï¼ŒåŒ…å«ä¸æ´»è·ƒçš„è´¡çŒ®è€…
                include_inactive = self.config.get("include_inactive", False)
                if include_inactive or len(filtered) < 2:  # ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›å€™é€‰äºº
                    filtered[author] = info

        # å¦‚æœè¿‡æ»¤åå€™é€‰äººå¤ªå°‘ï¼ŒåŒ…å«æ‰€æœ‰è´¡çŒ®è€…
        if len(filtered) < max(1, len(contributors_dict) * 0.3):  # è‡³å°‘ä¿ç•™30%çš„å€™é€‰äºº
            print(f"ğŸ”§ æ´»è·ƒåº¦è¿‡æ»¤è¿‡ä¸¥ï¼Œä¿ç•™æ‰€æœ‰ {len(contributors_dict)} ä½è´¡çŒ®è€… (æ´»è·ƒ:{len(filtered)}, ä¸æ´»è·ƒ:{inactive_count})")
            return contributors_dict

        return filtered
        
    def _save_decision_performance_log(self, perf_data):
        """ä¿å­˜å†³ç­–è®¡ç®—æ€§èƒ½è¯¦ç»†æ—¥å¿—"""
        try:
            import json
            from pathlib import Path
            from datetime import datetime
            
            # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
            if hasattr(self.git_ops, 'repo_path'):
                repo_path = Path(self.git_ops.repo_path)
            else:
                repo_path = Path(".")
                
            log_file = repo_path / ".merge_work" / "decision_performance.json"
            log_file.parent.mkdir(exist_ok=True)
            
            # æ„å»ºæ—¥å¿—æ¡ç›®
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'component': 'EnhancedContributorAnalyzer.compute_final_decision_batch',
                'version': '2.3',
                'performance_data': perf_data,
                'efficiency_metrics': {
                    'decisions_per_second': perf_data.get('files_processed', 0) / max(perf_data.get('decision_calculation_time', 1), 0.001),
                    'avg_decision_time_ms': perf_data.get('avg_decision_time_ms', 0),
                    'success_rate': perf_data.get('valid_decisions', 0) / max(perf_data.get('files_processed', 1), 1) * 100
                }
            }
            
            # åŠ è½½ç°æœ‰æ—¥å¿—
            logs = []
            if log_file.exists():
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        logs = json.load(f)
                except:
                    logs = []
            
            # æ·»åŠ æ–°æ—¥å¿—
            logs.append(log_entry)
            
            # ä¿æŒæœ€è¿‘20æ¡è®°å½•
            if len(logs) > 20:
                logs = logs[-20:]
                
            # å†™å…¥æ–‡ä»¶
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(logs, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å†³ç­–æ€§èƒ½æ—¥å¿—å¤±è´¥: {e}")
